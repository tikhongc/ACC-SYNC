"""
Enhanced Full Review Sync Script
Synchronizes all reviews data from ACC API to local database using async parallel optimization

Features:
- Async parallel API calls for maximum performance
- Workflow caching optimization (reduces API calls by 60-90%)
- Batch UPSERT database operations
- Circuit breaker pattern for reliability
- Performance monitoring and bottleneck analysis

Requires EnhancedReviewSyncManager for optimal performance.
"""

import sys
import os

# Set UTF-8 encoding for Windows console
if sys.platform == "win32":
    import codecs
    try:
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    except AttributeError:
        # In newer Python versions, stdout/stderr may already be text streams
        pass

from typing import Dict, List, Optional, Any
from datetime import datetime, timezone
import json
import time
import functools

# Add project root to path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# Override print to always flush output (fixes Windows console buffering)
print = functools.partial(print, flush=True)

# Import review sync components
try:
    from database_sql.review_data_access import ReviewDataAccess
    from database_sql.neon_config import NeonConfig
except ImportError:
    print("Warning: Could not import from database_sql, trying alternative path")
    try:
        from review_data_access import ReviewDataAccess
        from neon_config import NeonConfig
    except ImportError:
        print("Error: Could not import ReviewDataAccess and NeonConfig")
        sys.exit(1)

# Import API client
try:
    import requests
    import config
    import utils
except ImportError:
    print("Error: Could not import required modules")
    print("Please ensure config.py and utils.py are available")
    sys.exit(1)


class FullReviewSync:
    """
    Full review synchronization manager with async parallel optimization
    
    Features:
    - Async parallel API calls using EnhancedReviewSyncManager
    - Workflow caching optimization (reduces API calls by 60-90%)
    - Batch UPSERT database operations
    - Circuit breaker pattern for reliability
    - Performance monitoring and bottleneck analysis
    """
    
    def __init__(
        self, 
        project_id: str, 
        access_token: Optional[str] = None,
        sync_manager: Any = None
    ):
        """
        Initialize full review sync with enhanced async capabilities
        
        Args:
            project_id: ACC project ID
            access_token: OAuth access token (optional, will get from utils if not provided)
            sync_manager: EnhancedReviewSyncManager instance (required for async parallel sync)
        """
        # Store both original and cleaned project IDs
        self.original_project_id = project_id  # For database operations (with b. prefix)
        self.project_id = self._clean_project_id(project_id)  # For API calls (without b. prefix)
        self.access_token = access_token or self._get_access_token()
        self.headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json"
        }
        
        # Store sync_manager reference for caching
        self.sync_manager = sync_manager
        
        # Initialize data access
        if sync_manager and hasattr(sync_manager, 'da'):
            # Use sync_manager's data access if available
            self.data_access = sync_manager.da
        else:
            neon_config = NeonConfig()
            self.data_access = ReviewDataAccess(neon_config.get_db_params())
        
        # Sync statistics
        self.stats = {
            'workflows_total': 0,
            'workflows_synced': 0,
            'reviews_total': 0,
            'reviews_synced': 0,
            'file_versions_total': 0,
            'file_versions_synced': 0,
            'progress_steps_total': 0,
            'progress_steps_synced': 0,
            'workflow_templates_total': 0,
            'workflow_templates_synced': 0,
            'errors': [],
            'start_time': None,
            'end_time': None
        }
    
    def _clean_project_id(self, project_id: str) -> str:
        """Remove 'b.' prefix from project ID if present"""
        if project_id.startswith('b.'):
            return project_id[2:]
        return project_id
    
    def _get_access_token(self) -> str:
        """Get access token from utils"""
        try:
            token = utils.get_access_token()
            if not token:
                raise ValueError("No access token available. Please authenticate first.")
            return token
        except Exception as e:
            print(f"Error getting access token: {e}")
            raise
    
    def _make_api_request(self, url: str, params: Optional[Dict] = None) -> Dict:
        """
        Make API request with error handling and optional caching
        
        Args:
            url: API endpoint URL
            params: Query parameters
            
        Returns:
            API response data
        """
        # Generate cache key
        params_str = json.dumps(params, sort_keys=True) if params else ''
        cache_key = f"{url}?{params_str}"
        
        # Check cache if sync_manager is available
        if self.sync_manager and hasattr(self.sync_manager, 'cache'):
            cached_data = self.sync_manager.cache.get('api', cache_key)
            if cached_data is not None:
                self.sync_manager.metrics.cache_hits += 1
                return cached_data
            self.sync_manager.metrics.cache_misses += 1
        
        # Make actual API request
        start_time = time.time()
        try:
            print(f"   Making API request to: {url}")
            print(f"   Params: {params}")
            response = requests.get(url, headers=self.headers, params=params, timeout=10)
            
            if response.status_code != 200:
                error_msg = f"API request failed: {response.status_code}"
                try:
                    error_data = response.json()
                    error_msg += f" - {error_data.get('detail', response.text)}"
                except:
                    error_msg += f" - {response.text}"
                raise Exception(error_msg)
            
            data = response.json()
            
            # Update metrics if sync_manager is available
            if self.sync_manager and hasattr(self.sync_manager, 'metrics'):
                elapsed = time.time() - start_time
                self.sync_manager.metrics.api_calls += 1
                self.sync_manager.metrics.api_time += elapsed
            
            # Cache the result
            if self.sync_manager and hasattr(self.sync_manager, 'cache'):
                self.sync_manager.cache.set('api', cache_key, value=data)
            
            return data
        
        except requests.exceptions.Timeout:
            if self.sync_manager and hasattr(self.sync_manager, 'metrics'):
                self.sync_manager.metrics.api_errors += 1
            raise Exception(f"API request timeout: {url}")
        except requests.exceptions.RequestException as e:
            if self.sync_manager and hasattr(self.sync_manager, 'metrics'):
                self.sync_manager.metrics.api_errors += 1
            raise Exception(f"API request error: {str(e)}")
    
    def sync_all_workflows(self) -> int:
        """
        Sync all workflows from ACC
        
        Returns:
            Number of workflows synced
        """
        print("\n" + "="*80)
        print("SYNCING WORKFLOWS")
        print("="*80)
        
        try:
            # Get all workflows with pagination
            all_workflows = []
            offset = 0
            limit = 50
            
            while True:
                url = f"{config.AUTODESK_API_BASE}/construction/reviews/v1/projects/{self.project_id}/workflows"
                params = {
                    'limit': limit,
                    'offset': offset,
                    'filter[status]': 'ACTIVE'
                }
                
                print(f"\nFetching workflows (offset: {offset}, limit: {limit})...")
                data = self._make_api_request(url, params)
                
                workflows = data.get('results', [])
                all_workflows.extend(workflows)
                
                print(f"  Retrieved {len(workflows)} workflows")
                
                # Check if there are more results
                pagination = data.get('pagination', {})
                total_results = pagination.get('totalResults', 0)
                
                if offset + len(workflows) >= total_results:
                    break
                
                offset += limit
            
            self.stats['workflows_total'] = len(all_workflows)
            print(f"\nTotal workflows to sync: {len(all_workflows)}")
            
            # Sync workflows directly
            for idx, workflow in enumerate(all_workflows, 1):
                try:
                    workflow_id = workflow.get('id')
                    workflow_name = workflow.get('name', 'Unknown')
                    
                    print(f"[{idx}/{len(all_workflows)}] {workflow_name}...", end=' ')
                    
                    workflow_data = {
                        'workflow_uuid': workflow_id,
                        'project_id': self.original_project_id,
                        'data_source': 'acc_sync',
                        'acc_workflow_id': workflow_id,
                        'name': workflow.get('name', ''),
                        'description': workflow.get('description'),
                        'notes': workflow.get('notes'),
                        'status': workflow.get('status', 'ACTIVE'),
                        'additional_options': workflow.get('additionalOptions', {}),
                        'approval_status_options': workflow.get('approvalStatusOptions', []),
                        'copy_files_options': workflow.get('copyFilesOptions', {}),
                        'attached_attributes': workflow.get('attachedAttributes', []),
                        'update_attributes_options': workflow.get('updateAttributesOptions', {}),
                        'steps': workflow.get('steps', []),
                        'created_by': workflow.get('createdBy', {}),
                        'created_at': workflow.get('createdAt'),
                        'updated_at': workflow.get('updatedAt'),
                        'last_synced_at': datetime.now().isoformat(),
                        'sync_status': 'synced'
                    }
                    
                    self.data_access.create_workflow(workflow_data)
                    self.stats['workflows_synced'] += 1
                    print("[OK]")
                    
                except Exception as e:
                    print(f"[ERROR] {str(e)}")
                    self.stats['errors'].append(f"Workflow {workflow.get('id', 'unknown')}: {str(e)}")
            
            return len(all_workflows)
        
        except Exception as e:
            error_msg = f"Error syncing workflows: {str(e)}"
            print(f"\nERROR: {error_msg}")
            self.stats['errors'].append(error_msg)
            return 0
    
    def sync_all_reviews(self) -> int:
        """
        Sync all reviews from ACC with full details (file versions, progress)
        ä½¿ç”¨å¼‚æ­¥å¹¶è¡Œ + å·¥ä½œæµç¼“å­˜ä¼˜åŒ–
        
        Returns:
            Number of reviews synced
        """
        print("\n" + "="*80)
        print("SYNCING REVIEWS")
        print("="*80)
        
        # [ASYNC] æ£€æŸ¥æ˜¯å¦æœ‰ EnhancedReviewSyncManager
        if not self.sync_manager or not hasattr(self.sync_manager, 'async_sync_reviews_parallel'):
            error_msg = "éœ€è¦ EnhancedReviewSyncManager æ¥è¿›è¡Œä¼˜åŒ–åŒæ­¥"
            print(f"[ERROR] {error_msg}")
            self.stats['errors'].append(error_msg)
            raise Exception(error_msg)
        
        print("\n[ASYNC] ä½¿ç”¨ EnhancedReviewSyncManager å¼‚æ­¥å¹¶è¡ŒåŒæ­¥...")
        
        try:
            return self._sync_reviews_with_enhanced_manager()
        except Exception as e:
            error_msg = f"å¼‚æ­¥å¹¶è¡ŒåŒæ­¥å¤±è´¥: {str(e)}"
            print(f"âœ— {error_msg}")
            self.stats['errors'].append(error_msg)
            return 0
    
    def _sync_reviews_with_enhanced_manager(self) -> int:
        """ä½¿ç”¨ EnhancedReviewSyncManager è¿›è¡Œå¼‚æ­¥å¹¶è¡ŒåŒæ­¥"""
        import asyncio
        
        async def async_sync():
            # èŽ·å–æ‰€æœ‰ reviews
            all_reviews = []
            offset = 0
            limit = 50
            
            while True:
                url = f"{config.AUTODESK_API_BASE}/construction/reviews/v1/projects/{self.project_id}/reviews"
                params = {
                    'limit': limit,
                    'offset': offset
                }
                
                print(f"\nFetching reviews (offset: {offset}, limit: {limit})...")
                data = self._make_api_request(url, params)
                
                reviews = data.get('results', [])
                all_reviews.extend(reviews)
                
                print(f"  Retrieved {len(reviews)} reviews")
                
                # Check if there are more results
                pagination = data.get('pagination', {})
                total_results = pagination.get('totalResults', 0)
                
                if offset + len(reviews) >= total_results:
                    break
                
                offset += limit
            
            self.stats['reviews_total'] = len(all_reviews)
            print(f"\nTotal reviews to sync: {len(all_reviews)}")
            
            if not all_reviews:
                return 0
            
            # [ASYNC] ä½¿ç”¨ EnhancedReviewSyncManager çš„å¼‚æ­¥å¹¶è¡ŒåŠŸèƒ½
            try:
                # ä¸´æ—¶è®¾ç½® headers åˆ° sync_manager
                if hasattr(self.sync_manager, '_temp_headers'):
                    self.sync_manager._temp_headers = self.headers
                else:
                    self.sync_manager._temp_headers = self.headers
                
                sync_stats = await self.sync_manager.async_sync_reviews_parallel(
                    api_client=self,  # ä¼ å…¥ self ä»¥ä¾¿èŽ·å– headers
                    project_id=self.original_project_id,
                    reviews=all_reviews,
                    show_progress=True
                )
                
                # ðŸ”§ ä¿®å¤ï¼šå°† EnhancedReviewSyncManager çš„ç»Ÿè®¡ä¿¡æ¯åŒæ­¥åˆ° FullReviewSync.stats
                self.stats['reviews_total'] = len(all_reviews)
                self.stats['reviews_synced'] = sync_stats.get('reviews_synced', 0)
                self.stats['file_versions_total'] = sync_stats.get('file_versions_total', 0)
                self.stats['file_versions_synced'] = sync_stats.get('file_versions_synced', 0)
                self.stats['progress_steps_total'] = sync_stats.get('progress_steps_total', 0)
                self.stats['progress_steps_synced'] = sync_stats.get('progress_steps_synced', 0)
                
                # åˆå¹¶é”™è¯¯ä¿¡æ¯
                if 'errors' in sync_stats:
                    self.stats['errors'].extend(sync_stats['errors'])
                
                print(f"\nâœ“ å¼‚æ­¥å¹¶è¡ŒåŒæ­¥å®Œæˆ")
                print(f"   åŒæ­¥çš„è¯„å®¡: {sync_stats.get('reviews_synced', 0)}")
                print(f"   æ›´æ–°çš„è¯„å®¡: {sync_stats.get('reviews_updated', 0)}")
                print(f"   æ–‡ä»¶ç‰ˆæœ¬: {sync_stats.get('file_versions_synced', 0)}")
                print(f"   è¿›åº¦æ­¥éª¤: {sync_stats.get('progress_steps_synced', 0)}")
                
                return len(all_reviews)
                
            except Exception as e:
                error_msg = f"å¼‚æ­¥å¹¶è¡ŒåŒæ­¥å¤±è´¥: {str(e)}"
                print(f"âœ— {error_msg}")
                self.stats['errors'].append(error_msg)
                return 0
        
        # è¿è¡Œå¼‚æ­¥å‡½æ•°
        return asyncio.run(async_sync())
    
    
    
    def clean_review_tables(self) -> None:
        """
        Clean all review-related tables, views, and types before full sync
        This ensures a fresh start for full synchronization
        """
        print("\n" + "="*80)
        print("CLEANING REVIEW TABLES")
        print("="*80)
        
        conn = None
        cursor = None
        
        try:
            conn = self.data_access.get_connection()
            cursor = conn.cursor()
            
            # Use a single SQL statement with CASCADE to drop everything efficiently
            print("\nDropping all review-related objects...")
            
            drop_sql = """
            -- Drop views
            DROP VIEW IF EXISTS reviews_overview CASCADE;
            DROP VIEW IF EXISTS pending_tasks_view CASCADE;
            
            -- Drop materialized views
            DROP MATERIALIZED VIEW IF EXISTS mv_file_approval_summary CASCADE;
            
            -- Drop tables (CASCADE will handle dependencies)
            DROP TABLE IF EXISTS file_approval_history CASCADE;
            DROP TABLE IF EXISTS review_notifications CASCADE;
            DROP TABLE IF EXISTS approval_decisions CASCADE;
            DROP TABLE IF EXISTS review_comments CASCADE;
            DROP TABLE IF EXISTS review_progress CASCADE;
            DROP TABLE IF EXISTS review_file_versions CASCADE;
            DROP TABLE IF EXISTS review_candidates CASCADE;  -- Ensure removal of deprecated table
            DROP TABLE IF EXISTS review_step_candidates CASCADE;  -- Also drop new table for clean rebuild
            DROP TABLE IF EXISTS workflow_notes CASCADE;
            DROP TABLE IF EXISTS reviews CASCADE;
            DROP TABLE IF EXISTS workflow_templates CASCADE;
            DROP TABLE IF EXISTS workflows CASCADE;
            
            -- Drop enum types
            DROP TYPE IF EXISTS data_source_type CASCADE;
            DROP TYPE IF EXISTS workflow_status_type CASCADE;
            DROP TYPE IF EXISTS review_status_type CASCADE;
            DROP TYPE IF EXISTS approval_status_type CASCADE;
            DROP TYPE IF EXISTS step_type CASCADE;
            DROP TYPE IF EXISTS candidate_type CASCADE;
            DROP TYPE IF EXISTS reviewer_type CASCADE;
            DROP TYPE IF EXISTS time_unit_type CASCADE;
            """
            
            cursor.execute(drop_sql)
            conn.commit()
            
            print("  All review tables, views, and types dropped successfully!")
            
        except Exception as e:
            if conn:
                conn.rollback()
            error_msg = f"Error cleaning review tables: {str(e)}"
            print(f"\nERROR: {error_msg}")
            self.stats['errors'].append(error_msg)
            raise
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()
    
    def ensure_account_schema(self) -> None:
        """
        Ensure account schema exists and project record is available
        This is required because review schema depends on project_users table
        """
        print("\n" + "="*80)
        print("CHECKING ACCOUNT SCHEMA AND PROJECT RECORD")
        print("="*80)
        
        conn = None
        cursor = None
        
        try:
            conn = self.data_access.get_connection()
            cursor = conn.cursor()
            
            # Check if project_users table exists
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'project_users'
                );
            """)
            table_exists = cursor.fetchone()[0]
            
            if table_exists:
                print("âœ“ Account schema already exists (project_users table found)")
            else:
                print("âœ— Account schema missing")
                print("Please run: python test_account_sync.py")
                raise Exception("Account schema not found. Please run account sync first.")
            
            # Check if project record exists
            cursor.execute("""
                SELECT EXISTS (
                    SELECT 1 FROM projects 
                    WHERE project_id = %s
                );
            """, [self.original_project_id])
            
            project_exists = cursor.fetchone()[0]
            
            if project_exists:
                print(f"âœ“ Project record exists: {self.original_project_id}")
            else:
                print(f"âœ— Project record missing: {self.original_project_id}")
                print("Please run: python test_account_sync.py")
                raise Exception(f"Project record not found: {self.original_project_id}")
            
            print("âœ“ All dependencies satisfied for review sync")
            
        except Exception as e:
            error_msg = f"Error checking account schema: {str(e)}"
            print(f"\nERROR: {error_msg}")
            self.stats['errors'].append(error_msg)
            raise
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()
    
    def recreate_review_schema(self) -> None:
        """
        Recreate review schema by executing review_system_schema.sql and additional schema files
        """
        print("\n" + "="*80)
        print("RECREATING REVIEW SCHEMA")
        print("="*80)
        
        conn = None
        cursor = None
        
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            
            # 1. Execute main review schema
            schema_file = os.path.join(current_dir, 'review_system_schema.sql')
            
            if not os.path.exists(schema_file):
                raise Exception(f"Schema file not found: {schema_file}")
            
            print(f"\n1. Reading main schema file: {schema_file}")
            
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_sql = f.read()
            
            print("   Executing main schema SQL...")
            
            conn = self.data_access.get_connection()
            cursor = conn.cursor()
            
            # Execute the main schema SQL as a single block to handle dollar-quoted strings properly
            try:
                cursor.execute(schema_sql)
                conn.commit()
            except Exception as e:
                print(f"   Warning: Schema execution had issues: {str(e)[:200]}...")
                # Try to continue anyway
                conn.rollback()
            
            print("   [OK] Main review schema created successfully!")
            
            # 2. Execute review_step_candidates schema
            candidates_schema_file = os.path.join(current_dir, 'create_review_step_candidates.sql')
            
            if os.path.exists(candidates_schema_file):
                print(f"\n2. Reading candidates schema file: {candidates_schema_file}")
                
                with open(candidates_schema_file, 'r', encoding='utf-8') as f:
                    candidates_sql = f.read()
                
                print("   Executing review_step_candidates schema SQL...")
                
                # Execute the candidates schema SQL
                cursor.execute(candidates_sql)
                conn.commit()
                
                print("   [OK] review_step_candidates schema created successfully!")
            else:
                print(f"\n2. [WARNING] Candidates schema file not found: {candidates_schema_file}")
                print("   [INFO] review_step_candidates table may not be created with proper constraints")
            
            print(f"\n[SUCCESS] Complete review schema recreated successfully!")
            
        except Exception as e:
            error_msg = f"Error recreating review schema: {str(e)}"
            print(f"\n[ERROR] {error_msg}")
            self.stats['errors'].append(error_msg)
            raise
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()
    
    def run_full_sync(self, clean_first: bool = True) -> Dict:
        """
        Run complete synchronization of all review data
        
        Args:
            clean_first: If True, drop and recreate all review tables before sync
        
        Returns:
            Sync statistics
        """
        print("\n" + "="*80)
        print("STARTING FULL REVIEW SYNCHRONIZATION")
        print("="*80)
        print(f"Project ID: {self.project_id}")
        print(f"Clean First: {clean_first}")
        print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        self.stats['start_time'] = datetime.now()
        
        try:
            # Step 0: Clean and recreate schema if requested
            if clean_first:
                self.clean_review_tables()
                self.recreate_review_schema()
            
            # Step 1: Sync workflows
            self.sync_all_workflows()
            
            # Step 2: Sync reviews (includes file versions and progress)
            self.sync_all_reviews()
            
            self.stats['end_time'] = datetime.now()
            
            # Print final summary
            self._print_final_summary()
            
            return self.stats
        
        except Exception as e:
            self.stats['end_time'] = datetime.now()
            error_msg = f"Fatal error during sync: {str(e)}"
            print(f"\nFATAL ERROR: {error_msg}")
            self.stats['errors'].append(error_msg)
            return self.stats
    
    def _print_final_summary(self):
        """Print final synchronization summary"""
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        print("\n" + "="*80)
        print("SYNCHRONIZATION COMPLETE")
        print("="*80)
        print(f"\nDuration: {duration:.2f} seconds")
        print(f"\nWorkflows:")
        print(f"  Total: {self.stats['workflows_total']}")
        print(f"  Synced: {self.stats['workflows_synced']}")
        print(f"\nReviews:")
        print(f"  Total: {self.stats['reviews_total']}")
        print(f"  Synced: {self.stats['reviews_synced']}")
        print(f"\nFile Versions:")
        print(f"  Total: {self.stats['file_versions_total']}")
        print(f"  Synced: {self.stats['file_versions_synced']}")
        print(f"\nProgress Steps:")
        print(f"  Total: {self.stats['progress_steps_total']}")
        print(f"  Synced: {self.stats['progress_steps_synced']}")
        print(f"\nWorkflow Templates:")
        print(f"  Total: {self.stats['workflow_templates_total']}")
        print(f"  Synced: {self.stats['workflow_templates_synced']}")
        
        if self.stats['errors']:
            print(f"\nErrors: {len(self.stats['errors'])}")
            print("\nFirst 10 errors:")
            for error in self.stats['errors'][:10]:
                print(f"  - {error}")
        else:
            print("\nNo errors occurred!")
        
        print("\n" + "="*80)


def main():
    """Main entry point for full review sync"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Full Review Synchronization - Syncs all review data from ACC to local database'
    )
    parser.add_argument(
        '--project-id', 
        default='b.1eea4119-3553-4167-b93d-3a3d5d07d33d',
        help='ACC Project ID (default: b.1eea4119-3553-4167-b93d-3a3d5d07d33d)'
    )
    parser.add_argument(
        '--token', 
        help='OAuth Access Token (optional, will use stored token if not provided)'
    )
    parser.add_argument(
        '--no-clean',
        action='store_true',
        help='Skip cleaning tables (incremental sync instead of full sync)'
    )
    
    args = parser.parse_args()
    
    try:
        # Create sync instance
        sync = FullReviewSync(args.project_id, args.token)
        
        # Run full sync (clean first unless --no-clean is specified)
        clean_first = not args.no_clean
        stats = sync.run_full_sync(clean_first=clean_first)
        
        # Save stats to file
        stats_file = f"review_sync_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            # Convert datetime objects to strings for JSON serialization
            stats_copy = stats.copy()
            if stats_copy['start_time']:
                stats_copy['start_time'] = stats_copy['start_time'].isoformat()
            if stats_copy['end_time']:
                stats_copy['end_time'] = stats_copy['end_time'].isoformat()
            
            json.dump(stats_copy, f, indent=2, ensure_ascii=False)
        
        print(f"\nStatistics saved to: {stats_file}")
        
        # Exit with appropriate code
        if stats['errors']:
            sys.exit(1)
        else:
            sys.exit(0)
    
    except Exception as e:
        print(f"\nFATAL ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

