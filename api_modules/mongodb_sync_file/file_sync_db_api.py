# -*- coding: utf-8 -*-
"""
æ–‡ä»¶åŒæ­¥æ•°æ®åº“APIæ¨¡å—
ä¸“é—¨å¤„ç†æ–‡ä»¶æ•°æ®çš„æ•°æ®åº“åŒæ­¥åŠŸèƒ½ï¼Œæ”¯æŒå…¨é‡åŒæ­¥å’Œå¢é‡åŒæ­¥
"""

import time
import logging
from datetime import datetime, timezone, timedelta
import pytz
from flask import Blueprint, jsonify, request
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, deque
from typing import Dict, List, Tuple, Any

import config
import utils
from database.data_access_layer import DataAccessLayer
from database.incremental_sync import IncrementalSyncManager
from database.data_sync_strategy import DataTransformer

# çµ±ä¸€çš„ä¸­åœ‹æ™‚å€å®šç¾©
CHINA_TZ = pytz.timezone('Asia/Shanghai')

# ============================================================================
# çµ±ä¸€æ™‚é–“è™•ç†å·¥å…·å‡½æ•¸
# ============================================================================

def get_china_time():
    """ç²å–ç•¶å‰ä¸­åœ‹æ™‚é–“"""
    return datetime.now(CHINA_TZ)

def parse_api_datetime(datetime_str):
    """è§£æAPIè¿”å›çš„æ—¥æœŸæ™‚é–“å­—ç¬¦ä¸²ï¼Œçµ±ä¸€è½‰æ›ç‚ºä¸­åœ‹æ™‚å€"""
    if not datetime_str:
        return None
    
    try:
        # ACC APIé€šå¸¸è¿”å›ISOæ ¼å¼çš„æ™‚é–“æˆ³
        if datetime_str.endswith('Z'):
            # UTCæ™‚é–“æˆ³ï¼Œè½‰æ›ç‚ºä¸­åœ‹æ™‚å€
            dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
            return dt.astimezone(CHINA_TZ)
        else:
            # å˜—è©¦ç›´æ¥è§£æ
            dt = datetime.fromisoformat(datetime_str)
            # å¦‚æœæ²’æœ‰æ™‚å€ä¿¡æ¯ï¼Œå‡è¨­ç‚ºUTCä¸¦è½‰æ›ç‚ºä¸­åœ‹æ™‚å€
            if dt.tzinfo is None:
                dt = pytz.utc.localize(dt)
            return dt.astimezone(CHINA_TZ)
    except Exception as e:
        logger.warning(f"è§£ææ—¥æœŸæ™‚é–“å¤±æ•—: {datetime_str}, {str(e)}")
        return None

def normalize_db_datetime(dt):
    """æ¨™æº–åŒ–æ•¸æ“šåº«ä¸­çš„æ—¥æœŸæ™‚é–“ï¼Œçµ±ä¸€è½‰æ›ç‚ºä¸­åœ‹æ™‚å€"""
    if not dt:
        return None
    
    try:
        # ä½¿ç”¨timestampé€²è¡Œè½‰æ›ï¼Œé¿å…æ™‚å€æ¯”è¼ƒå•é¡Œ
        if hasattr(dt, 'timestamp'):
            # å¦‚æœæœ‰timestampæ–¹æ³•ï¼Œä½¿ç”¨å®ƒ
            timestamp = dt.timestamp()
        elif hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
            # æœ‰æ™‚å€ä¿¡æ¯çš„datetime
            timestamp = dt.timestamp()
        else:
            # æ²’æœ‰æ™‚å€ä¿¡æ¯ï¼Œå‡è¨­ç‚ºUTC
            utc_dt = pytz.utc.localize(dt)
            timestamp = utc_dt.timestamp()
        
        # å¾timestampå‰µå»ºä¸­åœ‹æ™‚å€çš„datetime
        return datetime.fromtimestamp(timestamp, CHINA_TZ)
        
    except Exception as e:
        logger.warning(f"æ¨™æº–åŒ–æ—¥æœŸæ™‚é–“å¤±æ•—: {dt}, é¡å‹: {type(dt)}, éŒ¯èª¤: {str(e)}")
        try:
            # å˜—è©¦æ›¿ä»£æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨pytzè½‰æ›
            if hasattr(dt, 'tzinfo'):
                if dt.tzinfo is None:
                    return pytz.utc.localize(dt).astimezone(CHINA_TZ)
                else:
                    return dt.astimezone(CHINA_TZ)
            else:
                # MongoDB datetimeå°è±¡
                return pytz.utc.localize(dt).astimezone(CHINA_TZ)
        except Exception as e2:
            logger.error(f"æ‰€æœ‰æ™‚é–“è½‰æ›æ–¹æ³•éƒ½å¤±æ•—: {dt}, éŒ¯èª¤1: {str(e)}, éŒ¯èª¤2: {str(e2)}")
            # æœ€å¾Œçš„fallbackï¼šè¿”å›ç•¶å‰ä¸­åœ‹æ™‚é–“
            return get_china_time()

def format_china_time(dt):
    """æ ¼å¼åŒ–ä¸­åœ‹æ™‚é–“é¡¯ç¤º"""
    if not dt:
        return None
    
    # ç¢ºä¿æ˜¯ä¸­åœ‹æ™‚å€
    if hasattr(dt, 'tzinfo') and dt.tzinfo != CHINA_TZ:
        dt = dt.astimezone(CHINA_TZ)
    
    return dt.strftime('%Y-%m-%d %H:%M:%S')

def debug_datetime_object(dt, name="datetime"):
    """èª¿è©¦æ—¥æœŸæ™‚é–“å°è±¡çš„è©³ç´°ä¿¡æ¯"""
    if dt is None:
        logger.debug(f"ğŸ” {name}: None")
        return
    
    logger.debug(f"ğŸ” {name} è©³ç´°ä¿¡æ¯:")
    logger.debug(f"   é¡å‹: {type(dt)}")
    logger.debug(f"   å€¼: {dt}")
    logger.debug(f"   å­—ç¬¦ä¸²è¡¨ç¤º: {str(dt)}")
    logger.debug(f"   repr: {repr(dt)}")
    
    if hasattr(dt, 'tzinfo'):
        logger.debug(f"   tzinfo: {dt.tzinfo}")
        logger.debug(f"   tzinfoé¡å‹: {type(dt.tzinfo)}")
        if dt.tzinfo:
            logger.debug(f"   tzinfoåç¨±: {getattr(dt.tzinfo, 'zone', 'Unknown')}")
    else:
        logger.debug(f"   æ²’æœ‰tzinfoå±¬æ€§")
    
    if hasattr(dt, 'timestamp'):
        try:
            logger.debug(f"   timestamp: {dt.timestamp()}")
        except Exception as e:
            logger.debug(f"   timestampéŒ¯èª¤: {e}")
    
    # å˜—è©¦è½‰æ›ç‚ºä¸åŒæ ¼å¼
    try:
        logger.debug(f"   isoformat: {dt.isoformat()}")
    except Exception as e:
        logger.debug(f"   isoformatéŒ¯èª¤: {e}")
        
    logger.debug(f"   ---")

from .file_sync_api import (
    get_project_top_folders, 
    get_folder_contents, 
    get_item_versions,
    batch_get_files_custom_attributes,
    get_folder_custom_attribute_definitions
)

# åˆ›å»ºBlueprint
file_sync_db_bp = Blueprint('file_sync_db', __name__)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FileSyncDatabaseManager:
    """æ–‡ä»¶åŒæ­¥æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.dal = DataAccessLayer()
        self.sync_manager = IncrementalSyncManager(self.dal)
        self.converter = DataTransformer()
        
    def full_sync_project(self, project_id: str, max_depth: int = 10, 
                         include_custom_attributes: bool = True) -> dict:
        """
        é¡¹ç›®å…¨é‡åŒæ­¥åˆ°æ•°æ®åº“
        
        Args:
            project_id: é¡¹ç›®ID
            max_depth: æœ€å¤§éå†æ·±åº¦
            include_custom_attributes: æ˜¯å¦åŒ…å«è‡ªå®šä¹‰å±æ€§
            
        Returns:
            åŒæ­¥ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        try:
            # 1. åˆ›å»ºåŒæ­¥ä»»åŠ¡è®°å½•
            task_id = self._create_sync_task(project_id, "full_sync", {
                "max_depth": max_depth,
                "include_custom_attributes": include_custom_attributes
            })
            
            # 2. ğŸ§¹ æ¸…é™¤é …ç›®ç¾æœ‰æ•¸æ“šï¼ˆå…¨é‡åŒæ­¥çš„é—œéµæ­¥é©Ÿï¼‰
            logger.info(f"ğŸ§¹ å…¨é‡åŒæ­¥ç¬¬ä¸€æ­¥ï¼šæ¸…é™¤é …ç›® {project_id} çš„ç¾æœ‰æ•¸æ“š")
            clear_stats = self.dal.clear_project_data(project_id)
            logger.info(f"æ•¸æ“šæ¸…ç†å®Œæˆ: {clear_stats}")
            
            # 3. è·å–APIè®¿é—®ä»¤ç‰Œ
            access_token = utils.get_access_token()
            if not access_token:
                raise Exception("æ— æ³•è·å–è®¿é—®ä»¤ç‰Œ")
            
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            # 4. è·å–é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            logger.info(f"é–‹å§‹é‡æ–°åŒæ­¥é …ç›®: {project_id}")
            
            # å¢å¼ºçš„é‡è¯•æœºåˆ¶
            max_retries = 5
            retry_delay = 3
            top_folders_data = None
            
            for attempt in range(max_retries):
                try:
                    logger.info(f"å°è¯•è·å–é¡¶çº§æ–‡ä»¶å¤¹ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")
                    top_folders_data = get_project_top_folders(project_id, headers)
                    
                    if top_folders_data and top_folders_data.get('data'):
                        logger.info(f"âœ… æˆåŠŸè·å–é¡¶çº§æ–‡ä»¶å¤¹ (å°è¯• {attempt + 1})")
                        break
                    else:
                        logger.warning(f"âš ï¸ è·å–åˆ°ç©ºçš„é¡¶çº§æ–‡ä»¶å¤¹æ•°æ® (å°è¯• {attempt + 1})")
                        if attempt < max_retries - 1:
                            logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                            retry_delay *= 1.5  # é€’å¢å»¶è¿Ÿ
                            continue
                        
                except Exception as e:
                    logger.error(f"âŒ è·å–é¡¶çº§æ–‡ä»¶å¤¹å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
                    if attempt < max_retries - 1:
                        logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        retry_delay *= 1.5
                        continue
                    else:
                        raise Exception(f"ç»è¿‡ {max_retries} æ¬¡é‡è¯•åä»æ— æ³•è·å–é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹: {str(e)}")
            
            if not top_folders_data or not top_folders_data.get('data'):
                raise Exception(f"ç»è¿‡ {max_retries} æ¬¡é‡è¯•åä»æ— æ³•è·å–æœ‰æ•ˆçš„é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹æ•°æ®")
            
            # 5. åˆå§‹åŒ–é¡¹ç›®è®°å½•
            project_data = DataTransformer.transform_project_data({
                "id": project_id,
                "attributes": {
                    "name": f"Project {project_id}"
                }
            })
            
            self.dal.create_or_update_project(project_data)
            
            # 6. é€’å½’åŒæ­¥æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
            sync_results = {
                "folders_synced": 0,
                "files_synced": 0,
                "versions_synced": 0,
                "errors": [],
                "total_size": 0
            }
            
            for top_folder in top_folders_data.get('data', []):
                folder_result = self._sync_folder_recursive(
                    project_id, top_folder, headers, 
                    max_depth, 0, include_custom_attributes, None
                )
                
                sync_results["folders_synced"] += folder_result["folders_synced"]
                sync_results["files_synced"] += folder_result["files_synced"]
                sync_results["versions_synced"] += folder_result["versions_synced"]
                sync_results["total_size"] += folder_result["total_size"]
                sync_results["errors"].extend(folder_result["errors"])
            
            # 7. æ›´æ–°é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯
            duration = time.time() - start_time
            self._update_project_statistics(project_id, sync_results, duration)
            
            # 8. å®ŒæˆåŒæ­¥ä»»åŠ¡
            self._complete_sync_task(task_id, sync_results, duration)
            
            logger.info(f"å…¨é‡åŒæ­¥å®Œæˆ: {sync_results}")
            return {
                "success": True,
                "task_id": task_id,
                "duration_seconds": duration,
                "results": sync_results
            }
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}")
            
            # æ›´æ–°å¤±è´¥çŠ¶æ€
            if 'task_id' in locals():
                self._fail_sync_task(task_id, str(e), duration)
            
            return {
                "success": False,
                "error": str(e),
                "duration_seconds": duration
            }
    
    def incremental_sync_project(self, project_id: str) -> dict:
        """
        é¡¹ç›®å¢é‡åŒæ­¥åˆ°æ•°æ®åº“
        
        Args:
            project_id: é¡¹ç›®ID
            
        Returns:
            åŒæ­¥ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        try:
            # 1. è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            last_sync_time = self.sync_manager.get_last_sync_time(project_id)
            logger.info(f"å¼€å§‹å¢é‡åŒæ­¥é¡¹ç›®: {project_id}, ä¸Šæ¬¡åŒæ­¥: {last_sync_time}")
            
            # 2. æ£€æµ‹å˜æ›´
            changes = self.sync_manager.detect_changes(project_id, last_sync_time)
            
            if not any(changes.values()):
                logger.info("æ²¡æœ‰æ£€æµ‹åˆ°å˜æ›´ï¼Œè·³è¿‡åŒæ­¥")
                return {
                    "success": True,
                    "message": "æ²¡æœ‰å˜æ›´éœ€è¦åŒæ­¥",
                    "changes": changes
                }
            
            # 3. åˆ›å»ºå¢é‡åŒæ­¥ä»»åŠ¡
            task_id = self._create_sync_task(project_id, "incremental_sync", {
                "since": last_sync_time.isoformat(),
                "detected_changes": changes
            })
            
            # 4. æ‰§è¡Œå¢é‡åŒæ­¥
            access_token = utils.get_access_token()
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            sync_results = self.sync_manager.incremental_sync_folders(
                project_id, changes["new_folders"] + changes["updated_folders"], headers
            )
            
            file_results = self.sync_manager.incremental_sync_files(
                project_id, changes["new_files"] + changes["updated_files"], headers
            )
            
            # 5. åˆå¹¶ç»“æœ
            total_results = {
                "folders_synced": sync_results.get("new", 0) + sync_results.get("updated", 0),
                "files_synced": file_results.get("new", 0) + file_results.get("updated", 0), 
                "versions_synced": 0,  # å¢é‡åŒæ­¥æš‚ä¸æ”¯æŒç‰ˆæœ¬åŒæ­¥
                "errors": []  # ç®€åŒ–é”™è¯¯å¤„ç†
            }
            
            # 6. å®Œæˆä»»åŠ¡
            duration = time.time() - start_time
            self._complete_sync_task(task_id, total_results, duration)
            
            logger.info(f"å¢é‡åŒæ­¥å®Œæˆ: {total_results}")
            return {
                "success": True,
                "task_id": task_id,
                "duration_seconds": duration,
                "changes": changes,
                "results": total_results
            }
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"å¢é‡åŒæ­¥å¤±è´¥: {str(e)}")
            
            if 'task_id' in locals():
                self._fail_sync_task(task_id, str(e), duration)
            
            return {
                "success": False,
                "error": str(e),
                "duration_seconds": duration
            }
    
    def _sync_folder_recursive(self, project_id: str, folder_data: dict, headers: dict,
                              max_depth: int, current_depth: int, 
                              include_custom_attributes: bool, parent_path: str) -> dict:
        """é€’å½’åŒæ­¥æ–‡ä»¶å¤¹"""
        
        if current_depth >= max_depth:
            return {"folders_synced": 0, "files_synced": 0, "versions_synced": 0, "errors": [], "total_size": 0}
        
        try:
            folder_id = folder_data.get('id')
            folder_attributes = folder_data.get('attributes', {})
            
            # æ„å»ºè·¯å¾„ä¿¡æ¯
            folder_name = folder_attributes.get('displayName', folder_attributes.get('name', 'Unknown'))
            current_path = f"{parent_path}/{folder_name}" if parent_path else folder_name
            
            logger.info(f"åŒæ­¥æ–‡ä»¶å¤¹: {folder_name} (æ·±åº¦: {current_depth})")
            
            # è½¬æ¢æ–‡ä»¶å¤¹æ•°æ®
            # æ­£ç¡®çš„å‚æ•°: api_data, project_id, parent_id=None, path="", depth=0
            parent_folder_id = folder_data.get('relationships', {}).get('parent', {}).get('data', {}).get('id') if parent_path else None
            folder_doc = DataTransformer.transform_folder_data(
                folder_data, project_id, parent_folder_id, parent_path or "", current_depth
            )
            
            # è·å–è‡ªå®šä¹‰å±æ€§å®šä¹‰ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
            if include_custom_attributes:
                try:
                    custom_attrs = get_folder_custom_attribute_definitions(project_id, folder_id)
                    if custom_attrs:
                        folder_doc["custom_attribute_definitions"] = custom_attrs
                except Exception as e:
                    logger.warning(f"è·å–æ–‡ä»¶å¤¹è‡ªå®šä¹‰å±æ€§å¤±è´¥ {folder_name}: {str(e)}")
            
            # ä¿å­˜æ–‡ä»¶å¤¹åˆ°æ•°æ®åº“
            self.dal.create_or_update_folder(folder_doc)
            
            results = {"folders_synced": 1, "files_synced": 0, "versions_synced": 0, "errors": [], "total_size": 0}
            
            # è·å–æ–‡ä»¶å¤¹å†…å®¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            max_retries = 3
            contents_data = None
            
            for attempt in range(max_retries):
                try:
                    contents_data = get_folder_contents(project_id, folder_id, headers)
                    if contents_data:
                        break
                except Exception as e:
                    logger.warning(f"è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥ {folder_name} (å°è¯• {attempt + 1}): {str(e)}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        continue
                    else:
                        logger.error(f"è·å–æ–‡ä»¶å¤¹å†…å®¹æœ€ç»ˆå¤±è´¥ {folder_name}: {str(e)}")
                        results["errors"].append({
                            "folder_id": folder_id,
                            "error": f"æ— æ³•è·å–æ–‡ä»¶å¤¹å†…å®¹: {str(e)}"
                        })
                        return results
            
            if not contents_data:
                logger.warning(f"æ–‡ä»¶å¤¹å†…å®¹ä¸ºç©º: {folder_name}")
                return results
            
            # åˆ†ç¦»æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
            subfolders = []
            files = []
            
            for item in contents_data.get('data', []):
                if item.get('type') == 'folders':
                    subfolders.append(item)
                else:
                    files.append(item)
            
            # é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            for subfolder in subfolders:
                subfolder_result = self._sync_folder_recursive(
                    project_id, subfolder, headers, max_depth, 
                    current_depth + 1, include_custom_attributes, current_path
                )
                
                results["folders_synced"] += subfolder_result["folders_synced"]
                results["files_synced"] += subfolder_result["files_synced"]
                results["versions_synced"] += subfolder_result["versions_synced"]
                results["total_size"] += subfolder_result["total_size"]
                results["errors"].extend(subfolder_result["errors"])
            
            # æ‰¹é‡å¤„ç†æ–‡ä»¶
            if files:
                file_result = self._sync_files_batch(
                    project_id, files, headers, folder_id, current_path, include_custom_attributes, current_depth
                )
                
                results["files_synced"] += file_result["files_synced"]
                results["versions_synced"] += file_result["versions_synced"]
                results["total_size"] += file_result["total_size"]
                results["errors"].extend(file_result["errors"])
            
            return results
            
        except Exception as e:
            logger.error(f"åŒæ­¥æ–‡ä»¶å¤¹å¤±è´¥ {folder_data.get('id', 'unknown')}: {str(e)}")
            return {
                "folders_synced": 0, "files_synced": 0, "versions_synced": 0, 
                "errors": [{"folder_id": folder_data.get('id'), "error": str(e)}],
                "total_size": 0
            }
    
    def _sync_files_batch(self, project_id: str, files: list, headers: dict,
                         parent_folder_id: str, folder_path: str, 
                         include_custom_attributes: bool, current_depth: int = 0) -> dict:
        """æ‰¹é‡åŒæ­¥æ–‡ä»¶"""
        
        results = {"files_synced": 0, "versions_synced": 0, "total_size": 0, "errors": []}
        
        try:
            # æ‰¹é‡è·å–æ–‡ä»¶ç‰ˆæœ¬
            file_ids = [f.get('id') for f in files if f.get('id')]
            
            # å¹¶è¡Œè·å–ç‰ˆæœ¬ä¿¡æ¯
            versions_data = {}
            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_file = {
                    executor.submit(get_item_versions, project_id, file_id, headers): file_id
                    for file_id in file_ids
                }
                
                for future in as_completed(future_to_file):
                    file_id = future_to_file[future]
                    try:
                        versions = future.result()
                        versions_data[file_id] = versions
                    except Exception as e:
                        logger.error(f"è·å–æ–‡ä»¶ç‰ˆæœ¬å¤±è´¥ {file_id}: {str(e)}")
                        results["errors"].append({"file_id": file_id, "error": str(e)})
            
            # æ‰¹é‡è·å–è‡ªå®šä¹‰å±æ€§
            custom_attributes_data = {}
            if include_custom_attributes:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶èŠ‚ç‚¹ç”¨äºè‡ªå®šä¹‰å±æ€§è·å–
                temp_file_nodes = []
                for file_data in files:
                    file_id = file_data.get('id')
                    if file_id in versions_data:
                        temp_node = type('FileNode', (), {
                            'versions': versions_data[file_id],
                            'name': file_data.get('attributes', {}).get('displayName', 'Unknown')
                        })()
                        temp_file_nodes.append(temp_node)
                
                if temp_file_nodes:
                    custom_attributes_data = batch_get_files_custom_attributes(project_id, temp_file_nodes)
            
            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            for file_data in files:
                try:
                    file_id = file_data.get('id')
                    versions = versions_data.get(file_id, [])
                    
                    # è½¬æ¢æ–‡ä»¶æ•°æ®
                    # æ­£ç¡®çš„å‚æ•°: api_data, project_id, parent_folder_id, folder_path, depth=0
                    file_doc = DataTransformer.transform_file_data(
                        file_data, project_id, parent_folder_id, folder_path, current_depth
                    )
                    
                    # æ·»åŠ è‡ªå®šä¹‰å±æ€§
                    if include_custom_attributes and versions:
                        latest_version = versions[0]
                        version_id = latest_version.get('id')
                        if version_id in custom_attributes_data:
                            file_doc["custom_attributes"] = custom_attributes_data[version_id]
                    
                    # ä¿å­˜æ–‡ä»¶åˆ°æ•°æ®åº“
                    self.dal.create_or_update_file(file_doc)
                    results["files_synced"] += 1
                    
                    # ä¿å­˜æ–‡ä»¶ç‰ˆæœ¬
                    for version in versions:
                        version_doc = {
                            "_id": version.get("id"),
                            "file_id": file_id,
                            "project_id": project_id,
                            "urn": version.get("id"),
                            "version_number": version.get("attributes", {}).get("versionNumber", 1),
                            "display_name": version.get("attributes", {}).get("displayName", ""),
                            "file_size": version.get("attributes", {}).get("storageSize", 0),
                            "created_at": datetime.now(),
                            "updated_at": datetime.now()
                        }
                        self.dal.create_or_update_file_version(version_doc)
                        results["versions_synced"] += 1
                    
                    # ç´¯è®¡æ–‡ä»¶å¤§å° - ä¿®å¤total_sizeè®¡ç®—é—®é¢˜
                    if versions:
                        latest_version = versions[0]
                        # ä»ç‰ˆæœ¬æ•°æ®çš„attributes.storageSizeè·å–æ–‡ä»¶å¤§å°
                        file_size = (
                            latest_version.get('attributes', {}).get('storageSize', 0) or
                            latest_version.get('attributes', {}).get('fileSize', 0) or
                            0
                        )
                        if file_size and file_size > 0:
                            results["total_size"] += file_size
                            logger.debug(f"æ–‡ä»¶ {file_data.get('id')} å¤§å°: {file_size} bytes")
                    
                except Exception as e:
                    logger.error(f"åŒæ­¥æ–‡ä»¶å¤±è´¥ {file_data.get('id', 'unknown')}: {str(e)}")
                    results["errors"].append({
                        "file_id": file_data.get('id'),
                        "error": str(e)
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åŒæ­¥æ–‡ä»¶å¤±è´¥: {str(e)}")
            return {
                "files_synced": 0, "versions_synced": 0, "total_size": 0,
                "errors": [{"error": f"æ‰¹é‡åŒæ­¥å¤±è´¥: {str(e)}"}]
            }
    
    def _create_sync_task(self, project_id: str, task_type: str, parameters: dict) -> str:
        """åˆ›å»ºåŒæ­¥ä»»åŠ¡è®°å½•"""
        task_data = {
            "project_id": project_id,
            "task_type": task_type,
            "task_status": "running",
            "parameters": parameters,
            "progress": {
                "total_items": 0,
                "processed_items": 0,
                "success_count": 0,
                "error_count": 0,
                "current_stage": "initializing",
                "progress_percentage": 0.0
            },
            "start_time": datetime.now()
        }
        
        return self.dal.create_sync_task(task_data)
    
    def _complete_sync_task(self, task_id: str, results: dict, duration: float):
        """å®ŒæˆåŒæ­¥ä»»åŠ¡"""
        self.dal.update_sync_task_status(task_id, "completed", results, duration)
    
    def _record_successful_sync(self, project_id: str, sync_type: str, results: dict, duration: float):
        """è®°å½•æˆåŠŸçš„åŒæ­¥åˆ°ç®€åŒ–çš„å†å²è®°å½•"""
        try:
            # æ·»åŠ æŒç»­æ—¶é—´åˆ°ç»“æœä¸­
            results_with_duration = results.copy()
            results_with_duration["duration_seconds"] = duration
            
            # åˆ›å»ºç®€åŒ–çš„åŒæ­¥å†å²è®°å½•
            success = self.dal.create_sync_history_record(project_id, sync_type, results_with_duration)
            if success:
                logger.info(f"æˆåŠŸè®°å½•åŒæ­¥å†å²: {project_id} - {sync_type}")
            else:
                logger.warning(f"è®°å½•åŒæ­¥å†å²å¤±è´¥: {project_id} - {sync_type}")
                
        except Exception as e:
            logger.error(f"è®°å½•åŒæ­¥å†å²æ—¶å‡ºé”™: {str(e)}")
    
    def _fail_sync_task(self, task_id: str, error: str, duration: float):
        """æ ‡è®°åŒæ­¥ä»»åŠ¡å¤±è´¥"""
        results = {"error": error}
        self.dal.update_sync_task_status(task_id, "failed", results, duration)
    
    def _update_project_statistics(self, project_id: str, sync_results: dict, duration: float):
        """æ›´æ–°é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_folders": sync_results["folders_synced"],
            "total_files": sync_results["files_synced"],
            "total_size_bytes": sync_results["total_size"],
            "last_calculated": datetime.now()
        }
        
        self.dal.update_project_sync_status(project_id, "success", duration)
        self.dal.update_project_statistics(project_id, stats)


# å…¨å±€ç®¡ç†å™¨å®ä¾‹
sync_manager = FileSyncDatabaseManager()


# ============================================================================
# æ‰¹é‡ä¼˜åŒ–åŒæ­¥ç®¡ç†å™¨
# ============================================================================

class BatchOptimizedSyncManager:
    """æ‰¹é‡ä¼˜åŒ–çš„åŒæ­¥ç®¡ç†å™¨ - ä¸“æ³¨äºmetadataåŒæ­¥æ€§èƒ½ä¼˜åŒ–"""
    
    def __init__(self, batch_size: int = 100, api_delay: float = 0.2):
        self.dal = DataAccessLayer()
        self.batch_size = batch_size
        self.api_delay = api_delay  # APIè°ƒç”¨é—´éš”ï¼Œé¿å…é€Ÿç‡é™åˆ¶
        self.converter = DataTransformer()
        
    def batch_sync_project(self, project_id: str, max_depth: int = 10, 
                          include_custom_attributes: bool = True, task_id: str = None,
                          is_full_sync: bool = False) -> dict:
        """
        æ‰¹é‡ä¼˜åŒ–çš„é¡¹ç›®åŒæ­¥
        
        æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥ï¼š
        1. å¹¿åº¦ä¼˜å…ˆéå†ï¼Œé¿å…æ·±åº¦é€’å½’
        2. æ‰¹é‡APIè°ƒç”¨ï¼Œå‡å°‘ç½‘ç»œå¼€é”€
        3. æ‰¹é‡æ•°æ®åº“æ“ä½œï¼Œæå‡å†™å…¥æ€§èƒ½
        4. æ™ºèƒ½APIèŠ‚æµï¼Œé¿å…é€Ÿç‡é™åˆ¶
        
        Args:
            project_id: é …ç›®ID
            max_depth: æœ€å¤§éæ­·æ·±åº¦
            include_custom_attributes: æ˜¯å¦åŒ…å«è‡ªå®šç¾©å±¬æ€§
            task_id: ä»»å‹™ID
            is_full_sync: æ˜¯å¦ç‚ºå…¨é‡åŒæ­¥ï¼ˆæœƒå…ˆæ¸…é™¤ç¾æœ‰æ•¸æ“šï¼‰
        """
        start_time = time.time()
        
        try:
            # 1. åˆ›å»ºæˆ–ä½¿ç”¨ç°æœ‰åŒæ­¥ä»»åŠ¡è®°å½•
            if task_id is None:
                sync_type = "full_sync" if is_full_sync else "batch_optimized_sync"
                task_id = self._create_sync_task(project_id, sync_type, {
                    "max_depth": max_depth,
                    "include_custom_attributes": include_custom_attributes,
                    "batch_size": self.batch_size,
                    "is_full_sync": is_full_sync
                })
            
            # 2. ğŸ§¹ å¦‚æœæ˜¯å…¨é‡åŒæ­¥ï¼Œå…ˆæ¸…é™¤é …ç›®ç¾æœ‰æ•¸æ“š
            if is_full_sync:
                logger.info(f"ğŸ§¹ å…¨é‡åŒæ­¥æ¨¡å¼ï¼šæ¸…é™¤é …ç›® {project_id} çš„ç¾æœ‰æ•¸æ“š")
                clear_stats = self.dal.clear_project_data(project_id)
                logger.info(f"æ•¸æ“šæ¸…ç†å®Œæˆ: {clear_stats}")
            
            # 3. è·å–APIè®¿é—®ä»¤ç‰Œ
            access_token = utils.get_access_token()
            if not access_token:
                raise Exception("æ— æ³•è·å–è®¿é—®ä»¤ç‰Œ")
            
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            # 3. è·å–é¡¶çº§æ–‡ä»¶å¤¹
            logger.info(f"å¼€å§‹æ‰¹é‡ä¼˜åŒ–åŒæ­¥é¡¹ç›®: {project_id}")
            top_folders_data = self._get_top_folders_with_retry(project_id, headers)
            
            # 4. æ‰¹é‡æ”¶é›†æ‰€æœ‰é¡¹ç›®metadata
            self._update_sync_task_progress(task_id, {
                "current_stage": "collecting",
                "progress_percentage": 50.0,  # æ”¶é›†é˜¶æ®µå†…éƒ¨è¿›åº¦
                "processed_items": 0,
                "total_items": 0
            })
            
            all_folders, all_files = self._collect_all_items_bfs(
                project_id, top_folders_data['data'], headers, max_depth, task_id
            )
            
            # æ›´æ–°æ”¶é›†å®Œæˆï¼Œå¼€å§‹æ–‡ä»¶å¤¹å¤„ç†
            self._update_sync_task_progress(task_id, {
                "current_stage": "processing_folders",
                "progress_percentage": 0.0,  # æ–‡ä»¶å¤¹å¤„ç†é˜¶æ®µå¼€å§‹
                "processed_items": 0,
                "total_items": len(all_folders) + len(all_files),
                "total_folders": len(all_folders),
                "total_files": len(all_files)
            })
            
            # 5. æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ï¼ˆæŒ‰å±‚çº§é¡ºåºï¼‰
            folder_results = self._batch_process_folders(
                all_folders, project_id, include_custom_attributes, headers, task_id
            )
            
            # 6. æ‰¹é‡å¤„ç†æ–‡ä»¶
            self._update_sync_task_progress(task_id, {
                "current_stage": "processing_files",
                "progress_percentage": 0.0,  # æ–‡ä»¶å¤„ç†é˜¶æ®µå¼€å§‹
                "folders_processed": folder_results["success_count"],
                "files_processed": 0,
                "total_items": len(all_folders) + len(all_files)
            })
            
            file_results = self._batch_process_files(
                all_files, project_id, include_custom_attributes, headers, task_id
            )
            
            # 7. åˆå¹¶ç»“æœ
            sync_results = {
                "folders_synced": folder_results["success_count"],
                "files_synced": file_results["files_synced"],
                "versions_synced": file_results["versions_synced"],
                "total_size": file_results["total_size"],
                "errors": folder_results["errors"] + file_results["errors"]
            }
            
            # 8. å®ŒæˆåŒæ­¥
            self._update_sync_task_progress(task_id, {
                "current_stage": "finalizing",
                "progress_percentage": 100.0,  # æœ€ç»ˆåŒ–é˜¶æ®µå®Œæˆ
                "folders_processed": folder_results["success_count"],
                "files_processed": file_results["files_synced"],
                "versions_processed": file_results["versions_synced"],
                "total_items": len(all_folders) + len(all_files)
            })
            
            duration = time.time() - start_time
            self._update_project_statistics(project_id, sync_results, duration)
            self._complete_sync_task(task_id, sync_results, duration)
            
            # é€šçŸ¥ä»»å‹™è¿½è¹¤ç³»çµ±ä»»å‹™å®Œæˆ
            try:
                from api_modules.task_lifecycle_manager import task_manager
                task_manager.complete_task(task_id, sync_results)
            except ImportError:
                logger.warning("Task tracking system not available")
            
            logger.info(f"æ‰¹é‡ä¼˜åŒ–åŒæ­¥å®Œæˆ: {sync_results}")
            return {
                "success": True,
                "task_id": task_id,
                "duration_seconds": duration,
                "results": sync_results
            }
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"æ‰¹é‡ä¼˜åŒ–åŒæ­¥å¤±è´¥: {str(e)}")
            
            if 'task_id' in locals():
                self._fail_sync_task(task_id, str(e), duration)
                # é€šçŸ¥ä»»å‹™è¿½è¹¤ç³»çµ±ä»»å‹™å¤±æ•—
                try:
                    from api_modules.task_lifecycle_manager import task_manager
                    task_manager.fail_task(task_id, str(e))
                except ImportError:
                    logger.warning("Task tracking system not available")
            
            return {
                "success": False,
                "error": str(e),
                "duration_seconds": duration
            }
    
    def _collect_all_items_bfs(self, project_id: str, top_folders: List[dict], 
                              headers: dict, max_depth: int, task_id: str = None) -> Tuple[List, List]:
        """
        å¹¿åº¦ä¼˜å…ˆæ”¶é›†æ‰€æœ‰é¡¹ç›®metadata
        
        ä¼˜åŠ¿ï¼š
        - é¿å…é€’å½’è°ƒç”¨æ ˆé—®é¢˜
        - å¯ä»¥é¢„å…ˆçŸ¥é“æ€»é‡ï¼Œä¾¿äºè¿›åº¦è¿½è¸ª
        - å‡å°‘APIè°ƒç”¨çš„åµŒå¥—å¤æ‚åº¦
        """
        all_folders = []
        all_files = []
        
        # ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œå¹¿åº¦ä¼˜å…ˆéå†
        # é˜Ÿåˆ—å…ƒç´ : (folder_data, depth, parent_path)
        folder_queue = deque()
        
        # åˆå§‹åŒ–é˜Ÿåˆ—
        for folder in top_folders:
            folder_queue.append((folder, 0, ""))
        
        processed_folders = 0
        total_api_calls = 0
        
        while folder_queue:
            folder_data, depth, parent_path = folder_queue.popleft()
            
            if depth >= max_depth:
                continue
            
            folder_id = folder_data.get('id')
            folder_name = folder_data.get('attributes', {}).get('displayName', 'Unknown')
            current_path = f"{parent_path}/{folder_name}" if parent_path else folder_name
            
            # æ·»åŠ åˆ°æ–‡ä»¶å¤¹åˆ—è¡¨ï¼ˆå¸¦å±‚çº§ä¿¡æ¯ï¼‰
            all_folders.append((folder_data, depth, parent_path))
            
            # APIèŠ‚æµ - å…³é”®ä¼˜åŒ–ç‚¹
            if total_api_calls > 0 and total_api_calls % 10 == 0:
                logger.info(f"APIèŠ‚æµæš‚åœ {self.api_delay} ç§’ (å·²è°ƒç”¨ {total_api_calls} æ¬¡)")
                time.sleep(self.api_delay)
            
            try:
                # è·å–æ–‡ä»¶å¤¹å†…å®¹
                logger.info(f"æ”¶é›†æ–‡ä»¶å¤¹å†…å®¹: {folder_name} (æ·±åº¦: {depth})")
                contents_data = get_folder_contents(project_id, folder_id, headers)
                total_api_calls += 1
                
                if not contents_data or not contents_data.get('data'):
                    continue
                
                # åˆ†ç¦»æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
                for item in contents_data['data']:
                    item_type = item.get('type')
                    
                    if item_type == 'folders':
                        # æ·»åŠ å­æ–‡ä»¶å¤¹åˆ°é˜Ÿåˆ—
                        folder_queue.append((item, depth + 1, current_path))
                    else:
                        # æ·»åŠ æ–‡ä»¶åˆ°åˆ—è¡¨ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
                        all_files.append((item, folder_id, current_path, depth))
                
                processed_folders += 1
                
                # è¿›åº¦æ—¥å¿—å’Œæ•°æ®åº“æ›´æ–°
                if processed_folders % 10 == 0:
                    logger.info(f"å·²æ”¶é›† {processed_folders} ä¸ªæ–‡ä»¶å¤¹, {len(all_files)} ä¸ªæ–‡ä»¶")
                    
                    # æ›´æ–°æ”¶é›†è¿›åº¦åˆ°æ•°æ®åº“
                    if task_id:
                        # ä¼°ç®—æ”¶é›†è¿›åº¦ï¼ˆåŸºäºå·²å¤„ç†çš„æ–‡ä»¶å¤¹æ•°é‡ï¼‰
                        estimated_total = max(len(top_folders) * 5, processed_folders + len(folder_queue))
                        collect_progress = min((processed_folders / estimated_total) * 100, 90)
                        
                        self._update_sync_task_progress(task_id, {
                            "current_stage": "collecting",
                            "progress_percentage": collect_progress,
                            "processed_folders": processed_folders,
                            "collected_files": len(all_files),
                            "queue_remaining": len(folder_queue)
                        })
                
            except Exception as e:
                logger.error(f"æ”¶é›†æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥ {folder_name}: {str(e)}")
                continue
        
        # æ”¶é›†å®Œæˆï¼Œæ›´æ–°æœ€ç»ˆè¿›åº¦
        if task_id:
            self._update_sync_task_progress(task_id, {
                "current_stage": "collecting",
                "progress_percentage": 100.0,
                "processed_folders": processed_folders,
                "collected_files": len(all_files),
                "total_folders_found": len(all_folders),
                "total_files_found": len(all_files)
            })
        
        logger.info(f"æ”¶é›†å®Œæˆ: {len(all_folders)} ä¸ªæ–‡ä»¶å¤¹, {len(all_files)} ä¸ªæ–‡ä»¶, {total_api_calls} æ¬¡APIè°ƒç”¨")
        return all_folders, all_files
    
    def _batch_process_folders(self, folders_with_context: List[Tuple], project_id: str,
                              include_custom_attributes: bool, headers: dict, task_id: str) -> dict:
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹metadata
        
        å…³é”®ä¼˜åŒ–ï¼š
        1. æŒ‰æ·±åº¦æ’åºï¼Œç¡®ä¿çˆ¶æ–‡ä»¶å¤¹å…ˆå¤„ç†
        2. æ‰¹é‡æ•°æ®åº“æ“ä½œ
        3. å¯é€‰çš„è‡ªå®šä¹‰å±æ€§æ‰¹é‡è·å–
        """
        results = {"success_count": 0, "errors": []}
        
        # æŒ‰æ·±åº¦æ’åºï¼Œç¡®ä¿ä¾èµ–å…³ç³»æ­£ç¡®
        folders_with_context.sort(key=lambda x: x[1])  # æŒ‰depthæ’åº
        
        batch_docs = []
        processed_count = 0
        
        for folder_data, depth, parent_path in folders_with_context:
            try:
                # è½¬æ¢æ–‡ä»¶å¤¹æ•°æ®
                parent_folder_id = folder_data.get('relationships', {}).get('parent', {}).get('data', {}).get('id') if parent_path else None
                folder_doc = DataTransformer.transform_folder_data(
                    folder_data, project_id, parent_folder_id, parent_path, depth
                )
                
                # è·å–è‡ªå®šä¹‰å±æ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if include_custom_attributes:
                    try:
                        folder_id = folder_data.get('id')
                        custom_attrs = get_folder_custom_attribute_definitions(project_id, folder_id)
                        if custom_attrs:
                            folder_doc["custom_attribute_definitions"] = custom_attrs
                        
                        # APIèŠ‚æµ
                        time.sleep(self.api_delay * 0.5)  # è‡ªå®šä¹‰å±æ€§è°ƒç”¨é¢‘ç‡æ›´ä½
                        
                    except Exception as e:
                        logger.warning(f"è·å–æ–‡ä»¶å¤¹è‡ªå®šä¹‰å±æ€§å¤±è´¥: {str(e)}")
                
                batch_docs.append(folder_doc)
                processed_count += 1
                
                # æ‰¹é‡å¤„ç†
                if len(batch_docs) >= self.batch_size:
                    batch_result = self._batch_upsert_folders(batch_docs)
                    results["success_count"] += batch_result["success_count"]
                    results["errors"].extend(batch_result["errors"])
                    batch_docs = []
                    
                    # æ›´æ–°ä»»åŠ¡è¿›åº¦
                    folder_progress = (processed_count / len(folders_with_context)) * 100
                    self._update_sync_task_progress(task_id, {
                        "folders_processed": processed_count,
                        "current_stage": "processing_folders",
                        "progress_percentage": folder_progress,
                        "total_folders": len(folders_with_context)
                    })
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
                results["errors"].append({
                    "folder_id": folder_data.get('id'),
                    "error": str(e)
                })
        
        # å¤„ç†å‰©ä½™çš„æ–‡ä»¶å¤¹
        if batch_docs:
            batch_result = self._batch_upsert_folders(batch_docs)
            results["success_count"] += batch_result["success_count"]
            results["errors"].extend(batch_result["errors"])
        
        logger.info(f"æ–‡ä»¶å¤¹æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {results['success_count']}, é”™è¯¯ {len(results['errors'])}")
        return results
    
    def _batch_process_files(self, files_with_context: List[Tuple], project_id: str,
                            include_custom_attributes: bool, headers: dict, task_id: str) -> dict:
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶metadata
        
        å…³é”®ä¼˜åŒ–ï¼š
        1. æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„ï¼Œæ‰¹é‡è·å–è‡ªå®šä¹‰å±æ€§
        2. æ‰¹é‡æ•°æ®åº“æ“ä½œ
        3. ç‰ˆæœ¬ä¿¡æ¯ä¸€å¹¶å¤„ç†
        """
        results = {
            "files_synced": 0, "versions_synced": 0, 
            "total_size": 0, "errors": []
        }
        
        # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„ï¼Œä¾¿äºæ‰¹é‡å¤„ç†
        files_by_folder = defaultdict(list)
        for file_data, folder_id, folder_path, depth in files_with_context:
            files_by_folder[folder_id].append((file_data, folder_path, depth))
        
        processed_folders = 0
        total_folders = len(files_by_folder)
        
        for folder_id, files_in_folder in files_by_folder.items():
            try:
                # æ‰¹é‡è·å–è‡ªå®šä¹‰å±æ€§
                custom_attrs_data = {}
                if include_custom_attributes and files_in_folder:
                    version_ids = [f[0].get('id') for f in files_in_folder if f[0].get('id')]
                    
                    if version_ids:
                        # åˆ†æ‰¹è·å–ï¼Œé¿å…URLè¿‡é•¿
                        total_batches = (len(version_ids) + 19) // 20  # å‘ä¸Šå–æ•´
                        for i in range(0, len(version_ids), 20):
                            batch_ids = version_ids[i:i + 20]
                            current_batch = i // 20 + 1
                            
                            try:
                                # æŠ¥å‘Šè‡ªå®šä¹‰å±æ€§å¤„ç†è¿›åº¦
                                attrs_progress = (current_batch / total_batches) * 100
                                self._update_sync_task_progress(task_id, {
                                    "current_stage": "processing_attributes",
                                    "progress_percentage": attrs_progress,
                                    "processed_attribute_batches": current_batch,
                                    "total_attribute_batches": total_batches,
                                    "current_folder": folder_id
                                })
                                
                                # ä½¿ç”¨è‡ªå®šä¹‰å±æ€§APIç›´æ¥è°ƒç”¨
                                from .custom_attributes_api import CustomAttributesAPI
                                custom_attrs_api = CustomAttributesAPI()
                                attrs_result = custom_attrs_api.get_file_custom_attributes(project_id, batch_ids)
                                
                                if attrs_result and attrs_result.get('results'):
                                    custom_attrs_data.update(attrs_result['results'])
                                
                                # APIèŠ‚æµ
                                time.sleep(self.api_delay)
                                
                            except Exception as e:
                                logger.warning(f"æ‰¹é‡è·å–è‡ªå®šä¹‰å±æ€§å¤±è´¥: {str(e)}")
                
                # æ‰¹é‡è½¬æ¢æ–‡ä»¶æ•°æ®
                file_docs = []
                version_docs = []
                
                for file_data, folder_path, depth in files_in_folder:
                    try:
                        # è½¬æ¢æ–‡ä»¶æ•°æ®
                        file_doc = DataTransformer.transform_file_data(
                            file_data, project_id, folder_id, folder_path, depth
                        )
                        
                        # æ·»åŠ è‡ªå®šä¹‰å±æ€§
                        version_id = file_data.get('id')
                        if version_id in custom_attrs_data:
                            file_doc['custom_attributes'] = custom_attrs_data[version_id].get('customAttributes', {})
                            file_doc['has_custom_attributes'] = custom_attrs_data[version_id].get('hasCustomAttributes', False)
                        
                        file_docs.append(file_doc)
                        
                        # è½¬æ¢ç‰ˆæœ¬æ•°æ®
                        version_doc = DataTransformer.transform_version_data(
                            file_data, file_doc['_id'], project_id
                        )
                        version_docs.append(version_doc)
                        
                        # ç´¯è®¡æ–‡ä»¶å¤§å° - ä¿®å¤total_sizeè®¡ç®—é—®é¢˜
                        # æ–‡ä»¶å¤§å°ä¿¡æ¯åœ¨version_docçš„metadataä¸­
                        storage_size = version_doc.get('metadata', {}).get('file_size', 0)
                        
                        if storage_size and storage_size > 0:
                            results["total_size"] += storage_size
                            logger.debug(f"æ–‡ä»¶ {file_data.get('id')} å¤§å°: {storage_size} bytes")
                        else:
                            logger.debug(f"æ–‡ä»¶ {file_data.get('id')} å¤§å°ä¸º0æˆ–æœªæ‰¾åˆ°")
                        
                    except Exception as e:
                        logger.error(f"è½¬æ¢æ–‡ä»¶æ•°æ®å¤±è´¥: {str(e)}")
                        results["errors"].append({
                            "file_id": file_data.get('id'),
                            "error": str(e)
                        })
                
                # æ‰¹é‡æ•°æ®åº“æ“ä½œ
                if file_docs:
                    # æ‰¹é‡æ’å…¥æ–‡ä»¶
                    file_batch_result = self.dal.batch_upsert_files(file_docs)
                    results["files_synced"] += file_batch_result["inserted"] + file_batch_result["updated"]
                    
                    # æ‰¹é‡æ’å…¥ç‰ˆæœ¬
                    version_batch_result = self._batch_upsert_versions(version_docs)
                    results["versions_synced"] += version_batch_result["success_count"]
                    results["errors"].extend(version_batch_result["errors"])
                
                processed_folders += 1
                
                # æ›´æ–°è¿›åº¦
                if processed_folders % 5 == 0 or processed_folders == total_folders:
                    progress_percentage = (processed_folders / total_folders) * 100
                    logger.info(f"æ–‡ä»¶å¤„ç†è¿›åº¦: {processed_folders}/{total_folders} ({progress_percentage:.1f}%)")
                    
                    self._update_sync_task_progress(task_id, {
                        "files_processed": results["files_synced"],
                        "current_stage": "processing_files",
                        "progress_percentage": progress_percentage,
                        "total_file_folders": total_folders,
                        "processed_file_folders": processed_folders
                    })
                
            except Exception as e:
                logger.error(f"æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ {folder_id} å¤±è´¥: {str(e)}")
                results["errors"].append({
                    "folder_id": folder_id,
                    "error": str(e)
                })
        
        logger.info(f"æ–‡ä»¶æ‰¹é‡å¤„ç†å®Œæˆ: æ–‡ä»¶ {results['files_synced']}, ç‰ˆæœ¬ {results['versions_synced']}, å¤§å° {results['total_size']} bytes")
        return results
    
    def _batch_upsert_folders(self, folder_docs: List[Dict]) -> dict:
        """æ‰¹é‡æ’å…¥æ–‡ä»¶å¤¹"""
        results = {"success_count": 0, "errors": []}
        
        try:
            for folder_doc in folder_docs:
                if self.dal.create_or_update_folder(folder_doc):
                    results["success_count"] += 1
                else:
                    results["errors"].append({
                        "folder_id": folder_doc.get('_id'),
                        "error": "æ•°æ®åº“æ“ä½œå¤±è´¥"
                    })
        except Exception as e:
            logger.error(f"æ‰¹é‡æ–‡ä»¶å¤¹æ“ä½œå¤±è´¥: {str(e)}")
            results["errors"].append({"error": str(e)})
        
        return results
    
    def _batch_upsert_versions(self, version_docs: List[Dict]) -> dict:
        """æ‰¹é‡æ’å…¥ç‰ˆæœ¬"""
        results = {"success_count": 0, "errors": []}
        
        try:
            for version_doc in version_docs:
                if self.dal.create_or_update_file_version(version_doc):
                    results["success_count"] += 1
                else:
                    results["errors"].append({
                        "version_id": version_doc.get('_id'),
                        "error": "æ•°æ®åº“æ“ä½œå¤±è´¥"
                    })
        except Exception as e:
            logger.error(f"æ‰¹é‡ç‰ˆæœ¬æ“ä½œå¤±è´¥: {str(e)}")
            results["errors"].append({"error": str(e)})
        
        return results
    
    # å¤ç”¨ç°æœ‰çš„è¾…åŠ©æ–¹æ³•
    def _create_sync_task(self, project_id: str, task_type: str, parameters: dict) -> str:
        """åˆ›å»ºåŒæ­¥ä»»åŠ¡è®°å½•"""
        return sync_manager._create_sync_task(project_id, task_type, parameters)
    
    def _get_top_folders_with_retry(self, project_id: str, headers: dict):
        """è·å–é¡¶çº§æ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•ï¼‰"""
        # å¢å¼ºçš„é‡è¯•æœºåˆ¶
        max_retries = 5
        retry_delay = 3
        top_folders_data = None
        
        for attempt in range(max_retries):
            try:
                logger.info(f"å°è¯•è·å–é¡¶çº§æ–‡ä»¶å¤¹ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")
                top_folders_data = get_project_top_folders(project_id, headers)
                if top_folders_data and top_folders_data.get('data'):
                    logger.info(f"âœ… æˆåŠŸè·å–é¡¶çº§æ–‡ä»¶å¤¹ (å°è¯• {attempt + 1})")
                    break
                else:
                    logger.warning(f"âš ï¸ è·å–åˆ°ç©ºçš„é¡¶çº§æ–‡ä»¶å¤¹æ•°æ® (å°è¯• {attempt + 1})")
                    if attempt < max_retries - 1:
                        logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        retry_delay *= 1.5
                        continue
            except Exception as e:
                logger.error(f"âŒ è·å–é¡¶çº§æ–‡ä»¶å¤¹å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
                if attempt < max_retries - 1:
                    logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    retry_delay *= 1.5
                    continue
                else:
                    raise Exception(f"ç»è¿‡ {max_retries} æ¬¡é‡è¯•åä»æ— æ³•è·å–é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹: {str(e)}")
        
        if not top_folders_data or not top_folders_data.get('data'):
            raise Exception(f"ç»è¿‡ {max_retries} æ¬¡é‡è¯•åä»æ— æ³•è·å–æœ‰æ•ˆçš„é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹æ•°æ®")
        
        return top_folders_data
    
    def _update_project_statistics(self, project_id: str, sync_results: dict, duration: float):
        """æ›´æ–°é¡¹ç›®ç»Ÿè®¡"""
        return sync_manager._update_project_statistics(project_id, sync_results, duration)
    
    def _complete_sync_task(self, task_id: str, results: dict, duration: float):
        """å®ŒæˆåŒæ­¥ä»»åŠ¡"""
        return sync_manager._complete_sync_task(task_id, results, duration)
    
    def _fail_sync_task(self, task_id: str, error: str, duration: float):
        """æ ‡è®°ä»»åŠ¡å¤±è´¥"""
        return sync_manager._fail_sync_task(task_id, error, duration)
    
    def _update_sync_task_progress(self, task_id: str, progress_data: dict):
        """æ›´æ–°åŒæ­¥ä»»åŠ¡è¿›åº¦ï¼ˆä½¿ç”¨ç¨ç«‹ä»»å‹™è¿½è¹¤ç³»çµ±ï¼‰"""
        try:
            from api_modules.task_lifecycle_manager import task_manager
            task_manager.update_task(task_id, progress_data)
        except ImportError:
            logger.warning("Task tracking system not available")
        except Exception as e:
            logger.warning(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")

# ============================================================================
# å¢é‡åŒæ­¥ç®¡ç†å™¨
# ============================================================================

class IncrementalSyncManager:
    """çœŸæ­£çš„å¢é‡åŒæ­¥ç®¡ç†å™¨ - åŸºæ–¼ACC APIçš„lastModifiedTimeå°æ¯”"""
    
    def __init__(self, batch_size: int = 100, api_delay: float = 0.2):
        self.dal = DataAccessLayer()
        self.batch_size = batch_size
        self.api_delay = api_delay
        self.converter = DataTransformer()
    
    def incremental_sync_project(self, project_id: str, max_depth: int = 10, 
                                include_custom_attributes: bool = True, task_id: str = None) -> dict:
        """
        åŸ·è¡ŒçœŸæ­£çš„å¢é‡åŒæ­¥
        
        æ ¸å¿ƒé‚è¼¯ï¼š
        1. ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“
        2. éæ­·ACC APIç²å–æ‰€æœ‰é …ç›®
        3. å°æ¯”æ¯å€‹é …ç›®çš„lastModifiedTimeèˆ‡ä¸Šæ¬¡åŒæ­¥æ™‚é–“
        4. åªåŒæ­¥æœ‰è®Šæ›´çš„é …ç›®
        """
        start_time = time.time()
        
        try:
            # 1. ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“
            last_sync_time = self._get_last_sync_time(project_id)
            logger.info(f"ğŸ”„ é–‹å§‹å¢é‡åŒæ­¥é …ç›®: {project_id}, ä¸Šæ¬¡åŒæ­¥æ™‚é–“: {last_sync_time}")
            
            # 2. ç²å–APIè¨ªå•ä»¤ç‰Œ
            access_token = utils.get_access_token()
            if not access_token:
                raise Exception("æ— æ³•è·å–è®¿é—®ä»¤ç‰Œ")
            
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            # 3. ç²å–é …ç›®é ‚ç´šæ–‡ä»¶å¤¾
            top_folders_data = self._get_top_folders_with_retry(project_id, headers)
            
            # 4. æ”¶é›†æ‰€æœ‰é …ç›®ä¸¦æª¢æŸ¥è®Šæ›´
            self._update_sync_task_progress(task_id, {
                "current_stage": "checking_changes",
                "progress_percentage": 10.0,
                "message": "æª¢æŸ¥é …ç›®è®Šæ›´..."
            })
            
            changed_folders, changed_files = self._collect_changed_items(
                project_id, top_folders_data['data'], headers, max_depth, last_sync_time, task_id
            )
            
            # 5. å¦‚æœæ²’æœ‰è®Šæ›´ï¼Œç›´æ¥è¿”å›
            if not changed_folders and not changed_files:
                logger.info("ğŸ“‹ æ²’æœ‰æª¢æ¸¬åˆ°è®Šæ›´ï¼Œè·³éåŒæ­¥")
                return {
                    "success": True,
                    "message": "æ²’æœ‰è®Šæ›´éœ€è¦åŒæ­¥",
                    "results": {
                        "folders_synced": 0,
                        "files_synced": 0,
                        "versions_synced": 0,
                        "total_size": 0,
                        "errors": []
                    }
                }
            
            logger.info(f"ğŸ“Š æª¢æ¸¬åˆ°è®Šæ›´: {len(changed_folders)} å€‹æ–‡ä»¶å¤¾, {len(changed_files)} å€‹æ–‡ä»¶")
            
            # 6. åŒæ­¥è®Šæ›´çš„é …ç›®
            self._update_sync_task_progress(task_id, {
                "current_stage": "syncing_changes",
                "progress_percentage": 30.0,
                "changed_folders": len(changed_folders),
                "changed_files": len(changed_files)
            })
            
            sync_results = self._sync_changed_items(
                project_id, changed_folders, changed_files, headers, include_custom_attributes, task_id
            )
            
            # 7. æ›´æ–°é …ç›®åŒæ­¥æ™‚é–“
            duration = time.time() - start_time
            self._update_project_sync_time(project_id, duration)
            
            logger.info(f"âœ… å¢é‡åŒæ­¥å®Œæˆ: {sync_results}")
            return {
                "success": True,
                "duration_seconds": duration,
                "results": sync_results
            }
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"âŒ å¢é‡åŒæ­¥å¤±æ•—: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "duration_seconds": duration
            }
    
    def _get_last_sync_time(self, project_id: str):
        """ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“ï¼Œçµ±ä¸€ä½¿ç”¨ä¸­åœ‹æ™‚å€"""
        try:
            db = self.dal.connect()
            project = db.projects.find_one({"_id": project_id})
            
            if project and project.get("sync_info") and project["sync_info"].get("last_sync_time"):
                last_sync = project["sync_info"]["last_sync_time"]
                # ä½¿ç”¨çµ±ä¸€çš„æ™‚é–“æ¨™æº–åŒ–å‡½æ•¸
                return normalize_db_datetime(last_sync)
            
            # å¦‚æœæ²’æœ‰è¨˜éŒ„ï¼Œè¿”å›7å¤©å‰ï¼ˆä¸­åœ‹æ™‚å€ï¼‰
            return get_china_time() - timedelta(days=7)
            
        except Exception as e:
            logger.error(f"ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“å¤±æ•—: {str(e)}")
            return get_china_time() - timedelta(days=7)
    
    def _get_top_folders_with_retry(self, project_id: str, headers: dict):
        """ç²å–é ‚ç´šæ–‡ä»¶å¤¾ï¼ˆå¸¶é‡è©¦ï¼‰"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                from .file_sync_api import get_project_top_folders
                top_folders_data = get_project_top_folders(project_id, headers)
                
                if top_folders_data and top_folders_data.get('data'):
                    return top_folders_data
                    
            except Exception as e:
                logger.warning(f"ç²å–é ‚ç´šæ–‡ä»¶å¤¾å¤±æ•— (å˜—è©¦ {attempt + 1}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    raise Exception(f"ç¶“é {max_retries} æ¬¡é‡è©¦å¾Œä»ç„¡æ³•ç²å–é …ç›®é ‚ç´šæ–‡ä»¶å¤¾")
        
        raise Exception("ç„¡æ³•ç²å–æœ‰æ•ˆçš„é …ç›®é ‚ç´šæ–‡ä»¶å¤¾æ•¸æ“š")
    
    def _collect_changed_items(self, project_id: str, top_folders: List[dict], 
                              headers: dict, max_depth: int, last_sync_time, task_id: str = None):
        """æ”¶é›†æœ‰è®Šæ›´çš„é …ç›®"""
        from collections import deque
        
        # å¼·åˆ¶è½‰æ›åŸºæº–æ™‚é–“ç‚ºä¸­åœ‹æ™‚å€ï¼Œé¿å…æ™‚å€æ¯”è¼ƒå•é¡Œ
        try:
            if last_sync_time:
                # ä½¿ç”¨timestampæ–¹å¼ç¢ºä¿æ™‚å€çµ±ä¸€
                if hasattr(last_sync_time, 'timestamp'):
                    timestamp = last_sync_time.timestamp()
                    last_sync_time = datetime.fromtimestamp(timestamp, CHINA_TZ)
                else:
                    # å¦‚æœæ²’æœ‰timestampæ–¹æ³•ï¼Œå˜—è©¦å…¶ä»–æ–¹å¼
                    last_sync_time = normalize_db_datetime(last_sync_time)
                
                logger.info(f"ğŸ• å¢é‡åŒæ­¥åŸºæº–æ™‚é–“ï¼ˆä¸­åœ‹æ™‚å€ï¼‰: {format_china_time(last_sync_time)}")
            else:
                logger.warning("âš ï¸ æ²’æœ‰åŸºæº–æ™‚é–“ï¼Œä½¿ç”¨7å¤©å‰")
                last_sync_time = get_china_time() - timedelta(days=7)
                
        except Exception as e:
            logger.error(f"âŒ åŸºæº–æ™‚é–“è½‰æ›å¤±æ•—: {str(e)}")
            logger.error(f"   åŸå§‹æ™‚é–“: {last_sync_time}, é¡å‹: {type(last_sync_time)}")
            # ä½¿ç”¨fallbackæ™‚é–“
            last_sync_time = get_china_time() - timedelta(days=7)
            logger.info(f"ğŸ• ä½¿ç”¨fallbackåŸºæº–æ™‚é–“: {format_china_time(last_sync_time)}")
        
        changed_folders = []
        changed_files = []
        
        # ä½¿ç”¨éšŠåˆ—é€²è¡Œå»£åº¦å„ªå…ˆéæ­·
        folder_queue = deque()
        
        # åˆå§‹åŒ–éšŠåˆ—
        for folder in top_folders:
            folder_queue.append((folder, 0, ""))
        
        processed_folders = 0
        total_api_calls = 0
        
        while folder_queue:
            folder_data, depth, parent_path = folder_queue.popleft()
            
            if depth >= max_depth:
                continue
            
            folder_id = folder_data.get('id')
            folder_name = folder_data.get('attributes', {}).get('displayName', 'Unknown')
            current_path = f"{parent_path}/{folder_name}" if parent_path else folder_name
            
            # æª¢æŸ¥æ–‡ä»¶å¤¾æ˜¯å¦æœ‰è®Šæ›´
            folder_last_modified = self._parse_datetime(folder_data.get('attributes', {}).get('lastModifiedTime'))
            
            if folder_last_modified and last_sync_time:
                try:
                    # ä½¿ç”¨timestampé€²è¡Œæ•¸å€¼æ¯”è¼ƒï¼Œå®Œå…¨é¿å…æ™‚å€å•é¡Œ
                    api_timestamp = self._get_timestamp(folder_last_modified)
                    base_timestamp = self._get_timestamp(last_sync_time)
                    
                    if api_timestamp and base_timestamp and api_timestamp > base_timestamp:
                        changed_folders.append((folder_data, depth, parent_path))
                        logger.info(f"ğŸ“ æ–‡ä»¶å¤¾æœ‰è®Šæ›´: {folder_name} (API:{api_timestamp} > åŸºæº–:{base_timestamp})")
                        
                except Exception as e:
                    logger.error(f"âŒ æ™‚é–“æˆ³æ¯”è¼ƒå¤±æ•—: {folder_name}, {str(e)}")
                    # å¦‚æœæ™‚é–“æˆ³æ¯”è¼ƒå¤±æ•—ï¼Œå‡è¨­æœ‰è®Šæ›´ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                    changed_folders.append((folder_data, depth, parent_path))
                    logger.warning(f"âš ï¸ æ™‚é–“æˆ³æ¯”è¼ƒå¤±æ•—ï¼Œå‡è¨­æ–‡ä»¶å¤¾æœ‰è®Šæ›´: {folder_name}")
            
            # APIç¯€æµ
            if total_api_calls > 0 and total_api_calls % 10 == 0:
                time.sleep(self.api_delay)
            
            try:
                # ç²å–æ–‡ä»¶å¤¾å…§å®¹
                from .file_sync_api import get_folder_contents
                contents_data = get_folder_contents(project_id, folder_id, headers)
                total_api_calls += 1
                
                if not contents_data or not contents_data.get('data'):
                    continue
                
                # åˆ†é›¢æ–‡ä»¶å¤¾å’Œæ–‡ä»¶
                for item in contents_data['data']:
                    item_type = item.get('type')
                    item_last_modified = self._parse_datetime(item.get('attributes', {}).get('lastModifiedTime'))
                    
                    if item_type == 'folders':
                        # æ·»åŠ å­æ–‡ä»¶å¤¾åˆ°éšŠåˆ—
                        folder_queue.append((item, depth + 1, current_path))
                    else:
                        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰è®Šæ›´
                        if item_last_modified and last_sync_time:
                            try:
                                # ä½¿ç”¨timestampé€²è¡Œæ•¸å€¼æ¯”è¼ƒ
                                file_timestamp = self._get_timestamp(item_last_modified)
                                base_timestamp = self._get_timestamp(last_sync_time)
                                
                                if file_timestamp and base_timestamp and file_timestamp > base_timestamp:
                                    changed_files.append((item, folder_id, current_path, depth))
                                    logger.debug(f"ğŸ“„ æ–‡ä»¶æœ‰è®Šæ›´: {item.get('attributes', {}).get('displayName', 'Unknown')} (æ™‚é–“æˆ³:{file_timestamp} > {base_timestamp})")
                            except Exception as e:
                                logger.error(f"âŒ æ–‡ä»¶æ™‚é–“æˆ³æ¯”è¼ƒå¤±æ•—: {item.get('attributes', {}).get('displayName', 'Unknown')}, {str(e)}")
                                # ä¿å®ˆç­–ç•¥ï¼šå‡è¨­æœ‰è®Šæ›´
                                changed_files.append((item, folder_id, current_path, depth))
                
                processed_folders += 1
                
                # æ›´æ–°é€²åº¦
                if processed_folders % 10 == 0 and task_id:
                    self._update_sync_task_progress(task_id, {
                        "current_stage": "checking_changes",
                        "progress_percentage": min(10 + (processed_folders / max(len(top_folders) * 5, 1)) * 20, 30),
                        "processed_folders": processed_folders,
                        "changed_folders": len(changed_folders),
                        "changed_files": len(changed_files)
                    })
                
            except Exception as e:
                logger.error(f"æª¢æŸ¥æ–‡ä»¶å¤¾å…§å®¹å¤±æ•— {folder_name}: {str(e)}")
                continue
        
        return changed_folders, changed_files
    
    def _parse_datetime(self, datetime_str):
        """è§£ææ—¥æœŸæ™‚é–“å­—ç¬¦ä¸²ï¼Œçµ±ä¸€è½‰æ›ç‚ºä¸­åœ‹æ™‚å€"""
        return parse_api_datetime(datetime_str)
    
    def _get_timestamp(self, dt):
        """ç²å–datetimeå°è±¡çš„timestampï¼Œé¿å…æ™‚å€æ¯”è¼ƒå•é¡Œ"""
        if not dt:
            return None
        
        try:
            # å¦‚æœæœ‰timestampæ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨
            if hasattr(dt, 'timestamp'):
                return dt.timestamp()
            
            # å¦‚æœæ²’æœ‰æ™‚å€ä¿¡æ¯ï¼Œå‡è¨­ç‚ºUTC
            if hasattr(dt, 'tzinfo') and dt.tzinfo is None:
                utc_dt = pytz.utc.localize(dt)
                return utc_dt.timestamp()
            
            # å¦‚æœæœ‰æ™‚å€ä¿¡æ¯ï¼Œè½‰æ›ç‚ºUTCå¾Œç²å–timestamp
            if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
                return dt.timestamp()
            
            # MongoDB datetimeå°è±¡è™•ç†
            utc_dt = pytz.utc.localize(dt)
            return utc_dt.timestamp()
            
        except Exception as e:
            logger.warning(f"ç²å–timestampå¤±æ•—: {dt}, éŒ¯èª¤: {str(e)}")
            return None
    
    def _sync_changed_items(self, project_id: str, changed_folders: List, changed_files: List, 
                           headers: dict, include_custom_attributes: bool, task_id: str = None):
        """åŒæ­¥æœ‰è®Šæ›´çš„é …ç›®"""
        sync_results = {
            "folders_synced": 0,
            "files_synced": 0,
            "versions_synced": 0,
            "total_size": 0,
            "errors": []
        }
        
        # åŒæ­¥æ–‡ä»¶å¤¾
        for i, (folder_data, depth, parent_path) in enumerate(changed_folders):
            try:
                folder_db_data = self.converter.transform_folder_data(
                    folder_data, project_id, parent_path
                )
                
                if self.dal.create_or_update_folder(folder_db_data):
                    sync_results["folders_synced"] += 1
                
                # æ›´æ–°é€²åº¦
                if task_id and i % 5 == 0:
                    progress = 30 + (i / len(changed_folders)) * 30
                    self._update_sync_task_progress(task_id, {
                        "current_stage": "syncing_folders",
                        "progress_percentage": progress,
                        "folders_processed": i + 1
                    })
                    
            except Exception as e:
                error_msg = f"åŒæ­¥æ–‡ä»¶å¤¾å¤±æ•—: {folder_data.get('attributes', {}).get('displayName', 'Unknown')}: {str(e)}"
                logger.error(error_msg)
                sync_results["errors"].append(error_msg)
        
        # åŒæ­¥æ–‡ä»¶
        for i, (file_data, parent_folder_id, folder_path, depth) in enumerate(changed_files):
            try:
                file_db_data = self.converter.transform_file_data(
                    file_data, project_id, parent_folder_id, folder_path
                )
                
                if self.dal.create_or_update_file(file_db_data):
                    sync_results["files_synced"] += 1
                
                # ç²å–æ–‡ä»¶ç‰ˆæœ¬
                try:
                    from .file_sync_api import get_item_versions
                    versions = get_item_versions(project_id, file_data.get('id'), headers)
                    
                    for version in versions:
                        version_db_data = self.converter.transform_version_data(
                            version, file_data.get('id'), project_id
                        )
                        
                        if self.dal.create_or_update_file_version(version_db_data):
                            sync_results["versions_synced"] += 1
                            sync_results["total_size"] += version_db_data.get("file_info", {}).get("size", 0)
                            
                except Exception as ve:
                    logger.warning(f"ç²å–æ–‡ä»¶ç‰ˆæœ¬å¤±æ•—: {str(ve)}")
                
                # æ›´æ–°é€²åº¦
                if task_id and i % 5 == 0:
                    progress = 60 + (i / len(changed_files)) * 30
                    self._update_sync_task_progress(task_id, {
                        "current_stage": "syncing_files",
                        "progress_percentage": progress,
                        "files_processed": i + 1
                    })
                    
            except Exception as e:
                error_msg = f"åŒæ­¥æ–‡ä»¶å¤±æ•—: {file_data.get('attributes', {}).get('displayName', 'Unknown')}: {str(e)}"
                logger.error(error_msg)
                sync_results["errors"].append(error_msg)
        
        return sync_results
    
    def _update_project_sync_time(self, project_id: str, duration: float):
        """æ›´æ–°é …ç›®åŒæ­¥æ™‚é–“"""
        try:
            from datetime import datetime
            self.dal.update_project_sync_status(
                project_id, 
                "completed", 
                duration=duration
            )
        except Exception as e:
            logger.error(f"æ›´æ–°é …ç›®åŒæ­¥æ™‚é–“å¤±æ•—: {str(e)}")
    
    def _update_sync_task_progress(self, task_id: str, progress_data: dict):
        """æ›´æ–°åŒæ­¥ä»»å‹™é€²åº¦"""
        if not task_id:
            return
            
        try:
            # æ›´æ–°ä»»å‹™è¿½è¹¤ç³»çµ±
            from api_modules.task_lifecycle_manager import task_manager
            task_manager.update_task(task_id, progress_data)
        except ImportError:
            logger.warning("Task tracking system not available")
        except Exception as e:
            logger.warning(f"æ›´æ–°ä»»å‹™é€²åº¦å¤±æ•—: {str(e)}")

# åˆ›å»ºç®¡ç†å™¨å®ä¾‹
batch_sync_manager = BatchOptimizedSyncManager(batch_size=100, api_delay=0.2)
incremental_sync_manager = IncrementalSyncManager(batch_size=100, api_delay=0.2)


def _force_cleanup_running_tasks(project_id: str) -> dict:
    """
    å¼ºåˆ¶æ¸…ç†é¡¹ç›®çš„æ‰€æœ‰è¿è¡Œä¸­ä»»åŠ¡
    """
    try:
        from datetime import datetime
        
        db = batch_sync_manager.dal.connect()
        
        # ç›´æ¥å–æ¶ˆæ‰€æœ‰runningçŠ¶æ€çš„ä»»åŠ¡
        result = db.sync_tasks.update_many(
            {
                'task_status': 'running',
                'project_id': project_id
            },
            {
                '$set': {
                    'task_status': 'cancelled',
                    'end_time': datetime.now(),
                    'updated_at': datetime.now(),
                    'results': {'error': 'æ–°åŒæ­¥å¯åŠ¨ï¼Œå¼ºåˆ¶æ¸…ç†'}
                }
            }
        )
        
        return {
            "cancelled_count": result.modified_count,
            "message": f"å¼ºåˆ¶å–æ¶ˆäº† {result.modified_count} ä¸ªè¿è¡Œä¸­çš„ä»»åŠ¡"
        }
        
    except Exception as e:
        logger.error(f"å¼ºåˆ¶æ¸…ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
        return {
            "cancelled_count": 0,
            "message": f"æ¸…ç†å¤±è´¥: {str(e)}"
        }


# èˆŠçš„å…§å­˜é€²åº¦å­˜å„²å·²ç§»é™¤ï¼Œç¾åœ¨ä½¿ç”¨ç¨ç«‹çš„ä»»å‹™è¿½è¹¤ç³»çµ±


def _complete_task(task_id: str, results: dict):
    """å®Œæˆä»»åŠ¡å¹¶è®°å½•åˆ°ç®€åŒ–çš„åŒæ­¥å†å²"""
    try:
        from datetime import datetime
        from database.data_access_layer import DataAccessLayer
        
        # ä½¿ç”¨ç®€åŒ–çš„åŒæ­¥è®°å½•
        dal = DataAccessLayer()
        project_id = results.get("project_id")
        duration = results.get("duration_seconds", 0)
        
        # å‡†å¤‡åŒæ­¥ç»“æœæ•°æ®
        sync_results = {
            "folders_synced": results.get("folders_synced", 0),
            "files_synced": results.get("files_synced", 0),
            "versions_synced": results.get("versions_synced", 0),
            "total_size": results.get("total_size", 0),  # ä¿®å¤total_sizeé—®é¢˜
            "duration_seconds": duration
        }
        
        # åªè®°å½•æˆåŠŸçš„åŒæ­¥åˆ°ç®€åŒ–çš„å†å²è®°å½•
        success = dal.create_sync_history_record(project_id, "batch_sync", sync_results)
        
        if success:
            logger.info(f"âœ… æˆåŠŸè®°å½•æ‰¹é‡åŒæ­¥å†å²: {project_id}")
        else:
            logger.warning(f"âš ï¸ è®°å½•åŒæ­¥å†å²å¤±è´¥ï¼Œä½†åŒæ­¥æœ¬èº«æˆåŠŸ: {project_id}")
        
        # ä»»å‹™å®Œæˆé‚è¼¯å·²ç§»è‡³ç¨ç«‹çš„ä»»å‹™è¿½è¹¤ç³»çµ±
            
        return success
        
    except Exception as e:
        logger.error(f"ä¿å­˜åŒæ­¥è®°å½•å¤±è´¥: {str(e)}")
        return False


def _fail_task(task_id: str, error_message: str):
    """æ¨™è¨˜ä»»å‹™å¤±æ•—"""
    try:
        logger.error(f"âŒ ä»»å‹™å¤±æ•— {task_id}: {error_message}")
        # ä»»å‹™å¤±æ•—é‚è¼¯å·²ç§»è‡³ç¨ç«‹çš„ä»»å‹™è¿½è¹¤ç³»çµ±
        return True
        
    except Exception as e:
        logger.error(f"æ¨™è¨˜ä»»å‹™å¤±æ•—æ™‚å‡ºéŒ¯: {str(e)}")
        return False


def _auto_cleanup_old_tasks(project_id: str) -> dict:
    """
    è‡ªåŠ¨æ¸…ç†é¡¹ç›®çš„æ—§åŒæ­¥ä»»åŠ¡
    
    æ¸…ç†ç­–ç•¥:
    1. å–æ¶ˆæ‰€æœ‰runningçŠ¶æ€ä½†è¶…è¿‡10åˆ†é’Ÿæœªæ›´æ–°çš„ä»»åŠ¡
    2. å–æ¶ˆæ‰€æœ‰runningçŠ¶æ€çš„é‡å¤ä»»åŠ¡ï¼ˆåªä¿ç•™æœ€æ–°çš„ä¸€ä¸ªï¼‰
    3. åˆ é™¤è¶…è¿‡24å°æ—¶çš„å·²å®Œæˆ/å¤±è´¥/å–æ¶ˆä»»åŠ¡
    
    Returns:
        dict: æ¸…ç†ç»“æœç»Ÿè®¡
    """
    try:
        from datetime import datetime, timedelta
        
        db = batch_sync_manager.dal.connect()
        cleanup_stats = {
            "stuck_tasks_cancelled": 0,
            "duplicate_tasks_cancelled": 0, 
            "old_tasks_deleted": 0,
            "errors": []
        }
        
        # 1. å–æ¶ˆå¡ä½çš„ä»»åŠ¡ï¼ˆè¶…è¿‡10åˆ†é’Ÿæœªæ›´æ–°ï¼‰
        cutoff_time = datetime.now() - timedelta(minutes=10)
        stuck_tasks = list(db.sync_tasks.find({
            'task_status': 'running',
            'project_id': project_id,
            'updated_at': {'$lt': cutoff_time}
        }))
        
        for task in stuck_tasks:
            try:
                updated_task = task.copy()
                updated_task['task_status'] = 'cancelled'
                updated_task['end_time'] = datetime.now()
                updated_task['updated_at'] = datetime.now()
                updated_task['results'] = {'error': 'ä»»åŠ¡è¶…æ—¶ï¼Œè‡ªåŠ¨æ¸…ç†'}
                
                result = db.sync_tasks.replace_one({'_id': task['_id']}, updated_task)
                if result.modified_count > 0:
                    cleanup_stats["stuck_tasks_cancelled"] += 1
                    logger.info(f"å–æ¶ˆå¡ä½ä»»åŠ¡: {task['_id']}")
            except Exception as e:
                cleanup_stats["errors"].append(f"å–æ¶ˆå¡ä½ä»»åŠ¡å¤±è´¥ {task['_id']}: {str(e)}")
        
        # 2. å¤„ç†é‡å¤çš„runningä»»åŠ¡ï¼ˆåªä¿ç•™æœ€æ–°çš„ï¼‰
        running_tasks = list(db.sync_tasks.find({
            'task_status': 'running',
            'project_id': project_id
        }).sort('created_at', -1))
        
        if len(running_tasks) > 1:
            # ä¿ç•™æœ€æ–°çš„ï¼Œå–æ¶ˆå…¶ä»–çš„
            for task in running_tasks[1:]:
                try:
                    updated_task = task.copy()
                    updated_task['task_status'] = 'cancelled'
                    updated_task['end_time'] = datetime.now()
                    updated_task['updated_at'] = datetime.now()
                    updated_task['results'] = {'error': 'é‡å¤ä»»åŠ¡ï¼Œè‡ªåŠ¨æ¸…ç†'}
                    
                    result = db.sync_tasks.replace_one({'_id': task['_id']}, updated_task)
                    if result.modified_count > 0:
                        cleanup_stats["duplicate_tasks_cancelled"] += 1
                        logger.info(f"å–æ¶ˆé‡å¤ä»»åŠ¡: {task['_id']}")
                except Exception as e:
                    cleanup_stats["errors"].append(f"å–æ¶ˆé‡å¤ä»»åŠ¡å¤±è´¥ {task['_id']}: {str(e)}")
        
        # 3. åˆ é™¤è¶…è¿‡24å°æ—¶çš„æ—§ä»»åŠ¡
        old_task_cutoff = datetime.now() - timedelta(hours=24)
        old_tasks_result = db.sync_tasks.delete_many({
            'project_id': project_id,
            'task_status': {'$in': ['completed', 'failed', 'cancelled']},
            'updated_at': {'$lt': old_task_cutoff}
        })
        cleanup_stats["old_tasks_deleted"] = old_tasks_result.deleted_count
        
        if cleanup_stats["old_tasks_deleted"] > 0:
            logger.info(f"åˆ é™¤ {cleanup_stats['old_tasks_deleted']} ä¸ªè¶…è¿‡24å°æ—¶çš„æ—§ä»»åŠ¡")
        
        return cleanup_stats
        
    except Exception as e:
        logger.error(f"è‡ªåŠ¨æ¸…ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
        return {
            "stuck_tasks_cancelled": 0,
            "duplicate_tasks_cancelled": 0,
            "old_tasks_deleted": 0,
            "errors": [f"æ¸…ç†å¤±è´¥: {str(e)}"]
        }


# ============================================================================
# API ç«¯ç‚¹
# ============================================================================

@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/sync', methods=['POST'])
def unified_sync_project_api(project_id):
    """
    çµ±ä¸€çš„é …ç›®åŒæ­¥API
    
    POST /api/file-sync-db/project/{project_id}/sync
    
    å‚æ•°:
    - syncType: åŒæ­¥é¡å‹ ("full_sync" | "incremental_sync") (é»˜è®¤: "incremental_sync")
    - maxDepth: æœ€å¤§éå†æ·±åº¦ (é»˜è®¤: 10)
    - includeCustomAttributes: æ˜¯å¦åŒ…å«è‡ªå®šä¹‰å±æ€§ (é»˜è®¤: true)
    - batchSize: æ‰¹é‡å¤„ç†å¤§å° (é»˜è®¤: 100)
    - apiDelay: APIè°ƒç”¨é—´éš”ç§’æ•° (é»˜è®¤: 0.2)
    """
    try:
        # è·å–å‚æ•°
        request_data = request.json or {}
        sync_type = request_data.get('syncType', 'incremental_sync')
        max_depth = request_data.get('maxDepth', 10)
        include_custom_attributes = request_data.get('includeCustomAttributes', True)
        batch_size = request_data.get('batchSize', 100)
        api_delay = request_data.get('apiDelay', 0.2)
        
        # é©—è­‰åŒæ­¥é¡å‹
        if sync_type not in ['full_sync', 'incremental_sync']:
            return jsonify({
                "success": False,
                "error": f"ç„¡æ•ˆçš„åŒæ­¥é¡å‹: {sync_type}ï¼Œå¿…é ˆæ˜¯ 'full_sync' æˆ– 'incremental_sync'"
            }), 400
        
        logger.info(f"é–‹å§‹{sync_type}é …ç›® {project_id}: maxDepth={max_depth}, batchSize={batch_size}")
        
        # ğŸ§¹ å¼ºåˆ¶æ¸…ç†è¯¥é¡¹ç›®çš„æ‰€æœ‰è¿è¡Œä¸­ä»»åŠ¡
        cleanup_result = _force_cleanup_running_tasks(project_id)
        logger.info(f"å¼ºåˆ¶æ¸…ç†è¿è¡Œä¸­ä»»åŠ¡: {cleanup_result}")
        
        # ç”Ÿæˆä»»åŠ¡ID
        import uuid
        task_id = str(uuid.uuid4())
        
        # æ ¹æ“šåŒæ­¥é¡å‹é¸æ“‡åŸ·è¡Œæ–¹å¼
        if sync_type == 'full_sync':
            return _execute_full_sync(project_id, task_id, max_depth, include_custom_attributes, batch_size, api_delay)
        else:
            return _execute_incremental_sync(project_id, task_id, max_depth, include_custom_attributes, batch_size, api_delay)
            
    except Exception as e:
        logger.error(f"çµ±ä¸€åŒæ­¥APIå¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"åŒæ­¥å¤±è´¥: {str(e)}"
        }), 500

def _execute_full_sync(project_id, task_id, max_depth, include_custom_attributes, batch_size, api_delay):
    """åŸ·è¡Œå…¨é‡åŒæ­¥"""
    import threading
    
    def run_full_sync():
        try:
            from datetime import datetime
            start_time = datetime.now()
            
            # è¨»å†Šä»»å‹™åˆ°è¿½è¹¤ç³»çµ±
            try:
                from api_modules.task_lifecycle_manager import task_manager
                task_manager.register_task(task_id, {
                    "current_stage": "initializing",
                    "progress_percentage": 0.0,
                    "project_id": project_id,
                    "task_type": "full_sync"
                })
            except ImportError:
                logger.warning("Task tracking system not available")
            
            # åˆ›å»ºæ‰¹é‡åŒæ­¥ç®¡ç†å™¨ä¸¦åŸ·è¡Œå…¨é‡åŒæ­¥
            batch_manager = BatchOptimizedSyncManager(batch_size=batch_size, api_delay=api_delay)
            result = batch_manager.batch_sync_project(
                project_id, 
                max_depth=max_depth,
                include_custom_attributes=include_custom_attributes,
                task_id=task_id,
                is_full_sync=True  # é—œéµï¼šå•Ÿç”¨æ•¸æ“šæ¸…ç†
            )
            
            # è®¡ç®—è€—æ—¶
            duration = (datetime.now() - start_time).total_seconds()
            
            # å®Œæˆä»»åŠ¡å¹¶è®°å½•åˆ°æ•°æ®åº“
            if result.get("success"):
                _complete_task(task_id, result.get("results", {}))
                # é€šçŸ¥ä»»å‹™è¿½è¹¤ç³»çµ±ä»»å‹™å®Œæˆ
                try:
                    from api_modules.task_lifecycle_manager import task_manager
                    task_manager.complete_task(task_id, result.get("results", {}))
                except ImportError:
                    pass
            else:
                _fail_task(task_id, result.get("error", "Unknown error"))
                # é€šçŸ¥ä»»å‹™è¿½è¹¤ç³»çµ±ä»»å‹™å¤±æ•—
                try:
                    from api_modules.task_lifecycle_manager import task_manager
                    task_manager.fail_task(task_id, result.get("error", "Unknown error"))
                except ImportError:
                    pass
                    
        except Exception as e:
            logger.error(f"å…¨é‡åŒæ­¥åŸ·è¡Œå¤±æ•—: {str(e)}")
            _fail_task(task_id, str(e))
            try:
                from api_modules.task_lifecycle_manager import task_manager
                task_manager.fail_task(task_id, str(e))
            except ImportError:
                pass
    
    # å¯åŠ¨åå°çº¿ç¨‹
    sync_thread = threading.Thread(target=run_full_sync)
    sync_thread.daemon = True
    sync_thread.start()
    
    return jsonify({
        "success": True,
        "message": "å…¨é‡åŒæ­¥å·²å¯åŠ¨",
        "data": {
            "task_id": task_id,
            "status": "running",
            "sync_type": "full_sync",
            "optimization_info": {
                "batch_size": batch_size,
                "api_delay": api_delay
            }
        }
    })

def _execute_incremental_sync(project_id, task_id, max_depth, include_custom_attributes, batch_size, api_delay):
    """åŸ·è¡Œå¢é‡åŒæ­¥"""
    import threading
    
    def run_incremental_sync():
        try:
            from datetime import datetime
            start_time = datetime.now()
            
            # è¨»å†Šä»»å‹™åˆ°è¿½è¹¤ç³»çµ±
            try:
                from api_modules.task_lifecycle_manager import task_manager
                task_manager.register_task(task_id, {
                    "current_stage": "initializing",
                    "progress_percentage": 0.0,
                    "project_id": project_id,
                    "task_type": "incremental_sync"
                })
            except ImportError:
                logger.warning("Task tracking system not available")
            
            # å‰µå»ºå¢é‡åŒæ­¥ç®¡ç†å™¨ä¸¦åŸ·è¡ŒçœŸæ­£çš„å¢é‡åŒæ­¥
            incremental_manager = IncrementalSyncManager(batch_size=batch_size, api_delay=api_delay)
            result = incremental_manager.incremental_sync_project(
                project_id, 
                max_depth=max_depth,
                include_custom_attributes=include_custom_attributes,
                task_id=task_id
            )
            
            # è®¡ç®—è€—æ—¶
            duration = (datetime.now() - start_time).total_seconds()
            
            # å®Œæˆä»»åŠ¡å¹¶è®°å½•åˆ°æ•°æ®åº“
            if result.get("success"):
                _complete_task(task_id, result.get("results", {}))
                try:
                    from api_modules.task_lifecycle_manager import task_manager
                    task_manager.complete_task(task_id, result.get("results", {}))
                except ImportError:
                    pass
            else:
                _fail_task(task_id, result.get("error", "Unknown error"))
                try:
                    from api_modules.task_lifecycle_manager import task_manager
                    task_manager.fail_task(task_id, result.get("error", "Unknown error"))
                except ImportError:
                    pass
                    
        except Exception as e:
            logger.error(f"å¢é‡åŒæ­¥åŸ·è¡Œå¤±æ•—: {str(e)}")
            _fail_task(task_id, str(e))
            try:
                from api_modules.task_lifecycle_manager import task_manager
                task_manager.fail_task(task_id, str(e))
            except ImportError:
                pass
    
    # å¯åŠ¨åå°çº¿ç¨‹
    sync_thread = threading.Thread(target=run_incremental_sync)
    sync_thread.daemon = True
    sync_thread.start()
    
    return jsonify({
        "success": True,
        "message": "å¢é‡åŒæ­¥å·²å¯åŠ¨",
        "data": {
            "task_id": task_id,
            "status": "running",
            "sync_type": "incremental_sync",
            "optimization_info": {
                "batch_size": batch_size,
                "api_delay": api_delay
            }
        }
    })

# ============================================================================
# èˆŠçš„APIç«¯é»ï¼ˆä¿ç•™ä»¥ä¾¿å‘å¾Œå…¼å®¹ï¼Œä½†æ¨™è¨˜ç‚ºå·²æ£„ç”¨ï¼‰
# ============================================================================

@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/full-sync', methods=['POST'])
def full_sync_project_api(project_id):
    """
    é¡¹ç›®å…¨é‡åŒæ­¥API (å·²æ£„ç”¨)
    
    âš ï¸ DEPRECATED: è«‹ä½¿ç”¨ POST /api/file-sync-db/project/{project_id}/sync 
    ä¸¦è¨­ç½® syncType: "full_sync"
    
    POST /api/file-sync-db/project/{project_id}/full-sync
    
    å‚æ•°:
    - maxDepth: æœ€å¤§éå†æ·±åº¦ (é»˜è®¤: 10)
    - includeCustomAttributes: æ˜¯å¦åŒ…å«è‡ªå®šä¹‰å±æ€§ (é»˜è®¤: true)
    """
    try:
        # è·å–å‚æ•°
        max_depth = request.json.get('maxDepth', 10) if request.json else request.args.get('maxDepth', 10, type=int)
        include_custom_attributes = request.json.get('includeCustomAttributes', True) if request.json else request.args.get('includeCustomAttributes', 'true').lower() == 'true'
        
        logger.info(f"å¼€å§‹å…¨é‡åŒæ­¥é¡¹ç›® {project_id}: maxDepth={max_depth}, includeCustomAttributes={include_custom_attributes}")
        
        # æ‰§è¡ŒåŒæ­¥
        result = sync_manager.full_sync_project(
            project_id, 
            max_depth=max_depth,
            include_custom_attributes=include_custom_attributes
        )
        
        if result["success"]:
            return jsonify({
                "success": True,
                "message": f"é¡¹ç›®å…¨é‡åŒæ­¥å®Œæˆ",
                "data": result
            })
        else:
            return jsonify({
                "success": False,
                "error": result["error"],
                "duration_seconds": result["duration_seconds"]
            }), 500
            
    except Exception as e:
        logger.error(f"å…¨é‡åŒæ­¥APIå¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"å…¨é‡åŒæ­¥å¤±è´¥: {str(e)}"
        }), 500


@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/incremental-sync', methods=['POST'])
def incremental_sync_project_api(project_id):
    """
    é¡¹ç›®å¢é‡åŒæ­¥API (å·²æ£„ç”¨)
    
    âš ï¸ DEPRECATED: è«‹ä½¿ç”¨ POST /api/file-sync-db/project/{project_id}/sync 
    ä¸¦è¨­ç½® syncType: "incremental_sync"
    """
    """
    é¡¹ç›®å¢é‡åŒæ­¥API
    
    POST /api/file-sync-db/project/{project_id}/incremental-sync
    """
    try:
        logger.info(f"å¼€å§‹å¢é‡åŒæ­¥é¡¹ç›® {project_id}")
        
        # æ‰§è¡Œå¢é‡åŒæ­¥
        result = sync_manager.incremental_sync_project(project_id)
        
        if result["success"]:
            return jsonify({
                "success": True,
                "message": "é¡¹ç›®å¢é‡åŒæ­¥å®Œæˆ",
                "data": result
            })
        else:
            return jsonify({
                "success": False,
                "error": result["error"],
                "duration_seconds": result["duration_seconds"]
            }), 500
            
    except Exception as e:
        logger.error(f"å¢é‡åŒæ­¥APIå¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"å¢é‡åŒæ­¥å¤±è´¥: {str(e)}"
        }), 500


@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/batch-sync', methods=['POST'])
def batch_sync_project_api(project_id):
    """
    æ‰¹é‡ä¼˜åŒ–çš„é¡¹ç›®åŒæ­¥API (å·²æ£„ç”¨)
    
    âš ï¸ DEPRECATED: è«‹ä½¿ç”¨ POST /api/file-sync-db/project/{project_id}/sync 
    ä¸¦è¨­ç½® syncType: "full_sync" æˆ– "incremental_sync"
    
    POST /api/file-sync-db/project/{project_id}/batch-sync
    
    å‚æ•°:
    - maxDepth: æœ€å¤§éå†æ·±åº¦ (é»˜è®¤: 10)
    - includeCustomAttributes: æ˜¯å¦åŒ…å«è‡ªå®šä¹‰å±æ€§ (é»˜è®¤: true)
    - batchSize: æ‰¹é‡å¤„ç†å¤§å° (é»˜è®¤: 100)
    - apiDelay: APIè°ƒç”¨é—´éš”ç§’æ•° (é»˜è®¤: 0.2)
    - isFullSync: æ˜¯å¦ç‚ºå…¨é‡åŒæ­¥ï¼Œæœƒå…ˆæ¸…é™¤ç¾æœ‰æ•¸æ“š (é»˜è®¤: false)
    """
    try:
        # è·å–å‚æ•°
        request_data = request.json or {}
        max_depth = request_data.get('maxDepth', 10)
        include_custom_attributes = request_data.get('includeCustomAttributes', True)
        batch_size = request_data.get('batchSize', 100)
        api_delay = request_data.get('apiDelay', 0.2)
        is_full_sync = request_data.get('isFullSync', False)
        
        sync_mode = "å…¨é‡åŒæ­¥" if is_full_sync else "å¢é‡åŒæ­¥"
        logger.info(f"å¼€å§‹æ‰¹é‡ä¼˜åŒ–{sync_mode}é¡¹ç›® {project_id}: maxDepth={max_depth}, batchSize={batch_size}, isFullSync={is_full_sync}")
        
        # ğŸ§¹ å¼ºåˆ¶æ¸…ç†è¯¥é¡¹ç›®çš„æ‰€æœ‰è¿è¡Œä¸­ä»»åŠ¡
        cleanup_result = _force_cleanup_running_tasks(project_id)
        logger.info(f"å¼ºåˆ¶æ¸…ç†è¿è¡Œä¸­ä»»åŠ¡: {cleanup_result}")
        
        # ç”Ÿæˆç®€å•çš„ä»»åŠ¡IDï¼ˆä¸å­˜å‚¨åˆ°æ•°æ®åº“ï¼‰
        import uuid
        task_id = str(uuid.uuid4())
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼ŒåŒæ­¥åœ¨åå°æ‰§è¡Œ
        import threading
        
        def run_batch_sync():
            try:
                from datetime import datetime
                start_time = datetime.now()
                
                # è¨»å†Šä»»å‹™åˆ°è¿½è¹¤ç³»çµ±
                try:
                    from api_modules.task_lifecycle_manager import task_manager
                    task_type = "full_sync" if is_full_sync else "batch_optimized_sync"
                    task_manager.register_task(task_id, {
                    "current_stage": "initializing",
                    "progress_percentage": 0.0,
                        "project_id": project_id,
                        "task_type": task_type,
                        "is_full_sync": is_full_sync
                })
                    logger.info(f"ä»»åŠ¡ {task_id} å·²æ³¨å†Œåˆ°ä»»åŠ¡è¿½è¸ªç³»ç»Ÿ (é¡å‹: {task_type})")
                except ImportError:
                    logger.warning("Task tracking system not available")
                
                # åˆ›å»ºæ‰¹é‡åŒæ­¥ç®¡ç†å™¨
                batch_manager = BatchOptimizedSyncManager(batch_size=batch_size, api_delay=api_delay)
                
                # æ‰§è¡Œæ‰¹é‡åŒæ­¥ï¼ˆä½¿ç”¨å†…å­˜è¿›åº¦ï¼‰
                result = batch_manager.batch_sync_project(
                    project_id, 
                    max_depth=max_depth,
                    include_custom_attributes=include_custom_attributes,
                    task_id=task_id,
                    is_full_sync=is_full_sync
                )
                
                # è®¡ç®—è€—æ—¶
                duration = (datetime.now() - start_time).total_seconds()
                
                # å®Œæˆä»»åŠ¡å¹¶è®°å½•åˆ°æ•°æ®åº“
                if result.get("success", False):
                    _complete_task(task_id, {
                        "project_id": project_id,
                        "start_time": start_time,
                        "duration_seconds": duration,
                        **result.get("results", {})
                    })
                else:
                    # å¤±è´¥çš„ä»»åŠ¡é€šçŸ¥è¿½è¹¤ç³»çµ±
                    try:
                        from api_modules.task_lifecycle_manager import task_manager
                        task_manager.fail_task(task_id, result.get("error", "Unknown error"))
                    except ImportError:
                        logger.warning("Task tracking system not available")
                    
            except Exception as e:
                logger.error(f"åå°æ‰¹é‡åŒæ­¥å¤±è´¥: {str(e)}")
                try:
                    from api_modules.task_lifecycle_manager import task_manager
                    task_manager.fail_task(task_id, str(e))
                except ImportError:
                    logger.warning("Task tracking system not available")
        
        # å¯åŠ¨åå°çº¿ç¨‹
        sync_thread = threading.Thread(target=run_batch_sync)
        sync_thread.daemon = True
        sync_thread.start()
        
        # ç«‹å³è¿”å›ä»»åŠ¡ä¿¡æ¯
        return jsonify({
            "success": True,
            "message": f"æ‰¹é‡ä¼˜åŒ–{sync_mode}å·²å¯åŠ¨",
            "data": {
                "task_id": task_id,
                "status": "running",
                "optimization_info": {
                    "batch_size": batch_size,
                    "api_delay": api_delay,
                    "sync_type": "full_sync" if is_full_sync else "batch_optimized",
                    "is_full_sync": is_full_sync
                }
            }
        })
            
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¼˜åŒ–åŒæ­¥APIå¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"æ‰¹é‡ä¼˜åŒ–åŒæ­¥å¤±è´¥: {str(e)}"
        }), 500


# èˆŠçš„é€²åº¦æŸ¥è©¢APIå·²ç§»é™¤ï¼Œç¾åœ¨ä½¿ç”¨ /api/task-tracking/project/<project_id>/sync-progress/<task_id>


@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/sync-status')
def get_sync_status(project_id):
    """
    è·å–é¡¹ç›®åŒæ­¥çŠ¶æ€
    
    GET /api/file-sync-db/project/{project_id}/sync-status
    """
    try:
        # è·å–é¡¹ç›®ä¿¡æ¯
        project = sync_manager.dal.get_project(project_id)
        if not project:
            return jsonify({
                "success": False,
                "error": "é¡¹ç›®ä¸å­˜åœ¨"
            }), 404
        
        # è·å–æœ€è¿‘çš„åŒæ­¥ä»»åŠ¡
        recent_tasks = sync_manager.dal.get_recent_sync_tasks(project_id, limit=5)
        
        return jsonify({
            "success": True,
            "data": {
                "project": {
                    "id": project_id,
                    "sync_info": project.get("sync_info", {}),
                    "statistics": project.get("statistics", {})
                },
                "recent_tasks": recent_tasks
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500


@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/folders')
def get_project_folders_from_db(project_id):
    """
    ä»æ•°æ®åº“è·å–é¡¹ç›®æ–‡ä»¶å¤¹åˆ—è¡¨
    
    GET /api/file-sync-db/project/{project_id}/folders
    
    å‚æ•°:
    - depth: æŒ‡å®šå±‚çº§ (å¯é€‰)
    - parent_id: çˆ¶æ–‡ä»¶å¤¹ID (å¯é€‰)
    - limit: é™åˆ¶æ•°é‡ (é»˜è®¤: 100)
    """
    try:
        depth = request.args.get('depth', type=int)
        parent_id = request.args.get('parent_id')
        limit = request.args.get('limit', 100, type=int)
        
        folders = sync_manager.dal.get_project_folders(
            project_id, 
            depth=depth, 
            parent_id=parent_id, 
            limit=limit
        )
        
        return jsonify({
            "success": True,
            "data": {
                "folders": folders,
                "total_count": len(folders)
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶å¤¹åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"è·å–æ–‡ä»¶å¤¹åˆ—è¡¨å¤±è´¥: {str(e)}"
        }), 500


@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/files')
def get_project_files_from_db(project_id):
    """
    ä»æ•°æ®åº“è·å–é¡¹ç›®æ–‡ä»¶åˆ—è¡¨
    
    GET /api/file-sync-db/project/{project_id}/files
    
    å‚æ•°:
    - folder_id: æ–‡ä»¶å¤¹ID (å¯é€‰)
    - file_type: æ–‡ä»¶ç±»å‹ (å¯é€‰)
    - limit: é™åˆ¶æ•°é‡ (é»˜è®¤: 100)
    """
    try:
        folder_id = request.args.get('folder_id')
        file_type = request.args.get('file_type')
        limit = request.args.get('limit', 100, type=int)
        
        files = sync_manager.dal.get_project_files(
            project_id,
            folder_id=folder_id,
            file_type=file_type,
            limit=limit
        )
        
        return jsonify({
            "success": True,
            "data": {
                "files": files,
                "total_count": len(files)
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"
        }), 500


@file_sync_db_bp.route('/api/file-sync-db/search')
def search_files_and_folders():
    """
    æœç´¢æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
    
    GET /api/file-sync-db/search
    
    å‚æ•°:
    - q: æœç´¢å…³é”®è¯ (å¿…éœ€)
    - project_id: é¡¹ç›®ID (å¿…éœ€)
    - type: æœç´¢ç±»å‹ files/folders/both (é»˜è®¤: both)
    - limit: é™åˆ¶æ•°é‡ (é»˜è®¤: 50)
    """
    try:
        query = request.args.get('q')
        project_id = request.args.get('project_id')
        search_type = request.args.get('type', 'both')
        limit = request.args.get('limit', 50, type=int)
        
        if not query or not project_id:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å‚æ•°: q å’Œ project_id"
            }), 400
        
        results = {}
        
        if search_type in ['files', 'both']:
            files = sync_manager.dal.search_files(project_id, query, limit)
            results['files'] = files
        
        if search_type in ['folders', 'both']:
            folders = sync_manager.dal.search_folders(project_id, query, limit)
            results['folders'] = folders
        
        return jsonify({
            "success": True,
            "data": results
        })
        
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"æœç´¢å¤±è´¥: {str(e)}"
        }), 500


@file_sync_db_bp.route('/api/file-sync-db/health')
def health_check():
    """å¥åº·æ£€æŸ¥API"""
    try:
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        db = sync_manager.dal.connect()
        
        # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "database_connected": True,
            "collections": {
                "projects": db.projects.count_documents({}),
                "folders": db.folders.count_documents({}),
                "files": db.files.count_documents({}),
                "file_versions": db.file_versions.count_documents({}),
                "sync_tasks": db.sync_tasks.count_documents({})
            }
        }
        
        return jsonify({
            "success": True,
            "status": "healthy",
            "data": stats
        })
        
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "status": "unhealthy",
            "error": str(e)
        }), 500


@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/sync-history')
def get_sync_history(project_id):
    """
    Get simplified sync history for a project (only successful syncs)
    
    GET /api/file-sync-db/project/{project_id}/sync-history
    
    Parameters:
    - limit: Number of records to return (default: 20)
    - offset: Number of records to skip (default: 0)
    - sync_type: Filter by sync type (optional: full_sync, batch_sync)
    """
    try:
        limit = request.args.get('limit', 20, type=int)
        offset = request.args.get('offset', 0, type=int)
        sync_type = request.args.get('sync_type')
        
        # ä½¿ç”¨ç®€åŒ–çš„åŒæ­¥å†å²è®°å½•
        records = sync_manager.dal.get_sync_history(
            project_id, 
            limit=limit, 
            offset=offset, 
            sync_type=sync_type
        )
        
        # è·å–æ€»æ•°
        total_count = sync_manager.dal.get_sync_history_count(project_id, sync_type)
        
        return jsonify({
            "success": True,
            "data": {
                "tasks": records,  # ä¿æŒå…¼å®¹æ€§ï¼Œä»ç„¶å«tasks
                "pagination": {
                    "total": total_count,
                    "limit": limit,
                    "offset": offset,
                    "has_more": (offset + limit) < total_count
                }
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–åŒæ­¥å†å²å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"è·å–åŒæ­¥å†å²å¤±è´¥: {str(e)}"
        }), 500


# èª¿è©¦APIå·²ç§»è‡³ç¨ç«‹çš„ä»»å‹™è¿½è¹¤ç³»çµ± /api/task-tracking/stats å’Œ /api/task-tracking/cleanup


@file_sync_db_bp.route('/api/file-sync-db/project/<project_id>/cleanup-tasks', methods=['POST'])
def cleanup_stuck_tasks(project_id):
    """æ¸…ç†å¡ä½çš„åŒæ­¥ä»»åŠ¡"""
    try:
        from datetime import datetime, timedelta
        from bson import ObjectId
        
        db = batch_sync_manager.dal.connect()
        
        # è·å–æ‰€æœ‰runningçŠ¶æ€çš„ä»»åŠ¡
        running_tasks = list(db.sync_tasks.find({
            'task_status': 'running',
            'project_id': project_id
        }).sort('created_at', -1))
        
        if len(running_tasks) <= 1:
            return jsonify({
                "success": True,
                "message": f"æ— éœ€æ¸…ç†ï¼Œåªæœ‰ {len(running_tasks)} ä¸ªè¿è¡Œä¸­çš„ä»»åŠ¡"
            })
        
        # ä¿ç•™æœ€æ–°çš„ï¼Œå…¶ä»–çš„æ ‡è®°ä¸ºå–æ¶ˆ
        latest_task = running_tasks[0]
        cancelled_count = 0
        
        for task in running_tasks[1:]:
            task_id = task['_id']
            
            # ä½¿ç”¨replace_oneæ¥æ›¿æ¢æ•´ä¸ªæ–‡æ¡£
            updated_task = task.copy()
            updated_task['task_status'] = 'cancelled'
            updated_task['end_time'] = datetime.now()
            updated_task['updated_at'] = datetime.now()
            updated_task['results'] = {'error': 'é‡å¤ä»»åŠ¡ï¼Œå·²æ¸…ç†'}
            
            result = db.sync_tasks.replace_one({'_id': task_id}, updated_task)
            if result.modified_count > 0:
                cancelled_count += 1
        
        return jsonify({
            "success": True,
            "message": f"æ¸…ç†å®Œæˆï¼Œä¿ç•™ä»»åŠ¡ {latest_task['_id']}ï¼Œå–æ¶ˆäº† {cancelled_count} ä¸ªé‡å¤ä»»åŠ¡"
        })
        
    except Exception as e:
        logger.error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {str(e)}"
        }), 500
