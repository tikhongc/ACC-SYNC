# -*- coding: utf-8 -*-
"""
å„ªåŒ–çš„åŒæ­¥ç®¡ç†å™¨
å¯¦ç¾é«˜æ€§èƒ½çš„å…¨é‡åŒæ­¥å’Œå¢é‡åŒæ­¥åŠŸèƒ½

æ ¸å¿ƒå„ªåŒ–ç­–ç•¥ï¼š
1. åˆ†å±¤ä¸¦ç™¼BFSéæ­·
2. æ‰¹é‡APIèª¿ç”¨
3. è‡ªé©æ‡‰APIç¯€æµ
4. æ™ºèƒ½ä¸¦ç™¼è™•ç†
5. æ‰¹é‡æ•¸æ“šåº«æ“ä½œ
"""

import time
import logging
from datetime import datetime, timedelta
from collections import deque, defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple, Any, Optional
import threading
import requests

# å°å…¥ç¾æœ‰æ¨¡çµ„
from database.data_access_layer import DataAccessLayer
from database.simplified_sync_schema import (
    CHINA_TZ, get_china_time, format_china_time
)
from .file_sync_api import (
    get_folder_contents, get_item_versions, 
    get_multiple_folder_contents_batch, get_versions_parallel
)
# å¾ file_sync_db_api å°å…¥æ™‚é–“è™•ç†å‡½æ•¸
from .file_sync_db_api import normalize_db_datetime, parse_api_datetime
from database.data_sync_strategy import DataTransformer
import utils

logger = logging.getLogger(__name__)

class OptimizedSyncManager:
    """
    å„ªåŒ–çš„åŒæ­¥ç®¡ç†å™¨
    
    ä¸»è¦æ”¹é€²ï¼š
    - åˆ†å±¤ä¸¦ç™¼BFSéæ­·ï¼Œé¿å…å–®ç·šç¨‹ç“¶é ¸
    - æ‰¹é‡APIèª¿ç”¨ï¼Œæ¸›å°‘ç¶²çµ¡é–‹éŠ·
    - è‡ªé©æ‡‰APIç¯€æµï¼Œå‹•æ…‹èª¿æ•´å»¶é²
    - æ™ºèƒ½ä¸¦ç™¼è™•ç†ï¼Œæœ€å¤§åŒ–è³‡æºåˆ©ç”¨
    """
    
    def __init__(self, batch_size: int = 100, api_delay: float = 0.02, max_workers: int = 6):
        self.dal = DataAccessLayer()
        self.batch_size = batch_size
        self.api_delay = api_delay
        self.max_workers = max_workers
        self.converter = DataTransformer()
        
        # è‡ªé©æ‡‰ç¯€æµç›¸é—œ
        self.adaptive_delay = True
        self.api_response_times = []
        self.max_response_history = 20
        
        # æ€§èƒ½çµ±è¨ˆ
        self.stats = {
            'api_calls': 0,
            'concurrent_operations': 0,
            'batch_operations': 0,
            'total_throttle_time': 0
        }
    
    # ============================================================================
    # å„ªåŒ–çš„å…¨é‡åŒæ­¥
    # ============================================================================
    
    def optimized_full_sync(self, project_id: str, max_depth: int = 10, 
                           include_custom_attributes: bool = True, task_id: str = None) -> dict:
        """
        å„ªåŒ–çš„å…¨é‡åŒæ­¥
        
        æ ¸å¿ƒæ”¹é€²ï¼š
        1. åˆ†å±¤ä¸¦ç™¼BFSæ”¶é›†
        2. æ‰¹é‡APIèª¿ç”¨
        3. è‡ªé©æ‡‰ç¯€æµ
        4. ä¸¦ç™¼æ•¸æ“šè™•ç†
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ é–‹å§‹å„ªåŒ–å…¨é‡åŒæ­¥: {project_id}")
            
            # 1. ç²å–APIè¨ªå•ä»¤ç‰Œ
            access_token = utils.get_access_token()
            if not access_token:
                raise Exception("ç„¡æ³•ç²å–è¨ªå•ä»¤ç‰Œ")
            
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            # 2. æ¸…é™¤ç¾æœ‰æ•¸æ“šï¼ˆå…¨é‡åŒæ­¥ï¼‰
            if task_id:
                self._update_sync_task_progress(task_id, {
                    "current_stage": "clearing_data",
                    "progress_percentage": 5.0,
                    "message": "æ¸…é™¤ç¾æœ‰æ•¸æ“š..."
                })
            
            self.dal.clear_project_data(project_id)
            logger.info(f"âœ… å·²æ¸…é™¤é …ç›®æ•¸æ“š: {project_id}")
            
            # 3. ç²å–é …ç›®é ‚ç´šæ–‡ä»¶å¤¾
            top_folders_data = self._get_top_folders_with_retry(project_id, headers)
            
            # 4. å„ªåŒ–çš„BFSæ”¶é›†æ‰€æœ‰é …ç›®
            if task_id:
                self._update_sync_task_progress(task_id, {
                    "current_stage": "collecting",
                    "progress_percentage": 10.0,
                    "message": "æ”¶é›†é …ç›®çµæ§‹..."
                })
            
            all_folders, all_files = self._collect_all_items_concurrent_bfs(
                project_id, top_folders_data['data'], headers, max_depth, task_id
            )
            
            logger.info(f"ğŸ“Š æ”¶é›†å®Œæˆ: {len(all_folders)} å€‹æ–‡ä»¶å¤¾, {len(all_files)} å€‹æ–‡ä»¶")
            
            # 5. ä¸¦ç™¼è™•ç†æ–‡ä»¶å¤¾
            if task_id:
                self._update_sync_task_progress(task_id, {
                    "current_stage": "processing_folders",
                    "progress_percentage": 60.0,
                    "message": "è™•ç†æ–‡ä»¶å¤¾..."
                })
            
            folder_results = self._enhanced_batch_process_folders(
                all_folders, project_id, include_custom_attributes, headers, task_id
            )
            
            # 6. ä¸¦ç™¼è™•ç†æ–‡ä»¶
            if task_id:
                self._update_sync_task_progress(task_id, {
                    "current_stage": "processing_files",
                    "progress_percentage": 80.0,
                    "message": "è™•ç†æ–‡ä»¶..."
                })
            
            file_results = self._enhanced_batch_process_files(
                all_files, project_id, include_custom_attributes, headers, task_id
            )
            
            # 7. æ›´æ–°é …ç›®åŒæ­¥æ™‚é–“
            self.dal.update_project_sync_status(project_id, "completed", get_china_time())
            
            # 8. è¨ˆç®—çµæœ
            duration = time.time() - start_time
            
            results = {
                "folders_synced": folder_results.get("folders_synced", 0),
                "files_synced": file_results.get("files_synced", 0),
                "versions_synced": file_results.get("versions_synced", 0),
                "total_size": file_results.get("total_size", 0),
                "errors": folder_results.get("errors", []) + file_results.get("errors", [])
            }
            
            # æ€§èƒ½çµ±è¨ˆ
            performance_stats = {
                "duration_seconds": duration,
                "api_calls": self.stats['api_calls'],
                "concurrent_operations": self.stats['concurrent_operations'],
                "batch_operations": self.stats['batch_operations'],
                "avg_api_response_time": sum(self.api_response_times) / len(self.api_response_times) if self.api_response_times else 0,
                "throttle_time": self.stats['total_throttle_time']
            }
            
            logger.info(f"âœ… å„ªåŒ–å…¨é‡åŒæ­¥å®Œæˆ: {duration:.2f}ç§’, APIèª¿ç”¨: {self.stats['api_calls']}")
            
            return {
                "success": True,
                "results": results,
                "performance": performance_stats,
                "optimization_info": {
                    "max_workers": self.max_workers,
                    "batch_size": self.batch_size,
                    "api_delay": self.api_delay,
                    "adaptive_delay": self.adaptive_delay
                }
            }
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"âŒ å„ªåŒ–å…¨é‡åŒæ­¥å¤±æ•—: {str(e)}")
            
            return {
                "success": False,
                "error": str(e),
                "duration_seconds": duration
            }
    
    # ============================================================================
    # å„ªåŒ–çš„å¢é‡åŒæ­¥
    # ============================================================================
    
    def optimized_incremental_sync(self, project_id: str, max_depth: int = 10, 
                                  include_custom_attributes: bool = True, task_id: str = None) -> dict:
        """
        å„ªåŒ–çš„å¢é‡åŒæ­¥
        
        æ ¸å¿ƒæ”¹é€²ï¼š
        1. åˆ†å±¤ä¸¦ç™¼è®Šæ›´æª¢æ¸¬
        2. æ™ºèƒ½æ™‚é–“æˆ³æ¯”è¼ƒ
        3. æ‰¹é‡è®Šæ›´è™•ç†
        4. ä¸¦ç™¼APIèª¿ç”¨
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ”„ é–‹å§‹å„ªåŒ–å¢é‡åŒæ­¥: {project_id}")
            
            # 1. ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“
            last_sync_time = self._get_last_sync_time(project_id)
            logger.info(f"ğŸ“… ä¸Šæ¬¡åŒæ­¥æ™‚é–“: {format_china_time(last_sync_time) if last_sync_time else 'None'}")
            
            # 2. ç²å–APIè¨ªå•ä»¤ç‰Œ
            access_token = utils.get_access_token()
            if not access_token:
                raise Exception("ç„¡æ³•ç²å–è¨ªå•ä»¤ç‰Œ")
            
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            # 3. ç²å–é …ç›®é ‚ç´šæ–‡ä»¶å¤¾
            top_folders_data = self._get_top_folders_with_retry(project_id, headers)
            
            # 4. æª¢æŸ¥æ˜¯å¦æœ‰åŸºæº–æ™‚é–“
            if not last_sync_time:
                logger.warning("âš ï¸ æ²’æœ‰ä¸Šæ¬¡åŒæ­¥æ™‚é–“ï¼Œç„¡æ³•åŸ·è¡Œå¢é‡åŒæ­¥")
                return {
                    "success": False,
                    "error": "æ²’æœ‰ä¸Šæ¬¡åŒæ­¥æ™‚é–“è¨˜éŒ„ï¼Œè«‹å…ˆåŸ·è¡Œå…¨é‡åŒæ­¥",
                    "message": "éœ€è¦å…ˆåŸ·è¡Œå…¨é‡åŒæ­¥ä¾†å»ºç«‹åŸºæº–æ™‚é–“",
                    "duration_seconds": time.time() - start_time
                }
            
            # 5. å„ªåŒ–çš„è®Šæ›´æª¢æ¸¬
            if task_id:
                self._update_sync_task_progress(task_id, {
                    "current_stage": "checking_changes",
                    "progress_percentage": 10.0,
                    "message": "æª¢æŸ¥è®Šæ›´..."
                })
            
            changed_folders, changed_files = self._collect_changed_items_concurrent(
                project_id, top_folders_data['data'], headers, max_depth, last_sync_time, task_id
            )
            
            # 6. å¦‚æœæ²’æœ‰è®Šæ›´ï¼Œç›´æ¥è¿”å›
            if not changed_folders and not changed_files:
                logger.info("ğŸ“‹ æ²’æœ‰æª¢æ¸¬åˆ°è®Šæ›´ï¼Œè·³éåŒæ­¥")
                return {
                    "success": True,
                    "message": "æ²’æœ‰è®Šæ›´éœ€è¦åŒæ­¥",
                    "results": {
                        "folders_synced": 0,
                        "files_synced": 0,
                        "versions_synced": 0,
                        "total_size": 0,
                        "errors": []
                    },
                    "performance": {
                        "duration_seconds": time.time() - start_time,
                        "api_calls": self.stats['api_calls']
                    }
                }
            
            logger.info(f"ğŸ“Š æª¢æ¸¬åˆ°è®Šæ›´: {len(changed_folders)} å€‹æ–‡ä»¶å¤¾, {len(changed_files)} å€‹æ–‡ä»¶")
            
            # 6. ä¸¦ç™¼åŒæ­¥è®Šæ›´çš„é …ç›®
            if task_id:
                self._update_sync_task_progress(task_id, {
                    "current_stage": "syncing_changes",
                    "progress_percentage": 50.0,
                    "message": "åŒæ­¥è®Šæ›´..."
                })
            
            sync_results = self._sync_changed_items_concurrent(
                project_id, changed_folders, changed_files, headers, include_custom_attributes, task_id
            )
            
            # 7. æ›´æ–°é …ç›®åŒæ­¥æ™‚é–“
            self.dal.update_project_sync_status(project_id, "completed", get_china_time())
            
            # 8. è¨ˆç®—çµæœ
            duration = time.time() - start_time
            
            performance_stats = {
                "duration_seconds": duration,
                "api_calls": self.stats['api_calls'],
                "concurrent_operations": self.stats['concurrent_operations'],
                "changes_detected": len(changed_folders) + len(changed_files),
                "smart_skips": self.stats.get('smart_skips', 0),
                "avg_api_response_time": sum(self.api_response_times) / len(self.api_response_times) if self.api_response_times else 0,
                "optimization_efficiency": self._calculate_optimization_efficiency()
            }
            
            logger.info(f"âœ… å„ªåŒ–å¢é‡åŒæ­¥å®Œæˆ: {duration:.2f}ç§’, è®Šæ›´: {len(changed_folders) + len(changed_files)}, æ™ºèƒ½è·³é: {self.stats.get('smart_skips', 0)}, å„ªåŒ–æ•ˆç‡: {performance_stats['optimization_efficiency']}%")
            
            return {
                "success": True,
                "results": sync_results,
                "performance": performance_stats,
                "optimization_info": {
                    "max_workers": self.max_workers,
                    "changes_detected": len(changed_folders) + len(changed_files)
                }
            }
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"âŒ å„ªåŒ–å¢é‡åŒæ­¥å¤±æ•—: {str(e)}")
            
            return {
                "success": False,
                "error": str(e),
                "duration_seconds": duration
            }
    
    # ============================================================================
    # åˆ†å±¤ä¸¦ç™¼BFSæ”¶é›†
    # ============================================================================
    
    def _collect_all_items_concurrent_bfs(self, project_id: str, top_folders: List[dict], 
                                         headers: dict, max_depth: int, task_id: str = None) -> Tuple[List, List]:
        """
        åˆ†å±¤ä¸¦ç™¼çš„BFSæ”¶é›†æ‰€æœ‰é …ç›®
        
        å„ªåŒ–ç­–ç•¥ï¼š
        1. æŒ‰å±¤ç´šè™•ç†ï¼Œæ¯å±¤å…§éƒ¨ä¸¦ç™¼
        2. æ‰¹é‡ç²å–æ–‡ä»¶å¤¾å…§å®¹
        3. è‡ªé©æ‡‰APIç¯€æµ
        """
        all_folders = []
        all_files = []
        
        # æŒ‰å±¤ç´šè™•ç†
        current_level = [(folder, 0, "") for folder in top_folders]
        processed_levels = 0
        
        while current_level and processed_levels < max_depth:
            logger.info(f"ğŸ”„ è™•ç†ç¬¬ {processed_levels + 1} å±¤ï¼Œå…± {len(current_level)} å€‹æ–‡ä»¶å¤¾")
            
            next_level = []
            
            # ç•¶å‰å±¤ç´šä¸¦ç™¼è™•ç†
            level_folders, level_files, level_subfolders = self._process_level_concurrent(
                project_id, current_level, headers, processed_levels
            )
            
            all_folders.extend(level_folders)
            all_files.extend(level_files)
            next_level.extend(level_subfolders)
            
            # æ›´æ–°é€²åº¦
            if task_id:
                progress = min(10 + (processed_levels / max_depth) * 40, 50)
                self._update_sync_task_progress(task_id, {
                    "current_stage": "collecting",
                    "progress_percentage": progress,
                    "processed_levels": processed_levels + 1,
                    "collected_folders": len(all_folders),
                    "collected_files": len(all_files)
                })
            
            current_level = next_level
            processed_levels += 1
        
        logger.info(f"ğŸ“Š BFSæ”¶é›†å®Œæˆ: {len(all_folders)} å€‹æ–‡ä»¶å¤¾, {len(all_files)} å€‹æ–‡ä»¶")
        return all_folders, all_files
    
    def _process_level_concurrent(self, project_id: str, level_folders: List[Tuple], 
                                 headers: dict, depth: int) -> Tuple[List, List, List]:
        """ä¸¦ç™¼è™•ç†å–®å€‹å±¤ç´šçš„æ‰€æœ‰æ–‡ä»¶å¤¾"""
        level_folder_results = []
        level_file_results = []
        level_subfolder_results = []
        
        # æå–æ–‡ä»¶å¤¾IDé€²è¡Œæ‰¹é‡è™•ç†
        folder_ids = [folder_info[0].get('id') for folder_info in level_folders]
        
        # æ‰¹é‡ç²å–æ–‡ä»¶å¤¾å…§å®¹
        start_time = time.time()
        contents_batch = get_multiple_folder_contents_batch(project_id, folder_ids, headers)
        api_time = time.time() - start_time
        
        self.stats['api_calls'] += len(folder_ids)
        self.stats['batch_operations'] += 1
        self._record_api_response_time(api_time)
        
        # ä¸¦ç™¼è™•ç†çµæœ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._process_single_folder_content, folder_info, contents_batch, depth): folder_info
                for folder_info in level_folders
            }
            
            for future in as_completed(futures):
                try:
                    folder_data, file_data, subfolder_data = future.result()
                    level_folder_results.extend(folder_data)
                    level_file_results.extend(file_data)
                    level_subfolder_results.extend(subfolder_data)
                    self.stats['concurrent_operations'] += 1
                except Exception as e:
                    logger.error(f"è™•ç†æ–‡ä»¶å¤¾å…§å®¹å¤±æ•—: {e}")
        
        # è‡ªé©æ‡‰ç¯€æµ
        self._adaptive_api_throttle()
        
        return level_folder_results, level_file_results, level_subfolder_results
    
    def _process_single_folder_content(self, folder_info: Tuple, contents_batch: dict, 
                                      depth: int) -> Tuple[List, List, List]:
        """è™•ç†å–®å€‹æ–‡ä»¶å¤¾çš„å…§å®¹"""
        folder_data, current_depth, parent_path = folder_info
        folder_id = folder_data.get('id')
        folder_name = folder_data.get('attributes', {}).get('displayName', 'Unknown')
        current_path = f"{parent_path}/{folder_name}" if parent_path else folder_name
        
        folder_results = [(folder_data, current_depth, parent_path)]
        file_results = []
        subfolder_results = []
        
        # ç²å–è©²æ–‡ä»¶å¤¾çš„å…§å®¹
        contents_data = contents_batch.get(folder_id, {})
        if contents_data and contents_data.get('data'):
            for item in contents_data['data']:
                item_type = item.get('type')
                if item_type == 'folders':
                    subfolder_results.append((item, current_depth + 1, current_path))
                else:
                    file_results.append((item, folder_id, current_path, current_depth))
        
        return folder_results, file_results, subfolder_results
    
    # ============================================================================
    # å„ªåŒ–çš„å¢é‡è®Šæ›´æª¢æ¸¬
    # ============================================================================
    
    def _collect_changed_items_concurrent(self, project_id: str, top_folders: List[dict], 
                                         headers: dict, max_depth: int, last_sync_time, 
                                         task_id: str = None) -> Tuple[List, List]:
        """
        ä¸¦ç™¼æª¢æ¸¬è®Šæ›´é …ç›®
        
        å„ªåŒ–ç­–ç•¥ï¼š
        1. åˆ†å±¤ä¸¦ç™¼æª¢æ¸¬
        2. æ™ºèƒ½æ™‚é–“æˆ³æ¯”è¼ƒ
        3. æ‰¹é‡APIèª¿ç”¨
        """
        # æ¨™æº–åŒ–åŸºæº–æ™‚é–“
        logger.info(f"ğŸ” åŸå§‹ last_sync_time: {last_sync_time}, é¡å‹: {type(last_sync_time)}")
        last_sync_time = self._normalize_sync_time(last_sync_time)
        logger.info(f"ğŸ” æ¨™æº–åŒ–å¾Œ last_sync_time: {last_sync_time}, é¡å‹: {type(last_sync_time)}")
        
        if last_sync_time:
            logger.info(f"ğŸ” åŸºæº–æ™‚é–“æˆ³: {self._get_timestamp(last_sync_time)}")
        else:
            logger.warning("âš ï¸ åŸºæº–æ™‚é–“ç‚ºç©ºï¼Œå°‡ä½¿ç”¨7å¤©å‰")
        
        changed_folders = []
        changed_files = []
        
        # æŒ‰å±¤ç´šæª¢æ¸¬è®Šæ›´
        current_level = [(folder, 0, "") for folder in top_folders]
        processed_levels = 0
        
        while current_level and processed_levels < max_depth:
            logger.info(f"ğŸ” æª¢æ¸¬ç¬¬ {processed_levels + 1} å±¤è®Šæ›´ï¼Œå…± {len(current_level)} å€‹æ–‡ä»¶å¤¾")
            
            next_level = []
            
            # ç•¶å‰å±¤ç´šä¸¦ç™¼æª¢æ¸¬
            level_changed_folders, level_changed_files, level_subfolders = self._detect_level_changes_concurrent(
                project_id, current_level, headers, last_sync_time, processed_levels
            )
            
            changed_folders.extend(level_changed_folders)
            changed_files.extend(level_changed_files)
            next_level.extend(level_subfolders)
            
            # æ›´æ–°é€²åº¦
            if task_id:
                progress = min(10 + (processed_levels / max_depth) * 20, 30)
                self._update_sync_task_progress(task_id, {
                    "current_stage": "checking_changes",
                    "progress_percentage": progress,
                    "processed_levels": processed_levels + 1,
                    "changed_folders": len(changed_folders),
                    "changed_files": len(changed_files)
                })
            
            current_level = next_level
            processed_levels += 1
        
        logger.info(f"ğŸ” è®Šæ›´æª¢æ¸¬å®Œæˆ: {len(changed_folders)} å€‹æ–‡ä»¶å¤¾, {len(changed_files)} å€‹æ–‡ä»¶")
        return changed_folders, changed_files
    
    def _detect_level_changes_concurrent(self, project_id: str, level_folders: List[Tuple], 
                                        headers: dict, last_sync_time, depth: int) -> Tuple[List, List, List]:
        """ä¸¦ç™¼æª¢æ¸¬å–®å€‹å±¤ç´šçš„è®Šæ›´"""
        level_changed_folders = []
        level_changed_files = []
        level_subfolders = []
        
        # æ‰¹é‡ç²å–æ–‡ä»¶å¤¾å…§å®¹
        folder_ids = [folder_info[0].get('id') for folder_info in level_folders]
        contents_batch = get_multiple_folder_contents_batch(project_id, folder_ids, headers)
        
        self.stats['api_calls'] += len(folder_ids)
        self.stats['batch_operations'] += 1
        
        # ä¸¦ç™¼æª¢æ¸¬è®Šæ›´
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._detect_folder_changes, folder_info, contents_batch, last_sync_time, depth): folder_info
                for folder_info in level_folders
            }
            
            for future in as_completed(futures):
                try:
                    folder_changes, file_changes, subfolders = future.result()
                    level_changed_folders.extend(folder_changes)
                    level_changed_files.extend(file_changes)
                    level_subfolders.extend(subfolders)
                    self.stats['concurrent_operations'] += 1
                except Exception as e:
                    logger.error(f"æª¢æ¸¬æ–‡ä»¶å¤¾è®Šæ›´å¤±æ•—: {e}")
        
        return level_changed_folders, level_changed_files, level_subfolders
    
    def _detect_folder_changes(self, folder_info: Tuple, contents_batch: dict, 
                              last_sync_time, depth: int) -> Tuple[List, List, List]:
        """æª¢æ¸¬å–®å€‹æ–‡ä»¶å¤¾çš„è®Šæ›´ - å„ªåŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ last_modified_time_rollup æ™ºèƒ½è·³é"""
        folder_data, current_depth, parent_path = folder_info
        folder_id = folder_data.get('id')
        folder_name = folder_data.get('attributes', {}).get('displayName', 'Unknown')
        current_path = f"{parent_path}/{folder_name}" if parent_path else folder_name
        
        changed_folders = []
        changed_files = []
        subfolders = []
        
        # ğŸš€ æ ¸å¿ƒå„ªåŒ–ï¼šä½¿ç”¨ last_modified_time_rollup é€²è¡Œæ™ºèƒ½åˆ†æ”¯è·³é
        folder_rollup_time = self._parse_datetime(folder_data.get('attributes', {}).get('lastModifiedTimeRollup'))
        
        # If entire branch (including all sub-items) has no changes, skip directly
        if folder_rollup_time and not self._is_changed(folder_rollup_time, last_sync_time):
            logger.debug(f"Smart skip branch: {folder_name} (rollup time: {folder_rollup_time} <= base time: {last_sync_time})")
            self.stats.setdefault('smart_skips', 0)
            self.stats['smart_skips'] += 1
            return changed_folders, changed_files, subfolders
        
        logger.debug(f"Branch has changes, continue checking: {folder_name} (rollup time: {folder_rollup_time} > base time: {last_sync_time})")
        
        # Check if folder itself has changes
        folder_last_modified = self._parse_datetime(folder_data.get('attributes', {}).get('lastModifiedTime'))
        if self._is_changed(folder_last_modified, last_sync_time):
            changed_folders.append((folder_data, current_depth, parent_path))
            logger.debug(f"Folder has changes: {folder_name}")
        
        # æª¢æŸ¥æ–‡ä»¶å¤¾å…§å®¹
        contents_data = contents_batch.get(folder_id, {})
        if contents_data and contents_data.get('data'):
            for item in contents_data['data']:
                item_type = item.get('type')
                
                if item_type == 'folders':
                    # Check subfolder rollup time for smart skipping
                    subfolder_rollup_time = self._parse_datetime(item.get('attributes', {}).get('lastModifiedTimeRollup'))
                    if subfolder_rollup_time and self._is_changed(subfolder_rollup_time, last_sync_time):
                    subfolders.append((item, current_depth + 1, current_path))
                        logger.debug(f"Subfolder has changes: {item.get('attributes', {}).get('displayName', 'Unknown')}")
                else:
                        logger.debug(f"Skipping subfolder branch: {item.get('attributes', {}).get('displayName', 'Unknown')}")
                        self.stats.setdefault('smart_skips', 0)
                        self.stats['smart_skips'] += 1
                else:
                    # Check if file has changes
                    item_last_modified = self._parse_datetime(item.get('attributes', {}).get('lastModifiedTime'))
                    if self._is_changed(item_last_modified, last_sync_time):
                        changed_files.append((item, folder_id, current_path, current_depth))
                        logger.debug(f"File has changes: {item.get('attributes', {}).get('displayName', 'Unknown')}")
        
        return changed_folders, changed_files, subfolders
    
    # ============================================================================
    # å¢å¼·çš„ä¸¦ç™¼è™•ç†
    # ============================================================================
    
    def _enhanced_batch_process_folders(self, folders_with_context: List[Tuple], project_id: str,
                                       include_custom_attributes: bool, headers: dict, task_id: str) -> dict:
        """å¢å¼·çš„æ–‡ä»¶å¤¾æ‰¹é‡è™•ç†"""
        results = {"folders_synced": 0, "errors": []}
        
        if not folders_with_context:
            return results
        
        # åˆ†æ‰¹è™•ç†ï¼Œé¿å…éå¤§çš„ä¸¦ç™¼
        batch_size = min(self.batch_size, len(folders_with_context))
        batches = [folders_with_context[i:i+batch_size] for i in range(0, len(folders_with_context), batch_size)]
        
        for batch_idx, batch in enumerate(batches):
            logger.info(f"ğŸ”„ è™•ç†æ–‡ä»¶å¤¾æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}, å¤§å°: {len(batch)}")
            
            # ä¸¦ç™¼è™•ç†ç•¶å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(self._process_single_folder, folder_info, project_id, headers): folder_info
                    for folder_info in batch
                }
                
                for future in as_completed(futures):
                    try:
                        folder_result = future.result()
                        if folder_result.get('success'):
                            results["folders_synced"] += 1
                        else:
                            results["errors"].append(folder_result.get('error'))
                        self.stats['concurrent_operations'] += 1
                    except Exception as e:
                        logger.error(f"è™•ç†æ–‡ä»¶å¤¾å¤±æ•—: {e}")
                        results["errors"].append(str(e))
            
            # æ‰¹é‡æ•¸æ“šåº«æ“ä½œ
            self.stats['batch_operations'] += 1
            
            # æ›´æ–°é€²åº¦
            if task_id:
                progress = 60 + (batch_idx + 1) / len(batches) * 15
                self._update_sync_task_progress(task_id, {
                    "current_stage": "processing_folders",
                    "progress_percentage": progress,
                    "processed_batches": batch_idx + 1,
                    "total_batches": len(batches)
                })
        
        logger.info(f"âœ… æ–‡ä»¶å¤¾è™•ç†å®Œæˆ: {results['folders_synced']} å€‹æˆåŠŸ, {len(results['errors'])} å€‹éŒ¯èª¤")
        return results
    
    def _enhanced_batch_process_files(self, files_with_context: List[Tuple], project_id: str,
                                     include_custom_attributes: bool, headers: dict, task_id: str) -> dict:
        """å¢å¼·çš„æ–‡ä»¶æ‰¹é‡è™•ç†"""
        results = {"files_synced": 0, "versions_synced": 0, "total_size": 0, "errors": []}
        
        if not files_with_context:
            return results
        
        # æŒ‰æ–‡ä»¶å¤¾åˆ†çµ„ï¼Œä¾¿æ–¼æ‰¹é‡è™•ç†
        files_by_folder = defaultdict(list)
        for file_data, folder_id, folder_path, depth in files_with_context:
            files_by_folder[folder_id].append((file_data, folder_path, depth))
        
        # ä¸¦ç™¼è™•ç†å¤šå€‹æ–‡ä»¶å¤¾
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._process_folder_files, project_id, folder_id, files, headers, include_custom_attributes): folder_id
                for folder_id, files in files_by_folder.items()
            }
            
            processed_folders = 0
            total_folders = len(files_by_folder)
            
            for future in as_completed(futures):
                try:
                    folder_results = future.result()
                    results["files_synced"] += folder_results["files_synced"]
                    results["versions_synced"] += folder_results["versions_synced"]
                    results["total_size"] += folder_results["total_size"]
                    results["errors"].extend(folder_results["errors"])
                    
                    processed_folders += 1
                    self.stats['concurrent_operations'] += 1
                    
                    # æ›´æ–°é€²åº¦
                    if task_id:
                        progress = 80 + (processed_folders / total_folders) * 15
                        self._update_sync_task_progress(task_id, {
                            "current_stage": "processing_files",
                            "progress_percentage": progress,
                            "processed_folders": processed_folders,
                            "total_folders": total_folders
                        })
                        
                except Exception as e:
                    logger.error(f"è™•ç†æ–‡ä»¶å¤¾æ–‡ä»¶å¤±æ•—: {e}")
                    results["errors"].append(str(e))
        
        logger.info(f"âœ… æ–‡ä»¶è™•ç†å®Œæˆ: {results['files_synced']} å€‹æ–‡ä»¶, {results['versions_synced']} å€‹ç‰ˆæœ¬")
        return results
    
    def _process_folder_files(self, project_id: str, folder_id: str, files: List[Tuple], 
                             headers: dict, include_custom_attributes: bool) -> dict:
        """è™•ç†å–®å€‹æ–‡ä»¶å¤¾å…§çš„æ‰€æœ‰æ–‡ä»¶"""
        results = {"files_synced": 0, "versions_synced": 0, "total_size": 0, "errors": []}
        
        try:
            # æå–æ–‡ä»¶ID
            file_ids = [file_data.get('id') for file_data, _, _ in files if file_data.get('id')]
            
            # æ‰¹é‡ç²å–ç‰ˆæœ¬ä¿¡æ¯
            versions_data = get_versions_parallel(project_id, file_ids, headers)
            self.stats['api_calls'] += len(file_ids)
            self.stats['batch_operations'] += 1
            
            # è™•ç†æ¯å€‹æ–‡ä»¶
            for file_data, folder_path, depth in files:
                file_id = file_data.get('id')
                if file_id in versions_data:
                    # è½‰æ›ä¸¦ä¿å­˜æ–‡ä»¶æ•¸æ“š
                    converted_file = self.converter.transform_file_data(
                        file_data, project_id, folder_id, folder_path, depth
                    )
                    
                    # ä¿å­˜åˆ°æ•¸æ“šåº«
                    self.dal.create_or_update_file(converted_file)
                    results["files_synced"] += 1
                    
                    # ä¿å­˜æ–‡ä»¶ç‰ˆæœ¬ - é—œéµä¿®å¾©ï¼
                    versions = versions_data[file_id]  # get_item_versionså·²ç¶“è¿”å›dataåˆ—è¡¨
                    for version in versions:
                        try:
                            version_doc = self.converter.transform_version_data(
                                version, file_id, project_id
                            )
                            
                            if self.dal.create_or_update_file_version(version_doc):
                                results["versions_synced"] += 1
                                
                            # è¨ˆç®—æ–‡ä»¶å¤§å°
                            size = version.get('attributes', {}).get('storageSize', 0)
                            if size:
                                results["total_size"] += size
                                
                        except Exception as ve:
                            logger.error(f"ä¿å­˜ç‰ˆæœ¬å¤±æ•— {file_id}: {str(ve)}")
                            results["errors"].append(f"ç‰ˆæœ¬ä¿å­˜å¤±æ•—: {str(ve)}")
            
        except Exception as e:
            logger.error(f"è™•ç†æ–‡ä»¶å¤¾æ–‡ä»¶å¤±æ•— {folder_id}: {e}")
            results["errors"].append(str(e))
        
        return results
    
    # ============================================================================
    # è¼”åŠ©æ–¹æ³•
    # ============================================================================
    
    def _normalize_sync_time(self, last_sync_time):
        """æ¨™æº–åŒ–åŒæ­¥æ™‚é–“"""
        try:
            if last_sync_time:
                # å¦‚æœå·²ç¶“æ˜¯datetimeå°è±¡ä¸”æœ‰æ™‚å€ä¿¡æ¯ï¼Œç›´æ¥è¿”å›
                if isinstance(last_sync_time, datetime):
                    if last_sync_time.tzinfo:
                        return last_sync_time.astimezone(CHINA_TZ)
                    else:
                        # å‡è¨­ç‚ºUTCæ™‚é–“
                        return pytz.utc.localize(last_sync_time).astimezone(CHINA_TZ)
                elif hasattr(last_sync_time, 'timestamp'):
                    timestamp = last_sync_time.timestamp()
                    return datetime.fromtimestamp(timestamp, CHINA_TZ)
                else:
                    # å­—ç¬¦ä¸²æ ¼å¼ï¼Œå˜—è©¦è§£æ
                    parsed = self._parse_datetime(last_sync_time)
                    if parsed:
                        return parsed
                    else:
                        return normalize_db_datetime(last_sync_time)
            else:
                # å¦‚æœæ²’æœ‰åŸºæº–æ™‚é–“ï¼Œè¿”å›Noneè€Œä¸æ˜¯7å¤©å‰
                logger.warning("âš ï¸ æ²’æœ‰åŸºæº–æ™‚é–“ï¼Œå¢é‡åŒæ­¥å°‡è·³é")
                return None
        except Exception as e:
            logger.error(f"æ™‚é–“æ¨™æº–åŒ–å¤±æ•—: {e}")
            # è¿”å›Noneè€Œä¸æ˜¯7å¤©å‰
            return None
    
    def _is_changed(self, item_time, base_time):
        """æª¢æŸ¥é …ç›®æ˜¯å¦æœ‰è®Šæ›´"""
        if not item_time or not base_time:
            return False
        
        try:
            item_timestamp = self._get_timestamp(item_time)
            base_timestamp = self._get_timestamp(base_time)
            
            return item_timestamp and base_timestamp and item_timestamp > base_timestamp
        except Exception as e:
            logger.error(f"æ™‚é–“æ¯”è¼ƒå¤±æ•—: {e}")
            return False
    
    def _get_timestamp(self, dt):
        """ç²å–datetimeå°è±¡çš„timestamp"""
        if not dt:
            return None
        
        try:
            if hasattr(dt, 'timestamp'):
                return dt.timestamp()
            
            if hasattr(dt, 'tzinfo') and dt.tzinfo is None:
                import pytz
                utc_dt = pytz.utc.localize(dt)
                return utc_dt.timestamp()
            
            if hasattr(dt, 'tzinfo') and dt.tzinfo is not None:
                return dt.timestamp()
            
            # MongoDB datetimeå°è±¡è™•ç†
            import pytz
            utc_dt = pytz.utc.localize(dt)
            return utc_dt.timestamp()
            
        except Exception as e:
            logger.warning(f"ç²å–timestampå¤±æ•—: {dt}, éŒ¯èª¤: {str(e)}")
            return None
    
    def _parse_datetime(self, datetime_str):
        """è§£ææ—¥æœŸæ™‚é–“å­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç¨®æ ¼å¼"""
        if not datetime_str:
            return None
        
        try:
            # é¦–å…ˆå˜—è©¦ä½¿ç”¨åŸæœ‰çš„APIè§£æå‡½æ•¸
            result = parse_api_datetime(datetime_str)
            if result:
                return result
        except:
            pass
        
        try:
            # å˜—è©¦è§£æRFC 2822æ ¼å¼ï¼ˆå¦‚ï¼šWed, 05 Nov 2025 09:47:55 GMTï¼‰
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(datetime_str)
            return dt.astimezone(CHINA_TZ)
        except:
            pass
        
        try:
            # å˜—è©¦å…¶ä»–å¸¸è¦‹æ ¼å¼
            import dateutil.parser
            dt = dateutil.parser.parse(datetime_str)
            if dt.tzinfo is None:
                dt = pytz.utc.localize(dt)
            return dt.astimezone(CHINA_TZ)
        except:
            pass
        
        logger.warning(f"ç„¡æ³•è§£ææ—¥æœŸæ™‚é–“: {datetime_str}")
        return None
    
    def _adaptive_api_throttle(self):
        """è‡ªé©æ‡‰APIç¯€æµ"""
        if not self.adaptive_delay:
            return
        
        if len(self.api_response_times) > 5:
            avg_response_time = sum(self.api_response_times[-5:]) / 5
            
            if avg_response_time > 2.0:
                # APIéŸ¿æ‡‰æ…¢ï¼Œå¢åŠ å»¶é²
                delay = 0.1
            elif avg_response_time > 1.0:
                delay = 0.05
            elif avg_response_time < 0.3:
                # APIéŸ¿æ‡‰å¿«ï¼Œæ¸›å°‘å»¶é²
                delay = 0.01
            else:
                delay = self.api_delay
            
            if delay > 0:
                time.sleep(delay)
                self.stats['total_throttle_time'] += delay
    
    def _record_api_response_time(self, response_time):
        """è¨˜éŒ„APIéŸ¿æ‡‰æ™‚é–“"""
        self.api_response_times.append(response_time)
        if len(self.api_response_times) > self.max_response_history:
            self.api_response_times.pop(0)
    
    def _get_top_folders_with_retry(self, project_id: str, headers: dict):
        """å¸¶é‡è©¦çš„é ‚ç´šæ–‡ä»¶å¤¾ç²å–"""
        from .file_sync_api import get_project_top_folders
        
        for attempt in range(3):
            try:
                result = get_project_top_folders(project_id, headers)
                if result and result.get('data'):
                    return result
            except Exception as e:
                logger.warning(f"ç²å–é ‚ç´šæ–‡ä»¶å¤¾å¤±æ•— (å˜—è©¦ {attempt + 1}/3): {e}")
                if attempt < 2:
                    time.sleep(1)
        
        raise Exception("ç„¡æ³•ç²å–é …ç›®é ‚ç´šæ–‡ä»¶å¤¾")
    
    def _get_last_sync_time(self, project_id: str):
        """ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“ï¼Œçµ±ä¸€ä½¿ç”¨ä¸­åœ‹æ™‚å€"""
        try:
            db = self.dal.connect()
            project = db.projects.find_one({"_id": project_id})
            
            if project and project.get("sync_info") and project["sync_info"].get("last_sync_time"):
                last_sync = project["sync_info"]["last_sync_time"]
                # ä½¿ç”¨çµ±ä¸€çš„æ™‚é–“æ¨™æº–åŒ–å‡½æ•¸
                from api_modules.file_sync_db_api import normalize_db_datetime
                normalized_time = normalize_db_datetime(last_sync)
                logger.info(f"ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“: {last_sync} -> {normalized_time}")
                return normalized_time
            else:
                logger.info("é …ç›®æ²’æœ‰ä¸Šæ¬¡åŒæ­¥æ™‚é–“è¨˜éŒ„")
                return None
        except Exception as e:
            logger.error(f"ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“å¤±æ•—: {e}")
            return None
    
    def _process_single_folder(self, folder_info: Tuple, project_id: str, headers: dict) -> dict:
        """è™•ç†å–®å€‹æ–‡ä»¶å¤¾"""
        try:
            folder_data, depth, parent_path = folder_info
            
            # è½‰æ›æ–‡ä»¶å¤¾æ•¸æ“š
            converted_folder = self.converter.transform_folder_data(folder_data, project_id, None, parent_path, depth)
            
            # ä¿å­˜åˆ°æ•¸æ“šåº«
            self.dal.create_or_update_folder(converted_folder)
            
            return {"success": True}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _sync_changed_items_concurrent(self, project_id: str, changed_folders: List, 
                                      changed_files: List, headers: dict, 
                                      include_custom_attributes: bool, task_id: str) -> dict:
        """ä¸¦ç™¼åŒæ­¥è®Šæ›´çš„é …ç›®"""
        results = {
            "folders_synced": 0,
            "files_synced": 0,
            "versions_synced": 0,
            "total_size": 0,
            "errors": []
        }
        
        # ä¸¦ç™¼è™•ç†æ–‡ä»¶å¤¾
        if changed_folders:
            folder_results = self._enhanced_batch_process_folders(
                changed_folders, project_id, include_custom_attributes, headers, task_id
            )
            results["folders_synced"] = folder_results["folders_synced"]
            results["errors"].extend(folder_results["errors"])
        
        # ä¸¦ç™¼è™•ç†æ–‡ä»¶
        if changed_files:
            file_results = self._enhanced_batch_process_files(
                changed_files, project_id, include_custom_attributes, headers, task_id
            )
            results["files_synced"] = file_results["files_synced"]
            results["versions_synced"] = file_results["versions_synced"]
            results["total_size"] = file_results["total_size"]
            results["errors"].extend(file_results["errors"])
        
        return results
    
    def _update_sync_task_progress(self, task_id: str, progress_data: dict):
        """æ›´æ–°åŒæ­¥ä»»å‹™é€²åº¦"""
        try:
            from api_modules.task_lifecycle_manager import task_manager
            task_manager.update_task(task_id, {"progress": progress_data})
        except Exception as e:
            logger.warning(f"æ›´æ–°ä»»å‹™é€²åº¦å¤±æ•—: {str(e)}")
    
    def _calculate_optimization_efficiency(self) -> float:
        """
        è¨ˆç®—å„ªåŒ–æ•ˆç‡
        
        Returns:
            float: å„ªåŒ–æ•ˆç‡ç™¾åˆ†æ¯” (0-100)
        """
        try:
            total_operations = self.stats.get('concurrent_operations', 0) + self.stats.get('smart_skips', 0)
            smart_skips = self.stats.get('smart_skips', 0)
            
            if total_operations == 0:
                return 0.0
            
            # è¨ˆç®—è·³éçš„æ¯”ä¾‹ä½œç‚ºå„ªåŒ–æ•ˆç‡
            efficiency = (smart_skips / total_operations) * 100
            return round(efficiency, 2)
            
        except Exception as e:
            logger.warning(f"è¨ˆç®—å„ªåŒ–æ•ˆç‡å¤±æ•—: {str(e)}")
            return 0.0


# ============================================================================
# å„ªåŒ–åŒæ­¥ç®¡ç†å™¨å¯¦ä¾‹
# ============================================================================

# å‰µå»ºå…¨å±€å„ªåŒ–åŒæ­¥ç®¡ç†å™¨å¯¦ä¾‹
optimized_sync_manager = OptimizedSyncManager(
    batch_size=100,
    api_delay=0.02,  # æ›´å°çš„å»¶é²
    max_workers=6    # æ›´å¤šçš„ä¸¦ç™¼ç·šç¨‹
)

# é«˜æ€§èƒ½é…ç½®çš„å¯¦ä¾‹
high_performance_sync_manager = OptimizedSyncManager(
    batch_size=150,
    api_delay=0.01,  # æœ€å°å»¶é²
    max_workers=8    # æœ€å¤§ä¸¦ç™¼
)

# ä¿å®ˆé…ç½®çš„å¯¦ä¾‹ï¼ˆé©ç”¨æ–¼APIé™åˆ¶è¼ƒåš´æ ¼çš„ç’°å¢ƒï¼‰
conservative_sync_manager = OptimizedSyncManager(
    batch_size=50,
    api_delay=0.05,
    max_workers=3
)
