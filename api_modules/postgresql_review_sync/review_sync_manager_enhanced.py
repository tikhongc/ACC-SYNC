"""
å®¡æ‰¹ç³»ç»ŸåŒæ­¥ç®¡ç†å™¨ - å¢å¼ºä¼˜åŒ–ç‰ˆ
è´Ÿè´£ä»ACCåŒæ­¥å·¥ä½œæµã€æ¨¡æ¿å’Œè¯„å®¡æ•°æ®åˆ°æœ¬åœ°æ•°æ®åº“

å¢å¼ºä¼˜åŒ–ç‰¹æ€§ï¼š
1. âœ… æ‰¹é‡UPSERTä¼˜åŒ– - ä½¿ç”¨PostgreSQLçš„ON CONFLICTè¯­æ³•
2. âœ… å¢å¼ºæ€§èƒ½ç›‘æ§ - è¯¦ç»†çš„æ€§èƒ½è¿½è¸ªå’Œç“¶é¢ˆåˆ†æ
3. âœ… å¼‚æ­¥å¹¶è¡ŒåŒæ­¥ (asyncio) - æ¯”ThreadPoolExecutoræ›´é«˜æ•ˆ
4. âœ… å†…å­˜ç¼“å­˜å±‚ (cachetools) - å‡å°‘é‡å¤APIè°ƒç”¨
5. âœ… æ™ºèƒ½é‡è¯•æœºåˆ¶ - æŒ‡æ•°é€€é¿å’Œæ–­è·¯å™¨æ¨¡å¼
6. âœ… å†…å­˜ä¼˜åŒ– - æµå¼å¤„ç†å¤§æ•°æ®é›†
7. âœ… å·¥ä½œæµæ¨¡æ¿åŒæ­¥ - æ”¯æŒåŸºç¡€æ¨¡æ¿å’Œè¯¦ç»†APIè°ƒç”¨
8. âœ… æ™ºèƒ½æ¨¡æ¿åˆ†æ - è‡ªåŠ¨è¯†åˆ«æ¨¡æ¿ç±»å‹å’Œç‰¹å¾

æ³¨æ„ï¼šè´¦æˆ·æ•°æ®åŒæ­¥åŠŸèƒ½å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨ç‹¬ç«‹çš„ database_sql/account_sync.py
"""

import sys
import os
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timezone, timedelta
import uuid
import json
import time
import asyncio
import aiohttp
import threading
from functools import wraps, partial
from dataclasses import dataclass, asdict
from collections import defaultdict
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# Override print to always flush output (fixes Windows console buffering)
print = partial(print, flush=True)

# Try different import paths
try:
    from database_sql.review_data_access import ReviewDataAccess
    from database_sql.neon_config import NeonConfig
    import psycopg2
    import psycopg2.extras
except ImportError:
    print("Warning: Could not import from database_sql, trying alternative path")
    try:
        sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../database_sql')))
        from review_data_access import ReviewDataAccess
        from neon_config import NeonConfig
        import psycopg2
        import psycopg2.extras
    except ImportError:
        print("Warning: Could not import dependencies, using placeholders")
        ReviewDataAccess = None
        NeonConfig = None
        psycopg2 = None

# cachetools ç¼“å­˜
try:
    from cachetools import TTLCache
    CACHETOOLS_AVAILABLE = True
except ImportError:
    CACHETOOLS_AVAILABLE = False
    print("Warning: cachetools not available, caching disabled. Install with: pip install cachetools")


# ============================================================================
# æ€§èƒ½ç›‘æ§æ•°æ®ç»“æ„
# ============================================================================

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    api_calls: int = 0
    api_time: float = 0.0
    api_errors: int = 0
    db_queries: int = 0
    db_time: float = 0.0
    db_errors: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    cache_time: float = 0.0
    total_time: float = 0.0
    memory_usage_mb: float = 0.0
    
    # è¯¦ç»†è®¡æ—¶
    timing_breakdown: Dict[str, float] = None
    
    def __post_init__(self):
        if self.timing_breakdown is None:
            self.timing_breakdown = defaultdict(float)
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        result['timing_breakdown'] = dict(result['timing_breakdown'])
        return result
    
    def get_cache_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.cache_hits + self.cache_misses
        return (self.cache_hits / total * 100) if total > 0 else 0.0
    
    def get_api_success_rate(self) -> float:
        """è®¡ç®—APIæˆåŠŸç‡"""
        total = self.api_calls + self.api_errors
        return (self.api_calls / total * 100) if total > 0 else 0.0


# ============================================================================
# Cachetools ç¼“å­˜ç®¡ç†å™¨ï¼ˆæ›¿ä»£ Redisï¼‰
# ============================================================================

class CacheToolsManager:
    """åŸºäº cachetools çš„ç¼“å­˜ç®¡ç†å™¨ï¼ˆRedis æ›¿ä»£æ–¹æ¡ˆï¼‰"""
    
    def __init__(self, ttl: int = 3600, max_size: int = 1000, enabled: bool = True):
        """
        åˆå§‹åŒ–ç¼“å­˜
        
        Args:
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        self.enabled = enabled and CACHETOOLS_AVAILABLE
        self.ttl = ttl
        self.max_size = max_size
        
        if self.enabled:
            try:
                # TTLCache: è‡ªåŠ¨è¿‡æœŸçš„ç¼“å­˜ï¼Œçº¿ç¨‹å®‰å…¨
                self.cache = TTLCache(maxsize=max_size, ttl=ttl)
                self.lock = threading.RLock()  # å¯é‡å…¥é”ï¼Œæ”¯æŒåµŒå¥—è°ƒç”¨
                print(f"[Cache] Memory cache enabled (TTL: {ttl}s, Max: {max_size} entries)")
            except Exception as e:
                print(f"[Cache] Initialization failed, cache disabled: {e}")
                self.enabled = False
                self.cache = None
                self.lock = None
        else:
            self.cache = None
            self.lock = None
            if not CACHETOOLS_AVAILABLE:
                print("[Cache] cachetools not installed. Install with: pip install cachetools")
    
    def _make_key(self, prefix: str, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_parts = [prefix] + [str(arg) for arg in args]
        return ':'.join(key_parts)
    
    def get(self, prefix: str, *args) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        if not self.enabled:
            return None
        
        try:
            key = self._make_key(prefix, *args)
            with self.lock:
                return self.cache.get(key)
        except Exception as e:
            print(f"[Cache] GET failed: {e}")
            return None
    
    def set(self, prefix: str, *args, value: Any) -> bool:
        """è®¾ç½®ç¼“å­˜"""
        if not self.enabled:
            return False
        
        try:
            key = self._make_key(prefix, *args)
            with self.lock:
                self.cache[key] = value
            return True
        except Exception as e:
            print(f"[Cache] SET failed: {e}")
            return False
    
    def delete(self, prefix: str, *args) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        if not self.enabled:
            return False
        
        try:
            key = self._make_key(prefix, *args)
            with self.lock:
                if key in self.cache:
                    del self.cache[key]
                    return True
            return False
        except Exception as e:
            print(f"[Cache] DELETE failed: {e}")
            return False
    
    def clear_pattern(self, pattern: str) -> int:
        """æ¸…é™¤åŒ¹é…æ¨¡å¼çš„ç¼“å­˜"""
        if not self.enabled:
            return 0
        
        try:
            with self.lock:
                keys_to_delete = [k for k in list(self.cache.keys()) if pattern in k]
                for key in keys_to_delete:
                    del self.cache[key]
                return len(keys_to_delete)
        except Exception as e:
            print(f"[Cache] CLEAR failed: {e}")
            return 0
    
    def clear_all(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        if not self.enabled:
            return False
        
        try:
            with self.lock:
                self.cache.clear()
            return True
        except Exception as e:
            print(f"[Cache] CLEAR ALL failed: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enabled:
            return {'enabled': False}
        
        try:
            with self.lock:
                current_size = len(self.cache)
                return {
                    'enabled': True,
                    'current_size': current_size,
                    'max_size': self.max_size,
                    'ttl': self.ttl,
                    'usage_percent': round(current_size / self.max_size * 100, 2) if self.max_size > 0 else 0
                }
        except Exception as e:
            print(f"[Cache] GET STATS failed: {e}")
            return {'enabled': True, 'error': str(e)}


# ============================================================================
# å¢å¼ºçš„åŒæ­¥ç®¡ç†å™¨
# ============================================================================

class EnhancedReviewSyncManager:
    """å¢å¼ºçš„å®¡æ‰¹ç³»ç»ŸåŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(
        self, 
        data_access: Optional[ReviewDataAccess] = None,
        max_concurrent: int = 10,
        enable_cache: bool = True,
        cache_ttl: int = 3600,
        cache_max_size: int = 1000,
        batch_size: int = 100
    ):
        """
        åˆå§‹åŒ–å¢å¼ºåŒæ­¥ç®¡ç†å™¨
        
        Args:
            data_access: æ•°æ®è®¿é—®å±‚å®ä¾‹
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            enable_cache: æ˜¯å¦å¯ç”¨å†…å­˜ç¼“å­˜
            cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            cache_max_size: ç¼“å­˜æœ€å¤§æ¡ç›®æ•°ï¼ˆæ¨èï¼š1000-10000ï¼‰
            batch_size: æ‰¹é‡æ“ä½œå¤§å°
        """
        self.da = data_access or ReviewDataAccess()
        self.max_concurrent = max_concurrent
        self.batch_size = batch_size
        
        # è´¦æˆ·åŒæ­¥åŠŸèƒ½å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨ç‹¬ç«‹çš„ account_sync.py
        
        # åˆå§‹åŒ– cachetools ç¼“å­˜ï¼ˆæ›¿ä»£ Redisï¼‰
        self.cache = CacheToolsManager(
            ttl=cache_ttl,
            max_size=cache_max_size,
            enabled=enable_cache
        )
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = PerformanceMetrics()
        
        # åŒæ­¥ç»Ÿè®¡
        self.sync_stats = {
            'workflows_synced': 0,
            'workflows_updated': 0,
            'workflows_skipped': 0,
            'templates_synced': 0,
            'templates_updated': 0,
            'templates_skipped': 0,
            'reviews_synced': 0,
            'reviews_updated': 0,
            'reviews_skipped': 0,
            'file_versions_synced': 0,
            'file_versions_total': 0,
            'progress_steps_synced': 0,
            'progress_steps_total': 0,
            'errors': []
        }
        
        # æ–­è·¯å™¨çŠ¶æ€
        self.circuit_breaker = {
            'failures': 0,
            'last_failure_time': None,
            'state': 'closed',  # closed, open, half-open
            'threshold': 5,
            'timeout': 60
        }
    
    # ========================================================================
    # æ€§èƒ½ç›‘æ§è£…é¥°å™¨
    # ========================================================================
    
    @staticmethod
    def track_performance(operation: str):
        """æ€§èƒ½è¿½è¸ªè£…é¥°å™¨ï¼ˆé™æ€æ–¹æ³•ç‰ˆæœ¬ï¼‰"""
        def decorator(func):
            @wraps(func)
            async def async_wrapper(self, *args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(self, *args, **kwargs)
                    elapsed = time.time() - start_time
                    self.metrics.timing_breakdown[operation] += elapsed
                    return result
                except Exception as e:
                    elapsed = time.time() - start_time
                    self.metrics.timing_breakdown[f"{operation}_error"] += elapsed
                    raise
            
            @wraps(func)
            def sync_wrapper(self, *args, **kwargs):
                start_time = time.time()
                try:
                    result = func(self, *args, **kwargs)
                    elapsed = time.time() - start_time
                    self.metrics.timing_breakdown[operation] += elapsed
                    return result
                except Exception as e:
                    elapsed = time.time() - start_time
                    self.metrics.timing_breakdown[f"{operation}_error"] += elapsed
                    raise
            
            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        return decorator
    
    # ========================================================================
    # æ–­è·¯å™¨æ¨¡å¼
    # ========================================================================
    
    def check_circuit_breaker(self) -> bool:
        """æ£€æŸ¥æ–­è·¯å™¨çŠ¶æ€"""
        cb = self.circuit_breaker
        
        if cb['state'] == 'open':
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å°è¯•æ¢å¤
            if cb['last_failure_time']:
                elapsed = time.time() - cb['last_failure_time']
                if elapsed > cb['timeout']:
                    cb['state'] = 'half-open'
                    print(f"[CIRCUIT] Circuit breaker entering half-open state, attempting recovery...")
                    return True
                else:
                    return False
            return False
        
        return True
    
    def record_success(self):
        """è®°å½•æˆåŠŸ"""
        cb = self.circuit_breaker
        if cb['state'] == 'half-open':
            cb['state'] = 'closed'
            cb['failures'] = 0
            # Circuit breaker closed, service restored
    
    def record_failure(self):
        """è®°å½•å¤±è´¥"""
        cb = self.circuit_breaker
        cb['failures'] += 1
        cb['last_failure_time'] = time.time()
        
        if cb['failures'] >= cb['threshold']:
            cb['state'] = 'open'
            # Circuit breaker opened, API calls paused
    
    # ========================================================================
    # å¼‚æ­¥APIè°ƒç”¨ï¼ˆä½¿ç”¨aiohttpï¼‰
    # ========================================================================
    
    @track_performance('api_call')
    async def _async_api_call(
        self, 
        session: aiohttp.ClientSession,
        method: str,
        url: str,
        headers: Optional[Dict] = None,
        **kwargs
    ) -> Optional[Dict]:
        """
        å¼‚æ­¥APIè°ƒç”¨
        
        Args:
            session: aiohttpä¼šè¯
            method: HTTPæ–¹æ³•
            url: URL
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            APIå“åº”æ•°æ®
        """
        # æ£€æŸ¥æ–­è·¯å™¨
        if not self.check_circuit_breaker():
            raise Exception("æ–­è·¯å™¨å·²æ‰“å¼€ï¼ŒAPIè°ƒç”¨è¢«é˜»æ­¢")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{method}:{url}"
        cached = self.cache.get('api', cache_key)
        if cached:
            self.metrics.cache_hits += 1
            return cached
        
        self.metrics.cache_misses += 1
        
        # æ‰§è¡ŒAPIè°ƒç”¨
        start_time = time.time()
        try:
            # å¦‚æœæä¾›äº† headersï¼Œä½¿ç”¨æä¾›çš„ï¼Œå¦åˆ™éœ€è¦ä»å¤–éƒ¨è·å–
            request_headers = headers or kwargs.pop('headers', {})
            
            # ç¡®ä¿ URL æ˜¯å®Œæ•´çš„
            if not url.startswith('http'):
                import config
                url = f"{config.AUTODESK_API_BASE}/construction/reviews/v1{url}"
            
            async with session.request(method, url, headers=request_headers, **kwargs) as response:
                response.raise_for_status()
                data = await response.json()
                
                elapsed = time.time() - start_time
                self.metrics.api_calls += 1
                self.metrics.api_time += elapsed
                
                # ç¼“å­˜ç»“æœ
                self.cache.set('api', cache_key, value=data)
                
                # è®°å½•æˆåŠŸ
                self.record_success()
                
                return data
                
        except Exception as e:
            elapsed = time.time() - start_time
            self.metrics.api_errors += 1
            self.metrics.api_time += elapsed
            
            # è®°å½•å¤±è´¥
            self.record_failure()
            
            print(f"[WARNING] API call failed ({url}): {e}")
            raise
    
    # ========================================================================
    # æ‰¹é‡UPSERTä¼˜åŒ–
    # ========================================================================
    
    @track_performance('batch_upsert_workflows')
    def batch_upsert_workflows(self, workflows: List[Dict[str, Any]]) -> Tuple[int, int]:
        """
        æ‰¹é‡UPSERTå·¥ä½œæµï¼ˆä½¿ç”¨PostgreSQLçš„ON CONFLICTï¼‰
        
        Args:
            workflows: å·¥ä½œæµæ•°æ®åˆ—è¡¨
            
        Returns:
            (æ’å…¥æ•°é‡, æ›´æ–°æ•°é‡)
        """
        if not workflows:
            return 0, 0
        
        start_time = time.time()
        
        try:
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            batch_data = []
            for wf in workflows:
                wf_data = self._transform_acc_workflow_data(wf)
                batch_data.append(wf_data)
            
            # æ‰§è¡Œæ‰¹é‡UPSERT
            sql = """
                INSERT INTO workflows (
                    workflow_uuid, project_id, data_source, acc_workflow_id,
                    name, description, notes, status, additional_options,
                    approval_status_options, copy_files_options, attached_attributes,
                    update_attributes_options, steps, created_by,
                    created_at, updated_at, last_synced_at, sync_status
                )
                VALUES (
                    %(workflow_uuid)s, %(project_id)s, %(data_source)s, %(acc_workflow_id)s,
                    %(name)s, %(description)s, %(notes)s, %(status)s, %(additional_options)s,
                    %(approval_status_options)s, %(copy_files_options)s, %(attached_attributes)s,
                    %(update_attributes_options)s, %(steps)s, %(created_by)s,
                    %(created_at)s, %(updated_at)s, %(last_synced_at)s, %(sync_status)s
                )
                ON CONFLICT (acc_workflow_id) 
                DO UPDATE SET
                    name = EXCLUDED.name,
                    description = EXCLUDED.description,
                    notes = EXCLUDED.notes,
                    status = EXCLUDED.status,
                    additional_options = EXCLUDED.additional_options,
                    approval_status_options = EXCLUDED.approval_status_options,
                    copy_files_options = EXCLUDED.copy_files_options,
                    attached_attributes = EXCLUDED.attached_attributes,
                    update_attributes_options = EXCLUDED.update_attributes_options,
                    steps = EXCLUDED.steps,
                    updated_at = EXCLUDED.updated_at,
                    last_synced_at = EXCLUDED.last_synced_at,
                    sync_status = EXCLUDED.sync_status
                RETURNING id, (xmax = 0) AS inserted
            """
            
            # æ‰§è¡Œæ‰¹é‡æ“ä½œ
            with self.da.get_connection() as conn:
                with conn.cursor() as cur:
                    results = []
                    for data in batch_data:
                        cur.execute(sql, data)
                        results.append(cur.fetchone())
                    conn.commit()
            
            # ç»Ÿè®¡ç»“æœ
            inserted = sum(1 for _, is_insert in results if is_insert)
            updated = len(results) - inserted
            
            elapsed = time.time() - start_time
            self.metrics.db_queries += 1
            self.metrics.db_time += elapsed
            
            print(f"  SUCCESS: Batch UPSERT workflows: {inserted} new, {updated} updated (duration: {elapsed:.2f}s)")
            
            return inserted, updated
            
        except Exception as e:
            elapsed = time.time() - start_time
            self.metrics.db_errors += 1
            self.metrics.db_time += elapsed
            error_msg = f"æ‰¹é‡UPSERTå·¥ä½œæµå¤±è´¥: {str(e)}"
            self.sync_stats['errors'].append(error_msg)
            print(f"âœ— {error_msg}")
            raise
    
    @track_performance('batch_upsert_reviews')
    def batch_upsert_reviews(self, reviews: List[Dict[str, Any]]) -> Tuple[int, int]:
        """
        æ‰¹é‡UPSERTè¯„å®¡
        
        Args:
            reviews: è¯„å®¡æ•°æ®åˆ—è¡¨
            
        Returns:
            (æ’å…¥æ•°é‡, æ›´æ–°æ•°é‡)
        """
        if not reviews:
            return 0, 0
        
        start_time = time.time()
        
        try:
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            batch_data = []
            for review in reviews:
                review_data = self._transform_acc_review_data(review, project_id=getattr(self, '_current_project_id', None))
                batch_data.append(review_data)
            
            # æ‰§è¡Œæ‰¹é‡UPSERT
            sql = """
                INSERT INTO reviews (
                    review_uuid, project_id, data_source, acc_review_id, acc_sequence_id,
                    name, description, notes, status, current_step_id, current_step_due_date,
                    current_step_name, workflow_uuid, created_by, assigned_to, next_action_by,
                    archived, archived_by, archived_at, archived_reason,
                    created_at, updated_at, started_at, finished_at,
                    last_synced_at, sync_status
                )
                VALUES (
                    %(review_uuid)s, %(project_id)s, %(data_source)s, %(acc_review_id)s, %(acc_sequence_id)s,
                    %(name)s, %(description)s, %(notes)s, %(status)s, %(current_step_id)s, %(current_step_due_date)s,
                    %(current_step_name)s, %(workflow_uuid)s, %(created_by)s, %(assigned_to)s, %(next_action_by)s,
                    %(archived)s, %(archived_by)s, %(archived_at)s, %(archived_reason)s,
                    %(created_at)s, %(updated_at)s, %(started_at)s, %(finished_at)s,
                    %(last_synced_at)s, %(sync_status)s
                )
                ON CONFLICT (review_uuid)
                DO UPDATE SET
                    name = EXCLUDED.name,
                    description = EXCLUDED.description,
                    notes = EXCLUDED.notes,
                    status = EXCLUDED.status,
                    current_step_id = EXCLUDED.current_step_id,
                    current_step_due_date = EXCLUDED.current_step_due_date,
                    current_step_name = EXCLUDED.current_step_name,
                    assigned_to = EXCLUDED.assigned_to,
                    next_action_by = EXCLUDED.next_action_by,
                    archived = EXCLUDED.archived,
                    archived_by = EXCLUDED.archived_by,
                    archived_at = EXCLUDED.archived_at,
                    archived_reason = EXCLUDED.archived_reason,
                    updated_at = EXCLUDED.updated_at,
                    finished_at = EXCLUDED.finished_at,
                    last_synced_at = EXCLUDED.last_synced_at,
                    sync_status = EXCLUDED.sync_status
                RETURNING id, (xmax = 0) AS inserted
            """
            
            # æ‰§è¡Œæ‰¹é‡æ“ä½œ
            with self.da.get_connection() as conn:
                with conn.cursor() as cur:
                    results = []
                    for data in batch_data:
                        cur.execute(sql, data)
                        results.append(cur.fetchone())
                    conn.commit()
            
            # ç»Ÿè®¡ç»“æœ
            inserted = sum(1 for _, is_insert in results if is_insert)
            updated = len(results) - inserted
            
            elapsed = time.time() - start_time
            self.metrics.db_queries += 1
            self.metrics.db_time += elapsed
            
            print(f"  SUCCESS: Batch UPSERT reviews: {inserted} new, {updated} updated (duration: {elapsed:.2f}s)")
            
            return inserted, updated
            
        except Exception as e:
            elapsed = time.time() - start_time
            self.metrics.db_errors += 1
            self.metrics.db_time += elapsed
            error_msg = f"Batch UPSERT reviews failed: {str(e)}"
            self.sync_stats['errors'].append(error_msg)
            print(f"âœ— {error_msg}")
            raise
    
    # ========================================================================
    # å¼‚æ­¥å¹¶è¡ŒåŒæ­¥ï¼ˆä½¿ç”¨asyncioï¼‰
    # ========================================================================
    
    async def fetch_all_workflows(
        self,
        session: aiohttp.ClientSession,
        project_id: str,
        show_progress: bool = True
    ) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·¥ä½œæµï¼ˆç”¨äºç¼“å­˜ï¼‰
        
        Args:
            session: aiohttpä¼šè¯
            project_id: é¡¹ç›®ID
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            æ‰€æœ‰å·¥ä½œæµåˆ—è¡¨
        """
        if show_progress:
            print(f"\n[FETCH] Getting all workflows for caching...")
        
        try:
            # è·å–å·¥ä½œæµåˆ—è¡¨
            headers = getattr(self, '_temp_headers', {})
            result = await self._async_api_call(
                session,
                'GET',
                f'/projects/{project_id}/workflows',
                headers=headers,
                params={'limit': 50, 'offset': 0}
            )
            
            workflows = result.get('results', [])
            total_results = result.get('pagination', {}).get('totalResults', len(workflows))
            
            if show_progress:
                print(f"   SUCCESS: Retrieved {len(workflows)} workflows")
            
            # å¦‚æœæœ‰æ›´å¤šé¡µé¢ï¼Œç»§ç»­è·å–
            if total_results > len(workflows):
                remaining_pages = (total_results - len(workflows) + 49) // 50
                if show_progress:
                    print(f"   éœ€è¦é¢å¤–è·å– {remaining_pages} é¡µ...")
                
                tasks = []
                for page in range(1, remaining_pages + 1):
                    offset = page * 50
                    task = self._async_api_call(
                        session,
                        'GET',
                        f'/projects/{project_id}/workflows',
                        headers=headers,
                        params={'limit': 50, 'offset': offset}
                    )
                    tasks.append(task)
                
                page_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in page_results:
                    if not isinstance(result, Exception):
                        workflows.extend(result.get('results', []))
            
            if show_progress:
                print(f"   [OK] Retrieved {len(workflows)} workflows total")
            
            return workflows
            
        except Exception as e:
            error_msg = f"è·å–å·¥ä½œæµå¤±è´¥: {str(e)}"
            self.sync_stats['errors'].append(error_msg)
            print(f"   [ERROR] {error_msg}")
            return []
    
    async def async_sync_reviews_parallel(
        self,
        api_client,
        project_id: str,
        reviews: List[Dict[str, Any]],
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥å¹¶è¡ŒåŒæ­¥è¯„å®¡ï¼ˆä½¿ç”¨asyncioï¼‰
        
        Args:
            api_client: APIå®¢æˆ·ç«¯
            project_id: é¡¹ç›®ID
            reviews: è¯„å®¡åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
        """
        total = len(reviews)
        
        if show_progress:
            print(f"\n[ASYNC] Starting parallel async sync for {total} reviews...")
            print(f"   Max concurrent: {self.max_concurrent}")
            print("=" * 60)
        
        start_time = time.time()
        
        # è®¾ç½®å½“å‰é¡¹ç›®IDä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
        self._current_project_id = project_id
        
        # åˆ›å»ºå¼‚æ­¥HTTPä¼šè¯
        async with aiohttp.ClientSession() as session:
            # Step 1: Pre-fetch all workflows and build cache
            if show_progress:
                print(f"\n[OPTIMIZE] Pre-fetching workflow cache...")
            
            all_workflows = await self.fetch_all_workflows(session, project_id, show_progress)
            workflow_cache = {wf['id']: wf for wf in all_workflows}
            
            if show_progress:
                print(f"   [OK] Workflow cache built: {len(workflow_cache)} workflows")
                
                # åˆ†æ review çš„ workflow åˆ†å¸ƒ
                workflow_usage = {}
                for review in reviews:
                    wf_id = review.get('workflowId')
                    if wf_id:
                        workflow_usage[wf_id] = workflow_usage.get(wf_id, 0) + 1
                
                print(f"   [STATS] Workflow usage analysis:")
                for wf_id, count in workflow_usage.items():
                    wf_name = workflow_cache.get(wf_id, {}).get('name', 'Unknown')
                    print(f"      - {wf_name}: {count} reviews")
                
                # è®¡ç®—é¢„æœŸçš„ç¼“å­˜èŠ‚çœ
                total_workflow_calls = sum(workflow_usage.values())
                unique_workflows = len(workflow_usage)
                saved_calls = total_workflow_calls - unique_workflows
                if saved_calls > 0:
                    print(f"   [OPTIMIZE] Expected to save {saved_calls} workflow API calls (savings: {saved_calls/total_workflow_calls*100:.1f}%)")
            
            # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(self.max_concurrent)
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡ï¼ˆä¼ å…¥ workflow_cacheï¼‰
            tasks = []
            for review in reviews:
                task = self._async_fetch_review_details(
                    session,
                    api_client,
                    project_id,
                    review,
                    semaphore,
                    workflow_cache  # Pass workflow cache
                )
                tasks.append(task)
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            if show_progress:
                print(f"\n[PHASE 1/2] Async API data fetching...")
            
            review_details = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            successful_reviews = []
            for idx, result in enumerate(review_details):
                if isinstance(result, Exception):
                    error_msg = f"è·å–è¯„å®¡è¯¦æƒ…å¤±è´¥: {str(result)}"
                    self.sync_stats['errors'].append(error_msg)
                    if show_progress:
                        print(f"   âœ— {error_msg}")
                else:
                    successful_reviews.append(result)
        
        api_time = time.time() - start_time
        self.metrics.api_time += api_time
        
        if show_progress:
            print(f"\nSUCCESS: API data fetching completed")
            print(f"   Total API calls: {self.metrics.api_calls} times")
            print(f"   Duration: {api_time:.2f}s")
            print(f"   Success: {len(successful_reviews)}/{total}")
        
        # Phase 2: Batch database writing
        if show_progress:
            print(f"\n[PHASE 2/2] Batch database writing...")
        
        db_start = time.time()
        
        # Batch UPSERT reviews
        inserted, updated = self.batch_upsert_reviews(successful_reviews)
        self.sync_stats['reviews_synced'] += inserted
        self.sync_stats['reviews_updated'] += updated
        
        # ğŸ”§ ä¿®å¤ï¼šåŒæ­¥ file versions å’Œ progress steps
        total_file_versions = 0
        total_progress_steps = 0
        
        for review in successful_reviews:
            review_id = review.get('id')
            if not review_id:
                continue
                
            # è·å–æ•°æ®åº“ä¸­çš„ review ID
            db_review = self.da.get_review_by_acc_id(review_id)
            if not db_review:
                continue
                
            local_review_id = db_review['id']
            
            # åŒæ­¥æ–‡ä»¶ç‰ˆæœ¬
            file_versions = review.get('fileVersions', [])
            if file_versions:
                self._sync_review_file_versions(local_review_id, file_versions)
                total_file_versions += len(file_versions)
                self.sync_stats['file_versions_synced'] += len(file_versions)
                
            # åŒæ­¥è¿›åº¦æ­¥éª¤
            progress_steps = review.get('steps', [])
            if progress_steps:
                self._sync_review_progress(local_review_id, progress_steps)
                total_progress_steps += len(progress_steps)
                self.sync_stats['progress_steps_synced'] += len(progress_steps)
        
        self.sync_stats['file_versions_total'] = total_file_versions
        self.sync_stats['progress_steps_total'] = total_progress_steps
        
        if show_progress and (total_file_versions > 0 or total_progress_steps > 0):
            print(f"  SUCCESS: åŒæ­¥æ–‡ä»¶ç‰ˆæœ¬: {total_file_versions} ä¸ª")
            print(f"  SUCCESS: åŒæ­¥è¿›åº¦æ­¥éª¤: {total_progress_steps} ä¸ª")
        
        db_time = time.time() - db_start
        total_time = time.time() - start_time
        
        self.metrics.total_time = total_time
        
        if show_progress:
            print("\n" + "=" * 60)
            print(f"[STATS] Performance statistics:")
            print(f"   APIè°ƒç”¨é˜¶æ®µ: {api_time:.2f}ç§’ ({api_time/total_time*100:.1f}%)")
            print(f"   æ•°æ®åº“å†™å…¥: {db_time:.2f}ç§’ ({db_time/total_time*100:.1f}%)")
            print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"   å¹³å‡æ¯ä¸ªè¯„å®¡: {total_time/total:.2f}ç§’")
        
        return self.sync_stats
    
    async def _async_fetch_review_details(
        self,
        session: aiohttp.ClientSession,
        api_client,
        project_id: str,
        review: Dict[str, Any],
        semaphore: asyncio.Semaphore,
        workflow_cache: Optional[Dict[str, Dict]] = None
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥è·å–è¯„å®¡è¯¦æƒ…
        
        Args:
            session: aiohttpä¼šè¯
            api_client: APIå®¢æˆ·ç«¯
            project_id: é¡¹ç›®ID
            review: è¯„å®¡åŸºç¡€ä¿¡æ¯
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            workflow_cache: å·¥ä½œæµç¼“å­˜ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å®Œæ•´çš„è¯„å®¡æ•°æ®
        """
        async with semaphore:
            review_id = review.get('id')
            
            # Optimization: Use workflow cache instead of API calls
            workflow_id = review.get('workflowId')
            workflow = None
            
            if workflow_cache and workflow_id and workflow_id in workflow_cache:
                # Cache hit: Get workflow from cache
                workflow = workflow_cache[workflow_id]
                self.metrics.cache_hits += 1
                
                # åªéœ€è¦è·å–2ä¸ªAPIç«¯ç‚¹ï¼ˆversions å’Œ progressï¼‰
                headers = getattr(self, '_temp_headers', {})
                tasks = [
                    self._async_api_call(
                        session,
                        'GET',
                        f'/projects/{project_id}/reviews/{review_id}/versions',
                        headers=headers
                    ),
                    self._async_api_call(
                        session,
                        'GET',
                        f'/projects/{project_id}/reviews/{review_id}/progress',
                        headers=headers
                    )
                ]
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # åˆå¹¶ç»“æœ
                versions = results[0] if not isinstance(results[0], Exception) else []
                progress = results[1] if not isinstance(results[1], Exception) else []
                
            else:
                # ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦è°ƒç”¨ workflow API
                if workflow_cache is not None:
                    self.metrics.cache_misses += 1
                
                # å¹¶è¡Œè·å–3ä¸ªAPIç«¯ç‚¹ï¼ˆåŒ…æ‹¬ workflowï¼‰
                headers = getattr(self, '_temp_headers', {})
                tasks = [
                    self._async_api_call(
                        session,
                        'GET',
                        f'/projects/{project_id}/reviews/{review_id}/versions',
                        headers=headers
                    ),
                    self._async_api_call(
                        session,
                        'GET',
                        f'/projects/{project_id}/reviews/{review_id}/progress',
                        headers=headers
                    ),
                    self._async_api_call(
                        session,
                        'GET',
                        f'/projects/{project_id}/reviews/{review_id}/workflow',
                        headers=headers
                    )
                ]
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # åˆå¹¶ç»“æœ
                versions = results[0] if not isinstance(results[0], Exception) else []
                progress = results[1] if not isinstance(results[1], Exception) else []
                workflow = results[2] if not isinstance(results[2], Exception) else {}
            
            # æ·»åŠ åˆ°è¯„å®¡æ•°æ®
            review['fileVersions'] = versions.get('results', []) if isinstance(versions, dict) else []
            review['steps'] = progress.get('results', []) if isinstance(progress, dict) else []
            review['workflow'] = workflow or {}
            
            return review
    
    # ========================================================================
    # æ•°æ®è½¬æ¢æ–¹æ³•ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
    # ========================================================================
    
    def _transform_acc_workflow_data(self, acc_data: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢ACCå·¥ä½œæµæ•°æ®ä¸ºæœ¬åœ°æ ¼å¼"""
        return {
            'workflow_uuid': acc_data.get('id'),  # ä½¿ç”¨ ACC çš„ workflow ID ä½œä¸º UUID
            'project_id': acc_data.get('projectId'),
            'data_source': 'acc_sync',
            'acc_workflow_id': acc_data.get('id'),
            'name': acc_data.get('name', 'Unnamed Workflow'),
            'description': acc_data.get('description'),
            'notes': acc_data.get('notes'),
            'status': self._map_workflow_status(acc_data.get('status', 'active')),
            'additional_options': json.dumps(acc_data.get('additionalOptions', {})),
            'approval_status_options': json.dumps(acc_data.get('approvalStatusOptions', [])),
            'copy_files_options': json.dumps(acc_data.get('copyFilesOptions', {})),
            'attached_attributes': json.dumps(acc_data.get('attachedAttributes', [])),
            'update_attributes_options': json.dumps(acc_data.get('updateAttributesOptions', {})),
            'steps': json.dumps(acc_data.get('steps', [])),
            'created_by': json.dumps(acc_data.get('createdBy', {})),
            'created_at': self._parse_timestamp(acc_data.get('createdAt')),
            'updated_at': self._parse_timestamp(acc_data.get('updatedAt')),
            'last_synced_at': datetime.now(timezone.utc),
            'sync_status': 'synced'
        }
    
    def _transform_acc_review_data(self, acc_data: Dict[str, Any], project_id: Optional[str] = None) -> Dict[str, Any]:
        """
        è½¬æ¢ACCè¯„å®¡æ•°æ®ä¸ºæœ¬åœ°æ ¼å¼
        
        æ³¨æ„ï¼šworkflow_id ä¼šç”±æ•°æ®åº“è§¦å‘å™¨è‡ªåŠ¨è®¾ç½®ï¼Œæ— éœ€æ‰‹åŠ¨æŸ¥æ‰¾
        """
        workflow_uuid = acc_data.get('workflowId')
        
        return {
            'review_uuid': acc_data.get('id'),  # ä½¿ç”¨ ACC çš„ review ID ä½œä¸º UUID
            'project_id': project_id or acc_data.get('projectId'),
            'data_source': 'acc_sync',
            'acc_review_id': acc_data.get('id'),
            'acc_sequence_id': acc_data.get('sequenceId'),
            'name': acc_data.get('name', 'Unnamed Review'),
            'description': acc_data.get('description'),
            'notes': acc_data.get('notes'),
            'status': self._map_review_status(acc_data.get('status', 'open')),
            'current_step_id': acc_data.get('currentStepId'),
            'current_step_due_date': self._parse_timestamp(acc_data.get('currentStepDueDate')),
            'current_step_name': acc_data.get('currentStepName'),
            # workflow_id ç”±æ•°æ®åº“è§¦å‘å™¨è‡ªåŠ¨è®¾ç½®ï¼Œåªéœ€æä¾› workflow_uuid
            'workflow_uuid': workflow_uuid,
            'created_by': json.dumps(acc_data.get('createdBy', {})),
            'assigned_to': json.dumps(acc_data.get('assignedTo', [])),
            'next_action_by': json.dumps(acc_data.get('nextActionBy', {})),
            'archived': acc_data.get('archived', False),
            'archived_by': json.dumps(acc_data.get('archivedBy', {})),
            'archived_at': self._parse_timestamp(acc_data.get('archivedAt')),
            'archived_reason': acc_data.get('archivedReason'),
            'created_at': self._parse_timestamp(acc_data.get('createdAt')),
            'updated_at': self._parse_timestamp(acc_data.get('updatedAt')),
            'started_at': self._parse_timestamp(acc_data.get('startedAt')),
            'finished_at': self._parse_timestamp(acc_data.get('finishedAt')),
            'last_synced_at': datetime.now(timezone.utc),
            'sync_status': 'synced'
        }
    
    def _map_workflow_status(self, acc_status: str) -> str:
        """æ˜ å°„ACCå·¥ä½œæµçŠ¶æ€åˆ°æœ¬åœ°çŠ¶æ€"""
        status_map = {
            'active': 'ACTIVE',
            'inactive': 'INACTIVE',
            'draft': 'DRAFT',
            'archived': 'ARCHIVED'
        }
        return status_map.get(acc_status.lower(), 'ACTIVE')
    
    def _map_review_status(self, acc_status: str) -> str:
        """æ˜ å°„ACCè¯„å®¡çŠ¶æ€åˆ°æœ¬åœ°çŠ¶æ€"""
        status_map = {
            'open': 'OPEN',
            'closed': 'CLOSED',
            'void': 'VOID',
            'failed': 'FAILED',
            'draft': 'DRAFT',
            'cancelled': 'CANCELLED'
        }
        return status_map.get(acc_status.lower(), 'OPEN')
    
    def _parse_timestamp(self, timestamp_str: Optional[str]) -> Optional[datetime]:
        """è§£ææ—¶é—´æˆ³å­—ç¬¦ä¸²"""
        if not timestamp_str:
            return None
        
        try:
            return datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        except:
            try:
                return datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%S.%f')
            except:
                return None
    
    # _get_workflow_id_by_uuid æ–¹æ³•å·²åˆ é™¤
    # workflow_id ç°åœ¨ç”±æ•°æ®åº“è§¦å‘å™¨è‡ªåŠ¨è®¾ç½®ï¼Œæ— éœ€æ‰‹åŠ¨æŸ¥æ‰¾
    
    # ========================================================================
    # æ–‡ä»¶ç‰ˆæœ¬å’Œè¿›åº¦åŒæ­¥ï¼ˆæ‰¹é‡ä¼˜åŒ–ï¼‰
    # ========================================================================
    
    @track_performance('sync_review_file_versions')
    def _sync_review_file_versions(self, review_id: int, file_versions: List[Dict]) -> None:
        """
        åŒæ­¥è¯„å®¡çš„æ–‡ä»¶ç‰ˆæœ¬ï¼ˆæ‰¹é‡ä¼˜åŒ–ç‰ˆï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•åªåŒæ­¥å®¡æ‰¹çŠ¶æ€ï¼Œä¸å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
        æ–‡ä»¶ä¿¡æ¯åº”è¯¥å·²ç»åœ¨ file_versions è¡¨ä¸­ï¼ˆç”± postgresql_sync_manager åŒæ­¥ï¼‰
        """
        if not file_versions:
            return
        
        db_start = time.time()
        batch_data = []
        
        for fv_data in file_versions:
            try:
                # å¤„ç†å®¡æ‰¹çŠ¶æ€å¯¹è±¡
                approve_status = fv_data.get('approveStatus', {})
                if isinstance(approve_status, dict):
                    approval_status_id = approve_status.get('id')
                    approval_status_value = approve_status.get('value', 'PENDING')
                    approval_label = approve_status.get('label')
                    approval_status = self._map_approval_status(approval_status_value)
                else:
                    # å…¼å®¹æ—§æ ¼å¼
                    approval_status = self._map_approval_status(fv_data.get('approvalStatus', 'pending'))
                    approval_status_id = None
                    approval_status_value = None
                    approval_label = fv_data.get('approvalLabel')
                
                # ğŸ”‘ å…³é”®æ”¹å˜ï¼šåªå­˜å‚¨ file_version_urn å’Œå®¡æ‰¹çŠ¶æ€
                # æ–‡ä»¶ä¿¡æ¯ä» file_versions è¡¨è·å–
                file_version_urn = fv_data.get('urn') or fv_data.get('versionUrn')
                
                if not file_version_urn:
                    print(f"  [WARNING] Missing file_version_urn, skipping file")
                    continue
                
                file_data = {
                    'review_id': review_id,
                    'file_version_urn': file_version_urn,  # ğŸ”‘ å¼•ç”¨ file_versions.urn
                    
                    # åªå­˜å‚¨å®¡æ‰¹ç›¸å…³ä¿¡æ¯
                    'approval_status': approval_status,
                    'approval_status_id': approval_status_id,
                    'approval_status_value': approval_status_value,
                    'approval_label': approval_label,
                    'approval_comments': fv_data.get('approvalComments'),
                    'review_content': fv_data.get('reviewContent', {}),
                    'custom_attributes': fv_data.get('customAttributes', []),
                    'copied_file_version_urn': fv_data.get('copiedFileVersionUrn')
                }
                
                batch_data.append(file_data)
            
            except Exception as e:
                error_msg = f"å‡†å¤‡æ–‡ä»¶ç‰ˆæœ¬æ•°æ®å¤±è´¥: {str(e)}"
                self.sync_stats['errors'].append(error_msg)
                print(f"âœ— {error_msg}")
        
        # æ‰¹é‡æ’å…¥
        if batch_data:
            try:
                inserted_count = self.da.batch_insert_review_files(batch_data)
                db_time = time.time() - db_start
                self.metrics.db_time += db_time
                self.metrics.db_queries += 1
                print(f"  SUCCESS: Batch insert {inserted_count} file versions (duration: {db_time:.2f}s)")
            except Exception as e:
                error_msg = f"æ‰¹é‡æ’å…¥æ–‡ä»¶ç‰ˆæœ¬å¤±è´¥: {str(e)}"
                self.sync_stats['errors'].append(error_msg)
                self.metrics.db_errors += 1
                print(f"âœ— {error_msg}")
    
    @track_performance('sync_review_progress')
    def _sync_review_progress(self, review_id: int, steps: List[Dict]) -> None:
        """
        åŒæ­¥è¯„å®¡è¿›åº¦ï¼ˆæ‰¹é‡ä¼˜åŒ–ç‰ˆï¼‰
        
        é‡è¦æ¦‚å¿µè¯´æ˜ï¼š
        1. review_step_candidates: ä» workflow æ¨¡æ¿åˆ›å»ºï¼Œå­˜å‚¨å€™é€‰å®¡æ‰¹äººé…ç½®ï¼ˆé™æ€é…ç½®ï¼‰
           - åªåŒ…å« workflow å®šä¹‰çš„æ­¥éª¤ï¼ˆä¸åŒ…å«"è¿”å›"äº§ç”Ÿçš„é‡å¤æ­¥éª¤ï¼‰
           - ç”¨äºæƒé™éªŒè¯ï¼š"è°å¯ä»¥å®¡æ‰¹è¿™ä¸ªæ­¥éª¤"
        
        2. review_progress: è®°å½•å®é™…æ‰§è¡Œå†å²å’ŒçŠ¶æ€ï¼ˆåŠ¨æ€è®°å½•ï¼‰
           - åŒ…å«æ‰€æœ‰å®é™…æ‰§è¡Œçš„æ­¥éª¤ï¼ˆåŒ…æ‹¬"è¿”å›"äº§ç”Ÿçš„é‡å¤æ­¥éª¤ï¼‰
           - è®°å½•å†å²ï¼š"è°åœ¨ä»€ä¹ˆæ—¶å€™å®¡æ‰¹äº†ï¼Œç»“æœå¦‚ä½•"
           - æ•°é‡å¯èƒ½å¤§äº workflow æ­¥éª¤æ•°ï¼ˆå› ä¸º"è¿”å›"åŠŸèƒ½ï¼‰
        
        æ–¹æ¡ˆ2å¯¦ç¾ï¼š
        - æ­¥é©Ÿ1: å¾ workflow template å‰µå»ºå®Œæ•´çš„ review_step_candidatesï¼ˆæ‰€æœ‰æ­¥é©Ÿï¼‰
        - æ­¥é©Ÿ2: åŒæ­¥ review_progressï¼ˆåªæœ‰å·²åŸ·è¡Œçš„æ­¥é©Ÿï¼Œè‡ªå‹•é—œè¯ template_step_idï¼‰
        """
        if not steps:
            return

        # æ­¥é©Ÿ 1: ç²å– workflow template é…ç½®
        workflow_steps_config = self._get_workflow_steps_for_review(review_id)
        if not workflow_steps_config:
            print(f"  [WARNING] No workflow template found for review {review_id}, skipping sync")
            return

        # æ­¥é©Ÿ 2: å¾ workflow template å‰µå»ºå®Œæ•´çš„ step_candidatesï¼ˆæ‰€æœ‰æ­¥é©Ÿï¼ŒåŒ…æ‹¬æœªåŸ·è¡Œçš„ï¼‰
        # âœ… ä½¿ç”¨ template step_idï¼Œç¢ºä¿é…ç½®å®Œæ•´æ€§
        try:
            self._create_candidates_from_template(review_id, workflow_steps_config)
        except Exception as e:
            error_msg = f"å¾æ¨¡æ¿å‰µå»ºå€™é¸äººé…ç½®å¤±æ•—: {str(e)}"
            self.sync_stats['errors'].append(error_msg)
            print(f"âœ— {error_msg}")

        # æ­¥é©Ÿ 3: åŒæ­¥ review_progressï¼ˆåªæœ‰å·²åŸ·è¡Œçš„æ­¥é©Ÿï¼‰
        # âœ… è‡ªå‹•é—œè¯åˆ° template_step_idï¼ˆé€šé step_order åŒ¹é…ï¼‰
        try:
            self._sync_progress_with_template_mapping(review_id, steps, workflow_steps_config)
        except Exception as e:
            error_msg = f"åŒæ­¥é€²åº¦ä¸¦é—œè¯æ¨¡æ¿å¤±æ•—: {str(e)}"
            self.sync_stats['errors'].append(error_msg)
            print(f"âœ— {error_msg}")
    
    def _map_approval_status(self, acc_status: str) -> str:
        """æ˜ å°„å®¡æ‰¹çŠ¶æ€"""
        status_map = {
            'pending': 'PENDING',
            'approved': 'APPROVED',
            'rejected': 'REJECTED',
            'in_review': 'IN_REVIEW'
        }
        return status_map.get(acc_status.lower(), 'PENDING')
    
    def _map_step_type(self, acc_type: str) -> str:
        """æ˜ å°„æ­¥éª¤ç±»å‹"""
        type_map = {
            'reviewer': 'REVIEWER',
            'approver': 'APPROVER',
            'initiator': 'INITIATOR',
            'final': 'FINAL'
        }
        return type_map.get(acc_type.lower(), 'REVIEWER')
    
    def _map_step_status(self, acc_status: str) -> str:
        """æ˜ å°„æ­¥éª¤çŠ¶æ€"""
        status_map = {
            'pending': 'PENDING',
            'claimed': 'CLAIMED',
            'in_progress': 'OPEN',
            'submitted': 'SUBMITTED',
            'approved': 'APPROVED',
            'rejected': 'REJECTED',
            'skipped': 'SKIPPED'
        }
        return status_map.get(acc_status.lower(), 'PENDING')
    
    def _get_workflow_steps_for_review(self, review_id: int) -> List[Dict]:
        """è·å–è¯„å®¡å…³è”çš„å·¥ä½œæµæ­¥éª¤é…ç½®"""
        try:
            with self.da.get_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    # å°è¯•é€šè¿‡ workflow_id å…³è”
                    cur.execute("""
                        SELECT w.steps, w.workflow_uuid, w.name as workflow_name
                        FROM reviews r
                        JOIN workflows w ON r.workflow_id = w.id
                        WHERE r.id = %s AND r.workflow_id IS NOT NULL
                    """, (review_id,))
                    
                    result = cur.fetchone()
                    if result and result['steps']:
                        steps = json.loads(result['steps']) if isinstance(result['steps'], str) else result['steps']
                        if isinstance(steps, list) and len(steps) > 0:
                            print(f"  [WORKFLOW] Found workflow via workflow_id: {result['workflow_name']} ({len(steps)} steps)")
                            return steps
                    
                    # å¦‚æœé€šè¿‡ workflow_id æ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡ workflow_uuid å…³è”
                    cur.execute("""
                        SELECT w.steps, w.workflow_uuid, w.name as workflow_name, r.workflow_uuid as review_workflow_uuid
                        FROM reviews r
                        JOIN workflows w ON r.workflow_uuid = w.workflow_uuid
                        WHERE r.id = %s AND r.workflow_uuid IS NOT NULL
                    """, (review_id,))
                    
                    result = cur.fetchone()
                    if result and result['steps']:
                        steps = json.loads(result['steps']) if isinstance(result['steps'], str) else result['steps']
                        if isinstance(steps, list) and len(steps) > 0:
                            print(f"  [WORKFLOW] Found workflow via workflow_uuid: {result['workflow_name']} ({len(steps)} steps)")
                            return steps
                    
                    # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
                    cur.execute("""
                        SELECT workflow_id, workflow_uuid, name
                        FROM reviews 
                        WHERE id = %s
                    """, (review_id,))
                    
                    review_info = cur.fetchone()
                    if review_info:
                        print(f"  [WARNING] No workflow found for review {review_id}")
                        print(f"            Review workflow_id: {review_info['workflow_id']}")
                        print(f"            Review workflow_uuid: {review_info['workflow_uuid']}")
                    
                    return []
        except Exception as e:
            print(f"[WARNING] Failed to get workflow steps config: {e}")
            return []
    
    def _get_candidates_from_workflow_config(self, workflow_steps: List[Dict], step_id: str) -> Dict:
        """ä»å·¥ä½œæµé…ç½®ä¸­è·å–æŒ‡å®šæ­¥éª¤çš„å€™é€‰äººä¿¡æ¯"""
        try:
            for step in workflow_steps:
                if step.get('id') == step_id:
                    candidates = step.get('candidates', {})
                    if candidates and isinstance(candidates, dict):
                        # ç¡®ä¿å€™é€‰äººæ•°æ®æ ¼å¼æ­£ç¡®
                        formatted_candidates = {
                            'users': candidates.get('users', []),
                            'roles': candidates.get('roles', []),
                            'companies': candidates.get('companies', [])
                        }
                        # åªè¿”å›éç©ºçš„å€™é€‰äººä¿¡æ¯
                        if any(formatted_candidates.values()):
                            return formatted_candidates
            return {}
        except Exception as e:
            print(f"[WARNING] Failed to parse workflow candidates config: {e}")
            return {}
    
    # ========================================================================
    # æ–‡ä»¶å®¡æ‰¹å†å²åŒæ­¥ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    # ========================================================================
    
    async def async_sync_file_approval_history(
        self,
        session: aiohttp.ClientSession,
        api_client,
        project_id: str,
        file_version_urn: str,
        review_data: Optional[Dict] = None
    ) -> int:
        """
        å¼‚æ­¥åŒæ­¥å•ä¸ªæ–‡ä»¶çš„å®¡æ‰¹å†å²
        
        Args:
            session: aiohttpä¼šè¯
            api_client: APIå®¢æˆ·ç«¯
            project_id: é¡¹ç›®ID
            file_version_urn: æ–‡ä»¶ç‰ˆæœ¬URN
            review_data: è¯„å®¡æ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒæ­¥çš„è®°å½•æ•°
        """
        try:
            # URLç¼–ç æ–‡ä»¶URN
            import urllib.parse
            encoded_urn = urllib.parse.quote(file_version_urn, safe='')
            
            # è°ƒç”¨APIè·å–å®¡æ‰¹çŠ¶æ€
            url = f'/projects/{project_id}/versions/{encoded_urn}/approval-statuses'
            result = await self._async_api_call(session, 'GET', url)
            
            if not result or 'results' not in result:
                return 0
            
            approval_records = []
            for item in result.get('results', []):
                approval_status = item.get('approvalStatus', {})
                review = item.get('review', {})
                
                record = {
                    'file_version_urn': file_version_urn,
                    'file_item_urn': review_data.get('itemUrn') if review_data else None,
                    'file_name': review_data.get('name') if review_data else None,
                    'review_acc_id': review.get('id'),
                    'review_sequence_id': review.get('sequenceId'),
                    'review_status': review.get('status'),
                    'review_name': review_data.get('name') if review_data else None,
                    'approval_status_id': approval_status.get('id'),
                    'approval_status_label': approval_status.get('label'),
                    'approval_status_value': approval_status.get('value'),
                    'approval_status_type': approval_status.get('type'),
                    'is_current': review.get('status') == 'OPEN',
                    'is_latest_in_review': True
                }
                
                approval_records.append(record)
            
            # æ‰¹é‡æ’å…¥
            if approval_records:
                inserted_count = self.da.batch_insert_file_approval_history(approval_records)
                return inserted_count
            
            return 0
            
        except Exception as e:
            error_msg = f"åŒæ­¥æ–‡ä»¶å®¡æ‰¹å†å²å¤±è´¥ (URN: {file_version_urn}): {str(e)}"
            self.sync_stats['errors'].append(error_msg)
            print(f"[WARNING] {error_msg}")
            return 0
    
    # ========================================================================
    # æ™ºèƒ½åˆ†é¡µè·å–ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    # ========================================================================
    
    async def async_fetch_all_reviews_with_pagination(
        self,
        session: aiohttp.ClientSession,
        api_client,
        project_id: str,
        limit_per_page: int = 50,
        show_progress: bool = True
    ) -> List[Dict]:
        """
        å¼‚æ­¥æ™ºèƒ½åˆ†é¡µè·å–æ‰€æœ‰è¯„å®¡
        
        Args:
            session: aiohttpä¼šè¯
            api_client: APIå®¢æˆ·ç«¯
            project_id: é¡¹ç›®ID
            limit_per_page: æ¯é¡µæ•°é‡
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            æ‰€æœ‰è¯„å®¡åˆ—è¡¨
        """
        if show_progress:
            print(f"\n[FETCH] Async smart pagination for review list...")
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨è·å–æ€»æ•°
        first_page = await self._async_api_call(
            session,
            'GET',
            f'/projects/{project_id}/reviews',
            params={'limit': limit_per_page, 'offset': 0}
        )
        
        total_results = first_page.get('pagination', {}).get('totalResults', 0)
        reviews = first_page.get('results', [])
        
        if show_progress:
            print(f"   æ€»è¯„å®¡æ•°: {total_results}")
            print(f"   ç¬¬ä¸€é¡µ: {len(reviews)} ä¸ª")
        
        # è®¡ç®—éœ€è¦çš„é¡µæ•°
        pages_needed = (total_results - limit_per_page + limit_per_page - 1) // limit_per_page
        
        if pages_needed > 0:
            if show_progress:
                print(f"   éœ€è¦é¢å¤–è·å– {pages_needed} é¡µ...")
            
            # å¹¶è¡Œè·å–å‰©ä½™é¡µé¢
            semaphore = asyncio.Semaphore(min(5, pages_needed))
            tasks = []
            
            for page in range(1, pages_needed + 1):
                offset = page * limit_per_page
                task = self._async_fetch_reviews_page(
                    session,
                    api_client,
                    project_id,
                    limit_per_page,
                    offset,
                    semaphore
                )
                tasks.append(task)
            
            page_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for idx, result in enumerate(page_results, 1):
                if isinstance(result, Exception):
                    error_msg = f"è·å–åˆ†é¡µæ•°æ®å¤±è´¥: {result}"
                    self.sync_stats['errors'].append(error_msg)
                    print(f"[WARNING] {error_msg}")
                else:
                    reviews.extend(result)
                    if show_progress and idx % 5 == 0:
                        print(f"   Retrieved {idx}/{pages_needed} pages")
        
        if show_progress:
            print(f"SUCCESS: Retrieved {len(reviews)} reviews total\n")
        
        return reviews
    
    async def _async_fetch_reviews_page(
        self,
        session: aiohttp.ClientSession,
        api_client,
        project_id: str,
        limit: int,
        offset: int,
        semaphore: asyncio.Semaphore
    ) -> List[Dict]:
        """å¼‚æ­¥è·å–å•é¡µè¯„å®¡æ•°æ®"""
        async with semaphore:
            try:
                result = await self._async_api_call(
                    session,
                    'GET',
                    f'/projects/{project_id}/reviews',
                    params={'limit': limit, 'offset': offset}
                )
                return result.get('results', [])
            except Exception as e:
                print(f"[WARNING] Failed to fetch review page (offset: {offset}): {e}")
                return []
    
    # ========================================================================
    # æ€§èƒ½æŠ¥å‘Š
    # ========================================================================
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š"""
        metrics_dict = self.metrics.to_dict()
        
        return {
            'summary': {
                'total_time': self.metrics.total_time,
                'api_calls': self.metrics.api_calls,
                'api_success_rate': self.metrics.get_api_success_rate(),
                'cache_hit_rate': self.metrics.get_cache_hit_rate(),
                'db_queries': self.metrics.db_queries,
                'memory_usage_mb': self.metrics.memory_usage_mb
            },
            'detailed_metrics': metrics_dict,
            'sync_stats': self.sync_stats,
            'circuit_breaker': self.circuit_breaker,
            'bottlenecks': self._identify_bottlenecks()
        }
    
    def _identify_bottlenecks(self) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # APIæ—¶é—´å æ¯”è¿‡é«˜
        if self.metrics.total_time > 0:
            api_ratio = self.metrics.api_time / self.metrics.total_time
            if api_ratio > 0.7:
                bottlenecks.append({
                    'type': 'api',
                    'severity': 'high',
                    'message': f'API calls consume {api_ratio*100:.1f}% of total time',
                    'suggestion': 'Consider increasing concurrency or using more caching'
                })
        
        # ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½
        cache_hit_rate = self.metrics.get_cache_hit_rate()
        if cache_hit_rate < 30 and self.cache.enabled:
            bottlenecks.append({
                'type': 'cache',
                'severity': 'medium',
                'message': f'Cache hit rate is only {cache_hit_rate:.1f}%',
                'suggestion': 'Consider increasing cache TTL or warming up cache'
            })
        
        # æ•°æ®åº“æ—¶é—´å æ¯”è¿‡é«˜
        if self.metrics.total_time > 0:
            db_ratio = self.metrics.db_time / self.metrics.total_time
            if db_ratio > 0.5:
                bottlenecks.append({
                    'type': 'database',
                    'severity': 'high',
                    'message': f'Database operations consume {db_ratio*100:.1f}% of total time',
                    'suggestion': 'Consider increasing batch size or optimizing SQL'
                })
        
        return bottlenecks
    
    def print_performance_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        report = self.get_performance_report()
        
        print("\n" + "=" * 80)
        print("[PERFORMANCE ANALYSIS REPORT]")
        print("=" * 80)
        
        summary = report['summary']
        print(f"\nSummary:")
        print(f"  Total Time: {summary['total_time']:.2f}s")
        print(f"  API Calls: {summary['api_calls']} times (Success Rate: {summary['api_success_rate']:.1f}%)")
        print(f"  Cache Hit Rate: {summary['cache_hit_rate']:.1f}%")
        print(f"  DB Queries: {summary['db_queries']} times")
        print(f"  Memory Usage: {summary['memory_usage_mb']:.2f}MB")
        
        # æ—¶é—´åˆ†è§£
        print(f"\nTiming Breakdown:")
        timing = self.metrics.timing_breakdown
        for operation, duration in sorted(timing.items(), key=lambda x: x[1], reverse=True)[:10]:
            percentage = (duration / summary['total_time'] * 100) if summary['total_time'] > 0 else 0
            print(f"  {operation:.<40} {duration:>8.2f}s ({percentage:>5.1f}%)")
        
        # ç“¶é¢ˆåˆ†æ
        bottlenecks = report['bottlenecks']
        if bottlenecks:
            print(f"\n[WARNING] Performance Bottlenecks:")
            for bn in bottlenecks:
                severity_label = {'high': '[HIGH]', 'medium': '[MEDIUM]', 'low': '[LOW]'}
                print(f"  {severity_label.get(bn['severity'], '[INFO]')} [{bn['type'].upper()}] {bn['message']}")
                print(f"     [TIP] {bn['suggestion']}")
        else:
            print(f"\n[OK] No significant bottlenecks detected")
        
        print("\n" + "=" * 80)
    
    # ========================================================================
    # å·¥ä½œæµæ¨¡æ¿åŒæ­¥åŠŸèƒ½ï¼ˆæ•´åˆè‡ª template_sync_api.pyï¼‰
    # ========================================================================
    
    async def sync_workflow_templates_enhanced(
        self,
        session: aiohttp.ClientSession,
        project_id: str,
        access_token: str,
        fetch_detailed_data: bool = True,
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """
        å¢å¼ºçš„å·¥ä½œæµæ¨¡æ¿åŒæ­¥ - æ”¯æŒè¯¦ç»†æ•°æ®è·å–
        
        Args:
            session: aiohttpä¼šè¯
            project_id: é¡¹ç›®ID
            access_token: è®¿é—®ä»¤ç‰Œ
            fetch_detailed_data: æ˜¯å¦è·å–è¯¦ç»†æ•°æ®ï¼ˆè°ƒç”¨å•ä¸ªworkflow APIï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        if show_progress:
            print(f"\nTARGET: å¼€å§‹åŒæ­¥å·¥ä½œæµæ¨¡æ¿: {project_id}")
            print(f"   è¯¦ç»†æ•°æ®è·å–: {'å¯ç”¨' if fetch_detailed_data else 'ç¦ç”¨'}")
        
        start_time = time.time()
        
        # è®¾ç½®ä¸´æ—¶headers
        self._temp_headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
        
        try:
            # æ¸…ç†é¡¹ç›®IDå‰ç¼€
            clean_project_id = project_id.replace('b.', '') if project_id.startswith('b.') else project_id
            
            # ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰å·¥ä½œæµï¼ˆä½œä¸ºæ¨¡æ¿ï¼‰
            all_workflows = await self._fetch_all_workflows_for_templates(
                session, clean_project_id, show_progress
            )
            
            if not all_workflows:
                return {
                    'project_id': project_id,
                    'total_templates_fetched': 0,
                    'templates_inserted': 0,
                    'templates_updated': 0,
                    'templates_skipped': 0,
                    'errors': [],
                    'sync_time': datetime.now(timezone.utc).isoformat()
                }
            
            # ç¬¬äºŒæ­¥ï¼šå¦‚æœå¯ç”¨è¯¦ç»†æ•°æ®è·å–ï¼Œå¹¶è¡Œè·å–æ¯ä¸ªå·¥ä½œæµçš„è¯¦ç»†ä¿¡æ¯
            if fetch_detailed_data:
                detailed_workflows = await self._fetch_workflows_details_parallel_for_templates(
                    session, clean_project_id, all_workflows, show_progress
                )
            else:
                detailed_workflows = all_workflows
            
            # ç¬¬ä¸‰æ­¥ï¼šåˆ†æå·¥ä½œæµå¹¶åˆ†ç±»ä¸ºæ¨¡æ¿
            template_analysis = self._analyze_workflows_for_templates(detailed_workflows, show_progress)
            
            # ç¬¬å››æ­¥ï¼šæ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“
            sync_stats = await self._batch_upsert_workflow_templates(
                project_id, template_analysis['suitable_templates'], show_progress
            )
            
            elapsed_time = time.time() - start_time
            
            result = {
                'project_id': project_id,
                'total_workflows_fetched': len(all_workflows),
                'detailed_workflows_fetched': len(detailed_workflows) if fetch_detailed_data else 0,
                'suitable_templates_found': len(template_analysis['suitable_templates']),
                'templates_inserted': sync_stats['inserted'],
                'templates_updated': sync_stats['updated'],
                'templates_skipped': sync_stats['skipped'],
                'template_analysis': template_analysis['summary'],
                'errors': sync_stats['errors'],
                'elapsed_time': f"{elapsed_time:.2f}ç§’",
                'sync_time': datetime.now(timezone.utc).isoformat()
            }
            
            if show_progress:
                print(f"\nâœ… æ¨¡æ¿åŒæ­¥å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                print(f"   å‘ç° {len(template_analysis['suitable_templates'])} ä¸ªé€‚åˆçš„æ¨¡æ¿")
                print(f"   æ–°å¢: {sync_stats['inserted']}, æ›´æ–°: {sync_stats['updated']}")
            
            return result
            
        except Exception as e:
            error_msg = f"æ¨¡æ¿åŒæ­¥å¤±è´¥: {str(e)}"
            self.sync_stats['errors'].append(error_msg)
            if show_progress:
                print(f"âŒ {error_msg}")
            raise
        
        finally:
            # æ¸…ç†ä¸´æ—¶headers
            if hasattr(self, '_temp_headers'):
                delattr(self, '_temp_headers')
    
    async def _fetch_all_workflows_for_templates(
        self,
        session: aiohttp.ClientSession,
        project_id: str,
        show_progress: bool = True
    ) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·¥ä½œæµç”¨äºæ¨¡æ¿åˆ†æ
        """
        if show_progress:
            print(f"INFO: è·å–å·¥ä½œæµåˆ—è¡¨...")
        
        all_workflows = []
        offset = 0
        limit = 50
        
        while True:
            try:
                result = await self._async_api_call(
                    session,
                    'GET',
                    f'/projects/{project_id}/workflows',
                    headers=self._temp_headers,
                    params={
                        'limit': limit,
                        'offset': offset,
                        'sort': 'name'
                    }
                )
                
                workflows = result.get('results', [])
                all_workflows.extend(workflows)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
                pagination = result.get('pagination', {})
                total_results = pagination.get('totalResults', len(workflows))
                
                if show_progress:
                    print(f"   è·å–åˆ° {len(workflows)} ä¸ªå·¥ä½œæµï¼Œæ€»è®¡ {len(all_workflows)}/{total_results}")
                
                if offset + len(workflows) >= total_results:
                    break
                
                offset += limit
                
            except Exception as e:
                error_msg = f"è·å–å·¥ä½œæµåˆ—è¡¨å¤±è´¥ (offset: {offset}): {str(e)}"
                self.sync_stats['errors'].append(error_msg)
                print(f"[WARNING] {error_msg}")
                break
        
        if show_progress:
            print(f"âœ… å…±è·å– {len(all_workflows)} ä¸ªå·¥ä½œæµ")
        
        return all_workflows
    
    async def _fetch_workflows_details_parallel_for_templates(
        self,
        session: aiohttp.ClientSession,
        project_id: str,
        workflows: List[Dict],
        show_progress: bool = True
    ) -> List[Dict]:
        """
        å¹¶è¡Œè·å–å·¥ä½œæµè¯¦ç»†ä¿¡æ¯ç”¨äºæ¨¡æ¿åˆ†æ
        """
        if show_progress:
            print(f"ğŸ” å¹¶è¡Œè·å– {len(workflows)} ä¸ªå·¥ä½œæµçš„è¯¦ç»†ä¿¡æ¯...")
        
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(min(self.max_concurrent, 10))
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = []
        for workflow in workflows:
            workflow_id = workflow.get('id')
            if workflow_id:
                task = self._fetch_single_workflow_detail_for_template(
                    session, project_id, workflow_id, workflow, semaphore
                )
                tasks.append(task)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        detailed_workflows = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
        successful_workflows = []
        for result in detailed_workflows:
            if isinstance(result, Exception):
                error_msg = f"è·å–å·¥ä½œæµè¯¦æƒ…å¤±è´¥: {str(result)}"
                self.sync_stats['errors'].append(error_msg)
                if show_progress:
                    print(f"   âš  {error_msg}")
            else:
                successful_workflows.append(result)
        
        if show_progress:
            print(f"âœ… æˆåŠŸè·å– {len(successful_workflows)}/{len(workflows)} ä¸ªè¯¦ç»†å·¥ä½œæµ")
        
        return successful_workflows
    
    async def _fetch_single_workflow_detail_for_template(
        self,
        session: aiohttp.ClientSession,
        project_id: str,
        workflow_id: str,
        base_workflow: Dict,
        semaphore: asyncio.Semaphore
    ) -> Dict:
        """
        è·å–å•ä¸ªå·¥ä½œæµçš„è¯¦ç»†ä¿¡æ¯
        """
        async with semaphore:
            try:
                # è°ƒç”¨è¯¦ç»†API
                detailed_data = await self._async_api_call(
                    session,
                    'GET',
                    f'/projects/{project_id}/workflows/{workflow_id}',
                    headers=self._temp_headers
                )
                
                # åˆå¹¶åŸºç¡€æ•°æ®å’Œè¯¦ç»†æ•°æ®
                if detailed_data:
                    # ä½¿ç”¨è¯¦ç»†æ•°æ®ï¼Œä½†ä¿ç•™ä¸€äº›åŸºç¡€å­—æ®µ
                    detailed_data['projectId'] = project_id
                    return detailed_data
                else:
                    return base_workflow
                    
            except Exception as e:
                # å¦‚æœè¯¦ç»†APIå¤±è´¥ï¼Œè¿”å›åŸºç¡€æ•°æ®
                print(f"âš  è·å–å·¥ä½œæµè¯¦æƒ…å¤±è´¥ {workflow_id}: {e}")
                return base_workflow
    
    def _analyze_workflows_for_templates(
        self,
        workflows: List[Dict],
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """
        åˆ†æå·¥ä½œæµå¹¶è¯†åˆ«é€‚åˆä½œä¸ºæ¨¡æ¿çš„å·¥ä½œæµ
        """
        if show_progress:
            print(f"ğŸ” åˆ†æ {len(workflows)} ä¸ªå·¥ä½œæµ...")
        
        suitable_templates = []
        analysis_summary = {
            'total_analyzed': len(workflows),
            'suitable_for_template': 0,
            'has_group_review': 0,
            'template_types': {},
            'complexity_distribution': {'simple': 0, 'moderate': 0, 'complex': 0}
        }
        
        for workflow in workflows:
            analysis = self._analyze_single_workflow_for_template(workflow)
            
            # åˆ¤æ–­æ˜¯å¦é€‚åˆä½œä¸ºæ¨¡æ¿
            if analysis['is_template_worthy']:
                # æ·»åŠ åˆ†æç»“æœåˆ°å·¥ä½œæµæ•°æ®
                workflow['template_analysis'] = analysis
                suitable_templates.append(workflow)
                analysis_summary['suitable_for_template'] += 1
                
                # ç»Ÿè®¡æ¨¡æ¿ç±»å‹
                template_type = analysis['template_type']
                analysis_summary['template_types'][template_type] = \
                    analysis_summary['template_types'].get(template_type, 0) + 1
                
                # ç»Ÿè®¡å¤æ‚åº¦
                complexity = analysis['complexity_level']
                analysis_summary['complexity_distribution'][complexity] += 1
                
                if analysis['has_group_review']:
                    analysis_summary['has_group_review'] += 1
        
        if show_progress:
            print(f"   é€‚åˆä½œä¸ºæ¨¡æ¿: {analysis_summary['suitable_for_template']} ä¸ª")
            print(f"   åŒ…å«ç»„å®¡æ ¸: {analysis_summary['has_group_review']} ä¸ª")
            if analysis_summary['template_types']:
                print(f"   æ¨¡æ¿ç±»å‹åˆ†å¸ƒ: {analysis_summary['template_types']}")
        
        return {
            'suitable_templates': suitable_templates,
            'summary': analysis_summary
        }
    
    def _analyze_single_workflow_for_template(self, workflow: Dict) -> Dict:
        """
        åˆ†æå•ä¸ªå·¥ä½œæµçš„æ¨¡æ¿ç‰¹å¾
        """
        steps = workflow.get('steps', [])
        
        analysis = {
            'steps_count': len(steps),
            'has_group_review': False,
            'has_specific_users': False,
            'has_specific_companies': False,
            'has_role_assignments': False,
            'complexity_score': 0,
            'is_template_worthy': False,
            'template_type': 'custom',
            'complexity_level': 'simple',
            'base_template_match': None
        }
        
        # åˆ†ææ­¥éª¤ç‰¹å¾
        for step in steps:
            # æ£€æŸ¥ç»„å®¡æ ¸
            group_review = step.get('groupReview', {})
            if group_review.get('enabled', False):
                analysis['has_group_review'] = True
                analysis['complexity_score'] += 2
            
            # æ£€æŸ¥å€™é€‰äººé…ç½®
            candidates = step.get('candidates', {})
            
            if candidates.get('users'):
                analysis['has_specific_users'] = True
                analysis['complexity_score'] += 3  # å…·ä½“ç”¨æˆ·é™ä½æ¨¡æ¿ä»·å€¼
            
            if candidates.get('companies'):
                analysis['has_specific_companies'] = True
                analysis['complexity_score'] += 2
            
            if candidates.get('roles'):
                analysis['has_role_assignments'] = True
                analysis['complexity_score'] += 1  # è§’è‰²åˆ†é…å¢åŠ æ¨¡æ¿ä»·å€¼
        
        # åˆ¤æ–­æ˜¯å¦é€‚åˆä½œä¸ºæ¨¡æ¿
        # ä¼˜å…ˆè€ƒè™‘æ²¡æœ‰å…·ä½“ç”¨æˆ·åˆ†é…çš„å·¥ä½œæµ
        analysis['is_template_worthy'] = (
            not analysis['has_specific_users'] and  # æ²¡æœ‰å…·ä½“ç”¨æˆ·
            analysis['steps_count'] > 0 and         # æœ‰æ­¥éª¤
            analysis['complexity_score'] <= 5       # å¤æ‚åº¦ä¸å¤ªé«˜
        )
        
        # ç¡®å®šæ¨¡æ¿ç±»å‹
        analysis['template_type'] = self._determine_enhanced_template_type(analysis)
        
        # ç¡®å®šå¤æ‚åº¦çº§åˆ«
        if analysis['complexity_score'] <= 2:
            analysis['complexity_level'] = 'simple'
        elif analysis['complexity_score'] <= 4:
            analysis['complexity_level'] = 'moderate'
        else:
            analysis['complexity_level'] = 'complex'
        
        # å°è¯•åŒ¹é…åŸºç¡€æ¨¡æ¿
        analysis['base_template_match'] = self._match_base_template(analysis)
        
        return analysis
    
    def _determine_enhanced_template_type(self, analysis: Dict) -> str:
        """
        å¢å¼ºçš„æ¨¡æ¿ç±»å‹ç¡®å®š
        """
        steps_count = analysis['steps_count']
        has_group = analysis['has_group_review']
        
        if has_group:
            if steps_count == 2:
                return 'two_step_group'
            elif steps_count == 3:
                return 'three_step_group'
            elif steps_count == 4:
                return 'four_step_group'
            elif steps_count == 5:
                return 'five_step_group'
            else:
                return 'custom_group'
        else:
            if steps_count == 1:
                return 'one_step'
            elif steps_count == 2:
                return 'two_step'
            elif steps_count == 3:
                return 'three_step'
            elif steps_count == 4:
                return 'four_step'
            elif steps_count == 5:
                return 'five_step'
            else:
                return 'custom'
    
    def _match_base_template(self, analysis: Dict) -> Optional[str]:
        """
        åŒ¹é…åŸºç¡€æ¨¡æ¿
        """
        steps_count = analysis['steps_count']
        has_group = analysis['has_group_review']
        
        # åŸºç¡€æ¨¡æ¿åŒ¹é…è§„åˆ™
        if has_group:
            template_map = {
                2: 'two_step_group',
                3: 'three_step_group',
                4: 'four_step_group'
            }
        else:
            template_map = {
                1: 'one_step',
                2: 'two_step',
                3: 'three_step',
                4: 'four_step',
                5: 'five_step'
            }
        
        return template_map.get(steps_count)
    
    async def _batch_upsert_workflow_templates(
        self,
        project_id: str,
        workflows: List[Dict],
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡UPSERTå·¥ä½œæµæ¨¡æ¿åˆ°workflow_templatesè¡¨
        """
        stats = {
            'inserted': 0,
            'updated': 0,
            'skipped': 0,
            'errors': []
        }
        
        if not workflows:
            return stats
        
        if show_progress:
            print(f"ğŸ’¾ æ‰¹é‡åŒæ­¥ {len(workflows)} ä¸ªæ¨¡æ¿åˆ°æ•°æ®åº“...")
        
        start_time = time.time()
        
        try:
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            batch_data = []
            for workflow in workflows:
                template_data = self._transform_workflow_to_template(project_id, workflow)
                batch_data.append(template_data)
            
            # æ‰§è¡Œæ‰¹é‡UPSERT
            upsert_sql = """
                INSERT INTO workflow_templates (
                    template_uuid, name, description, category, industry,
                    acc_template_id, data_source, template_type, steps_count,
                    steps_config, template_config, default_settings,
                    additional_options, approval_status_options, copy_files_options,
                    attached_attributes, update_attributes_options,
                    created_by, is_active, is_public, is_template,
                    base_template_key, template_characteristics,
                    last_synced_at, sync_status, created_at, updated_at
                ) VALUES (
                    %(template_uuid)s, %(name)s, %(description)s, %(category)s, %(industry)s,
                    %(acc_template_id)s, %(data_source)s, %(template_type)s, %(steps_count)s,
                    %(steps_config)s, %(template_config)s, %(default_settings)s,
                    %(additional_options)s, %(approval_status_options)s, %(copy_files_options)s,
                    %(attached_attributes)s, %(update_attributes_options)s,
                    %(created_by)s, %(is_active)s, %(is_public)s, %(is_template)s,
                    %(base_template_key)s, %(template_characteristics)s,
                    %(last_synced_at)s, %(sync_status)s, %(created_at)s, %(updated_at)s
                )
                ON CONFLICT (acc_template_id)
                DO UPDATE SET
                    name = EXCLUDED.name,
                    description = EXCLUDED.description,
                    steps_count = EXCLUDED.steps_count,
                    steps_config = EXCLUDED.steps_config,
                    template_config = EXCLUDED.template_config,
                    additional_options = EXCLUDED.additional_options,
                    approval_status_options = EXCLUDED.approval_status_options,
                    copy_files_options = EXCLUDED.copy_files_options,
                    attached_attributes = EXCLUDED.attached_attributes,
                    update_attributes_options = EXCLUDED.update_attributes_options,
                    is_active = EXCLUDED.is_active,
                    base_template_key = EXCLUDED.base_template_key,
                    template_characteristics = EXCLUDED.template_characteristics,
                    last_synced_at = EXCLUDED.last_synced_at,
                    sync_status = EXCLUDED.sync_status,
                    updated_at = EXCLUDED.updated_at
                RETURNING id, (xmax = 0) AS inserted
            """
            
            # æ‰§è¡Œæ‰¹é‡æ“ä½œ
            with self.da.get_connection() as conn:
                with conn.cursor() as cur:
                    results = []
                    for data in batch_data:
                        try:
                            cur.execute(upsert_sql, data)
                            result = cur.fetchone()
                            if result:
                                results.append(result)
                        except Exception as e:
                            error_msg = f"å¤„ç†æ¨¡æ¿å¤±è´¥ {data.get('name', 'unknown')}: {str(e)}"
                            stats['errors'].append(error_msg)
                            if show_progress:
                                print(f"   âš  {error_msg}")
                    
                    conn.commit()
            
            # ç»Ÿè®¡ç»“æœ
            inserted = sum(1 for _, is_insert in results if is_insert)
            updated = len(results) - inserted
            
            stats['inserted'] = inserted
            stats['updated'] = updated
            
            elapsed = time.time() - start_time
            self.metrics.db_queries += 1
            self.metrics.db_time += elapsed
            
            if show_progress:
                print(f"âœ… æ‰¹é‡UPSERTæ¨¡æ¿å®Œæˆ: {inserted}ä¸ªæ–°å»º, {updated}ä¸ªæ›´æ–° (è€—æ—¶: {elapsed:.2f}ç§’)")
            
            return stats
            
        except Exception as e:
            error_msg = f"æ‰¹é‡åŒæ­¥æ¨¡æ¿å¤±è´¥: {str(e)}"
            stats['errors'].append(error_msg)
            self.sync_stats['errors'].append(error_msg)
            if show_progress:
                print(f"âŒ {error_msg}")
            raise
    
    def _transform_workflow_to_template(self, project_id: str, workflow: Dict) -> Dict:
        """
        è½¬æ¢ACCå·¥ä½œæµæ•°æ®ä¸ºæ¨¡æ¿æ ¼å¼
        """
        now = datetime.now(timezone.utc)
        
        # è·å–åˆ†æç»“æœ
        analysis = workflow.get('template_analysis', {})
        
        # è§£ææ­¥éª¤é…ç½®
        steps = workflow.get('steps', [])
        steps_count = len(steps)
        
        # è½¬æ¢æ­¥éª¤é…ç½®
        steps_config = self._transform_steps_config_for_template(steps)
        
        # è§£ææ—¶é—´æˆ³
        created_at = self._parse_timestamp(workflow.get('createdAt'))
        updated_at = self._parse_timestamp(workflow.get('updatedAt'))
        
        return {
            'template_uuid': workflow.get('id'),  # ä½¿ç”¨ACC workflow IDä½œä¸ºUUID
            'name': workflow.get('name', 'Unnamed Template'),
            'description': workflow.get('description'),
            'category': 'acc_synced',
            'industry': 'construction',
            'acc_template_id': workflow.get('id'),
            'data_source': 'acc_sync',
            'template_type': analysis.get('template_type', 'custom'),
            'steps_count': steps_count,
            'steps_config': json.dumps(steps_config),
            'template_config': json.dumps(workflow),  # ä¿å­˜å®Œæ•´çš„åŸå§‹æ•°æ®
            'default_settings': json.dumps({}),
            'additional_options': json.dumps(workflow.get('additionalOptions', {})),
            'approval_status_options': json.dumps(workflow.get('approvalStatusOptions', [])),
            'copy_files_options': json.dumps(workflow.get('copyFilesOptions', {})),
            'attached_attributes': json.dumps(workflow.get('attachedAttributes', [])),
            'update_attributes_options': json.dumps(workflow.get('updateAttributesOptions', {})),
            'created_by': json.dumps(workflow.get('createdBy', {})),
            'is_active': workflow.get('status', 'ACTIVE') == 'ACTIVE',
            'is_public': True,  # ACCæ¨¡æ¿é»˜è®¤ä¸ºå…¬å…±
            'is_template': True,
            'base_template_key': analysis.get('base_template_match'),
            'template_characteristics': json.dumps({
                'complexity_level': analysis.get('complexity_level', 'simple'),
                'has_group_review': analysis.get('has_group_review', False),
                'has_role_assignments': analysis.get('has_role_assignments', False),
                'complexity_score': analysis.get('complexity_score', 0),
                'analysis_version': '1.0'
            }),
            'last_synced_at': now,
            'sync_status': 'synced',
            'created_at': created_at or now,
            'updated_at': updated_at or now
        }
    
    def _transform_steps_config_for_template(self, steps: List[Dict]) -> List[Dict]:
        """
        è½¬æ¢æ­¥éª¤é…ç½®ä¸ºæ¨¡æ¿æ ¼å¼
        """
        transformed_steps = []
        
        for idx, step in enumerate(steps):
            # è§£æç»„å®¡æ ¸é…ç½®
            group_review = step.get('groupReview', {})
            
            # ç¡®å®šå®¡æ ¸è€…ç±»å‹
            reviewer_type = 'SINGLE_REVIEWER'
            if group_review.get('enabled'):
                reviewer_type = 'GROUP_APPROVAL'
            elif len(step.get('candidates', {}).get('users', [])) > 1:
                reviewer_type = 'MULTIPLE_REVIEWERS'
            
            # è½¬æ¢æ­¥éª¤é…ç½®
            step_config = {
                'id': step.get('id', f'step_{idx + 1}'),
                'name': step.get('name', f'Step {idx + 1}'),
                'type': step.get('type', 'REVIEWER'),
                'order': idx + 1,
                'reviewer_type': reviewer_type,
                'time_allowed': step.get('duration', 3),
                'time_unit': step.get('dueDateType', 'CALENDAR_DAY'),
                'enable_sent_back': True,  # é»˜è®¤å¯ç”¨
                'group_review': {
                    'enabled': group_review.get('enabled', False),
                    'type': group_review.get('type', 'MINIMUM'),
                    'min': group_review.get('min', 1)
                },
                'candidates': step.get('candidates', {
                    'users': [],
                    'roles': [],
                    'companies': []
                }),
                'original_step_data': step  # ä¿å­˜åŸå§‹æ•°æ®
            }
            
            transformed_steps.append(step_config)
        
        return transformed_steps
    
    # ========================================================================
    # åŸºç¡€æ¨¡æ¿ç®¡ç†
    # ========================================================================
    
    def get_base_templates(self, category: Optional[str] = None) -> List[Dict]:
        """
        è·å–åŸºç¡€æ¨¡æ¿åˆ—è¡¨
        
        Args:
            category: æ¨¡æ¿åˆ†ç±»è¿‡æ»¤
            
        Returns:
            åŸºç¡€æ¨¡æ¿åˆ—è¡¨
        """
        try:
            with self.da.get_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                    if category:
                        cur.execute(
                            "SELECT * FROM base_templates WHERE category = %s AND is_active = true ORDER BY display_order",
                            (category,)
                        )
                    else:
                        cur.execute(
                            "SELECT * FROM base_templates WHERE is_active = true ORDER BY display_order"
                        )
                    
                    templates = cur.fetchall()
                    
                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨å¹¶è§£æJSONå­—æ®µ
                    result = []
                    for template in templates:
                        template_dict = dict(template)
                        
                        # è§£æJSONå­—æ®µ
                        if template_dict.get('base_steps_config'):
                            try:
                                template_dict['base_steps_config'] = json.loads(template_dict['base_steps_config'])
                            except:
                                template_dict['base_steps_config'] = []
                        
                        result.append(template_dict)
                    
                    return result
                    
        except Exception as e:
            error_msg = f"è·å–åŸºç¡€æ¨¡æ¿å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return []
    
    def create_workflow_from_base_template(
        self,
        base_template_key: str,
        workflow_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        åŸºäºåŸºç¡€æ¨¡æ¿åˆ›å»ºå·¥ä½œæµ
        
        Args:
            base_template_key: åŸºç¡€æ¨¡æ¿é”®å€¼
            workflow_data: å·¥ä½œæµé…ç½®æ•°æ®
            
        Returns:
            åˆ›å»ºç»“æœ
        """
        try:
            # è·å–åŸºç¡€æ¨¡æ¿
            base_templates = self.get_base_templates()
            base_template = next(
                (t for t in base_templates if t['template_key'] == base_template_key), 
                None
            )
            
            if not base_template:
                raise ValueError(f"åŸºç¡€æ¨¡æ¿ä¸å­˜åœ¨: {base_template_key}")
            
            # åˆå¹¶åŸºç¡€æ¨¡æ¿å’Œç”¨æˆ·é…ç½®
            merged_config = {
                'name': workflow_data.get('name', base_template['name']),
                'description': workflow_data.get('description', base_template['description']),
                'steps': base_template['base_steps_config'].copy(),
                'base_template_key': base_template_key,
                'template_type': base_template['template_key'],
                'steps_count': base_template['steps_count'],
                'has_group_review': base_template['has_group_review']
            }
            
            # åº”ç”¨ç”¨æˆ·è‡ªå®šä¹‰é…ç½®
            if 'steps_config' in workflow_data:
                # ç”¨æˆ·æä¾›äº†æ­¥éª¤é…ç½®ï¼Œåˆå¹¶åˆ°åŸºç¡€æ­¥éª¤ä¸­
                user_steps = workflow_data['steps_config']
                for i, user_step in enumerate(user_steps):
                    if i < len(merged_config['steps']):
                        # æ›´æ–°ç°æœ‰æ­¥éª¤
                        merged_config['steps'][i].update(user_step)
            
            return {
                'status': 'success',
                'workflow_config': merged_config,
                'base_template': base_template
            }
            
        except Exception as e:
            error_msg = f"åŸºäºåŸºç¡€æ¨¡æ¿åˆ›å»ºå·¥ä½œæµå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {'status': 'error', 'error': error_msg}
    
    # ========================================================================
    # æ³¨æ„ï¼šè´¦æˆ·æ•°æ®åŒæ­¥åŠŸèƒ½å·²ç§»é™¤
    # ç°åœ¨ä½¿ç”¨ç‹¬ç«‹çš„ database_sql/account_sync.py æ¥å¤„ç†è´¦æˆ·ã€ç”¨æˆ·ã€å…¬å¸ã€è§’è‰²åŒæ­¥
    # ========================================================================
    
    async def full_project_sync_with_templates(
        self,
        project_id: str,
        access_token: str,
        sync_templates: bool = True,
        fetch_detailed_template_data: bool = True,
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œæ•´çš„é¡¹ç›®åŒæ­¥ï¼ˆåŒ…å«æ¨¡æ¿å’Œè¯„å®¡æ•°æ®ï¼‰
        æ³¨æ„ï¼šè´¦æˆ·æ•°æ®åŒæ­¥å·²ç§»é™¤ï¼Œè¯·ä½¿ç”¨ç‹¬ç«‹çš„ account_sync.py
        
        Args:
            project_id: é¡¹ç›®ID
            access_token: è®¿é—®ä»¤ç‰Œ
            sync_templates: æ˜¯å¦åŒæ­¥å·¥ä½œæµæ¨¡æ¿
            fetch_detailed_template_data: æ˜¯å¦è·å–è¯¦ç»†æ¨¡æ¿æ•°æ®
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            å®Œæ•´çš„åŒæ­¥ç»“æœ
        """
        if show_progress:
            print(f"\n[START] Starting project sync (templates + reviews)")
            print(f"   é¡¹ç›®ID: {project_id}")
            print(f"   åŒ…å«å·¥ä½œæµæ¨¡æ¿: {sync_templates}")
            print(f"   è¯¦ç»†æ¨¡æ¿æ•°æ®: {fetch_detailed_template_data}")
            print("   æ³¨æ„: è´¦æˆ·æ•°æ®åŒæ­¥è¯·ä½¿ç”¨ç‹¬ç«‹çš„ account_sync.py")
            print("=" * 80)
        
        start_time = time.time()
        results = {}
        
        # è®¾ç½®ä¸´æ—¶headersä¾›APIè°ƒç”¨ä½¿ç”¨
        self._temp_headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
        
        try:
            # åˆ›å»ºå¼‚æ­¥HTTPä¼šè¯
            async with aiohttp.ClientSession() as session:
                
                # é˜¶æ®µ1: åŒæ­¥å·¥ä½œæµæ¨¡æ¿ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if sync_templates:
                    if show_progress:
                        print(f"\nTARGET: å¼€å§‹åŒæ­¥å·¥ä½œæµæ¨¡æ¿...")
                    
                    template_result = await self.sync_workflow_templates_enhanced(
                        session, project_id, access_token, 
                        fetch_detailed_template_data, show_progress
                    )
                    results['template_sync'] = template_result
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.sync_stats['templates_synced'] += template_result.get('templates_inserted', 0)
                    self.sync_stats['templates_updated'] += template_result.get('templates_updated', 0)
                
                # é˜¶æ®µ2: åŒæ­¥è¯„å®¡æ•°æ®
                if show_progress:
                    print(f"\nINFO: å¼€å§‹åŒæ­¥è¯„å®¡æ•°æ®...")
                
                # è·å–è¯„å®¡åˆ—è¡¨
                reviews = await self.async_fetch_all_reviews_with_pagination(
                    session, None, project_id, show_progress=show_progress
                )
                
                if reviews:
                    # å¼‚æ­¥å¹¶è¡ŒåŒæ­¥è¯„å®¡
                    review_result = await self.async_sync_reviews_parallel(
                        None, project_id, reviews, show_progress
                    )
                    results['review_sync'] = review_result
                else:
                    results['review_sync'] = {
                        'status': 'no_reviews',
                        'message': 'é¡¹ç›®ä¸­æ²¡æœ‰æ‰¾åˆ°è¯„å®¡æ•°æ®'
                    }
            
            total_time = time.time() - start_time
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            final_result = {
                'project_id': project_id,
                'sync_components': {
                    'workflow_templates': sync_templates,
                    'review_data': True,
                    'account_data': False  # è´¦æˆ·æ•°æ®åŒæ­¥å·²ç§»é™¤
                },
                'results': results,
                'sync_statistics': self.sync_stats,
                'performance_metrics': self.get_performance_report(),
                'execution_time': f"{total_time:.2f}ç§’",
                'sync_time': datetime.now(timezone.utc).isoformat()
            }
            
            if show_progress:
                print(f"\nâœ… å®Œæ•´é¡¹ç›®åŒæ­¥å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
                print(f"ğŸ“Š åŒæ­¥ç»Ÿè®¡:")
                if sync_templates:
                    print(f"   æ¨¡æ¿: {self.sync_stats['templates_synced']} æ–°å¢, {self.sync_stats['templates_updated']} æ›´æ–°")
                print(f"   å·¥ä½œæµ: {self.sync_stats['workflows_synced']} æ–°å¢, {self.sync_stats['workflows_updated']} æ›´æ–°")
                print(f"   è¯„å®¡: {self.sync_stats['reviews_synced']} æ–°å¢, {self.sync_stats['reviews_updated']} æ›´æ–°")
            
            return final_result
            
        except Exception as e:
            error_msg = f"é¡¹ç›®åŒæ­¥å¤±è´¥: {str(e)}"
            print(f"âœ— {error_msg}")
            return {
                'error': error_msg,
                'project_id': project_id,
                'results': results,
                'sync_statistics': self.sync_stats
            }
        
        finally:
            # æ¸…ç†ä¸´æ—¶headers
            if hasattr(self, '_temp_headers'):
                delattr(self, '_temp_headers')


    def _create_review_step_candidates_from_workflow_template(self, review_id: int, workflow_steps_config: List[Dict]) -> None:
        """
        åŸºäºå·¥ä½œæµæ¨¡æ¿ä¸ºè¯„å®¡åˆ›å»ºæ­¥éª¤å€™é€‰äººé…ç½®ï¼ˆæ­£ç¡®çš„é€»è¾‘ï¼‰
        
        Args:
            review_id: è¯„å®¡ID
            workflow_steps_config: å·¥ä½œæµæ¨¡æ¿çš„æ­¥éª¤é…ç½®
        """
        try:
            if not workflow_steps_config:
                print(f"  [CANDIDATES] No workflow template found for review {review_id}")
                return
            
            candidates_batch = []
            
            for step_order, step_config in enumerate(workflow_steps_config, 1):
                step_id = step_config.get('id')
                if not step_id:
                    print(f"  [WARNING] Step {step_order} has no ID, skipping")
                    continue
                
                step_name = step_config.get('name', 'Unknown Step')
                step_type = step_config.get('type', 'REVIEWER')
                step_candidates = step_config.get('candidates', {})
                
                # æ ‡å‡†åŒ–å€™é€‰äººæ•°æ®æ ¼å¼
                if isinstance(step_candidates, dict):
                    standardized_candidates = {
                        'users': step_candidates.get('users', []),
                        'roles': step_candidates.get('roles', []),
                        'companies': step_candidates.get('companies', [])
                    }
                else:
                    standardized_candidates = {'users': [], 'roles': [], 'companies': []}
                
                candidate_config = {
                    'review_id': review_id,
                    'step_id': step_id,
                    'step_name': step_name,
                    'step_type': step_type,
                    'step_order': step_order,
                    'candidates': standardized_candidates,
                    'source': 'workflow_template'  # æ¥æºæ˜¯å·¥ä½œæµæ¨¡æ¿
                }
                
                candidates_batch.append(candidate_config)
            
            # æ‰¹é‡æ’å…¥å€™é€‰äººé…ç½®
            if candidates_batch:
                self._batch_upsert_review_step_candidates(candidates_batch)
                print(f"  [CANDIDATES] Created {len(candidates_batch)} step candidate configurations from workflow template")
            else:
                print(f"  [CANDIDATES] No valid steps found in workflow template for review {review_id}")
                
        except Exception as e:
            print(f"[ERROR] Failed to create review step candidates from workflow template: {e}")

    def _create_review_step_candidates_from_progress(self, review_id: int, steps: List[Dict], workflow_steps_config: List[Dict]) -> None:
        """
        ä» review_progress åˆ›å»ºæ­¥éª¤å€™é€‰äººé…ç½®ï¼ˆä½¿ç”¨ ACC step_idï¼‰

        è¿™æ˜¯æ ¹æœ¬æ€§ä¿®å¤ï¼šä½¿ç”¨ ACC API è¿”å›çš„ step_id è€Œä¸æ˜¯ workflow template step_id
        è¿™æ · review_step_candidates.step_id ä¼šåŒ¹é… review_progress.step_id

        Args:
            review_id: è¯„å®¡ID
            steps: ACC API è¿”å›çš„æ­¥éª¤æ•°æ®ï¼ˆåŒ…å« ACC step_idï¼‰
            workflow_steps_config: å·¥ä½œæµæ­¥éª¤é…ç½®ï¼ˆç”¨äº candidates fallbackï¼‰
        """
        try:
            candidates_batch = []
            processed_steps = set()  # è·Ÿè¸ªå·²å¤„ç†çš„æ­¥éª¤ï¼Œé¿å…é‡å¤

            print(f"  [CANDIDATES] Creating step candidates from ACC progress data (review_id={review_id})")

            for step_data in steps:
                # å…³é”®ï¼šä½¿ç”¨ ACC API çš„ step_idï¼Œä¸æ˜¯ workflow template çš„ step_id
                step_id = step_data.get('stepId') or step_data.get('id')
                if not step_id:
                    print(f"  [WARNING] Step has no step_id, skipping")
                    continue

                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡è¿™ä¸ªæ­¥éª¤ï¼ˆé¿å…é‡å¤ï¼Œå› ä¸º send-back å¯èƒ½äº§ç”Ÿé‡å¤ step_idï¼‰
                step_key = (review_id, step_id)
                if step_key in processed_steps:
                    print(f"  [SKIP] Step {step_id} already processed")
                    continue
                processed_steps.add(step_key)

                # å€™é€‰äººä¼˜å…ˆçº§ï¼š
                # 1. nextActionBy.candidates (å½“å‰æ´»è·ƒæ­¥éª¤)
                # 2. step.candidates (æ­¥éª¤æœ¬èº«çš„å€™é€‰äºº)
                # 3. workflow_steps_config (ä» workflow template fallback)
                candidates = {}
                source = 'acc_sync'

                # ä¼˜å…ˆçº§1: nextActionBy (å½“å‰æ´»è·ƒæ­¥éª¤çš„å€™é€‰äºº)
                if step_data.get('status') in ['PENDING', 'CLAIMED', 'pending', 'claimed']:
                    next_action_by = step_data.get('nextActionBy', {})
                    if next_action_by and next_action_by.get('candidates'):
                        candidates = next_action_by.get('candidates', {})
                        source = 'acc_sync'
                        print(f"  [CANDIDATES] Step {step_id}: Using nextActionBy candidates")

                # ä¼˜å…ˆçº§2: æ­¥éª¤æœ¬èº«çš„ candidates
                if not candidates or (isinstance(candidates, dict) and not any(candidates.values())):
                    candidates = step_data.get('candidates', {})
                    if candidates and any(candidates.values()):
                        source = 'acc_sync'
                        print(f"  [CANDIDATES] Step {step_id}: Using step.candidates")

                # ä¼˜å…ˆçº§3: ä» workflow é…ç½® fallbackï¼ˆé€šè¿‡ step_order åŒ¹é…ï¼‰
                if not candidates or (isinstance(candidates, dict) and not any(candidates.values())):
                    step_order = step_data.get('stepOrder') or step_data.get('order', 0)
                    # é€šè¿‡ step_order åœ¨ workflow template ä¸­æŸ¥æ‰¾å€™é€‰äºº
                    if workflow_steps_config and step_order > 0 and step_order <= len(workflow_steps_config):
                        template_step = workflow_steps_config[step_order - 1]
                        template_candidates = template_step.get('candidates', {})
                        if template_candidates and any(template_candidates.values()):
                            candidates = template_candidates
                            source = 'workflow_template'
                            print(f"  [CANDIDATES] Step {step_id}: Fallback to workflow template candidates")

                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å€™é€‰äººï¼Œè®¾ç½®ç©ºç»“æ„
                if not candidates or (isinstance(candidates, dict) and not any(candidates.values())):
                    candidates = {'users': [], 'roles': [], 'companies': []}
                    source = 'acc_sync'
                    print(f"  [CANDIDATES] Step {step_id}: No candidates found, using empty structure")

                # æ ‡å‡†åŒ–å€™é€‰äººæ•°æ®æ ¼å¼
                if isinstance(candidates, dict):
                    standardized_candidates = {
                        'users': candidates.get('users', []),
                        'roles': candidates.get('roles', []),
                        'companies': candidates.get('companies', [])
                    }
                else:
                    standardized_candidates = {'users': [], 'roles': [], 'companies': []}

                # æ„å»ºå€™é€‰äººé…ç½®
                candidate_config = {
                    'review_id': review_id,
                    'step_id': step_id,  # âœ… ä½¿ç”¨ ACC step_idï¼ˆå…³é”®ä¿®å¤ï¼‰
                    'step_name': step_data.get('stepName') or step_data.get('name', 'Unknown Step'),
                    'step_type': step_data.get('stepType') or step_data.get('type', 'REVIEWER'),
                    'step_order': step_data.get('stepOrder') or step_data.get('order', 0),
                    'candidates': standardized_candidates,
                    'source': source
                }

                candidates_batch.append(candidate_config)

            # æ‰¹é‡ UPSERT å€™é€‰äººé…ç½®
            if candidates_batch:
                self._batch_upsert_review_step_candidates(candidates_batch)
                print(f"  [CANDIDATES] âœ“ Created {len(candidates_batch)} step candidate configurations using ACC step_ids")
            else:
                print(f"  [CANDIDATES] No valid steps to create candidate configurations")

        except Exception as e:
            print(f"âœ— [ERROR] Failed to create review step candidates from progress: {e}")
            import traceback
            traceback.print_exc()

    def _create_candidates_from_template(self, review_id: int, workflow_steps_config: List[Dict]) -> None:
        """
        å¾ workflow template å‰µå»ºå®Œæ•´çš„æ­¥é©Ÿå€™é¸äººé…ç½®

        é€™æ˜¯æ–¹æ¡ˆ2çš„æ ¸å¿ƒæ–¹æ³•ï¼š
        - ç‚ºæ‰€æœ‰ template æ­¥é©Ÿå‰µå»º candidatesï¼ˆå³ä½¿æœªåŸ·è¡Œï¼‰
        - ä½¿ç”¨ template step_idï¼ˆä¸æ˜¯ ACC step_idï¼‰
        - ç¢ºä¿é…ç½®å®Œæ•´æ€§

        Args:
            review_id: è©•å¯©ID
            workflow_steps_config: å·¥ä½œæµæ­¥é©Ÿé…ç½®ï¼ˆå®Œæ•´çš„æ¨¡æ¿æ­¥é©Ÿï¼‰
        """
        try:
            if not workflow_steps_config:
                print(f"  [CANDIDATES] No workflow steps config provided, skipping candidate creation")
                return

            candidates_batch = []

            print(f"  [CANDIDATES] Creating candidates from workflow template (review_id={review_id}, {len(workflow_steps_config)} steps)")

            for idx, template_step in enumerate(workflow_steps_config):
                # âœ… ä½¿ç”¨ template step_idï¼ˆé—œéµï¼ï¼‰
                template_step_id = template_step.get('id')
                if not template_step_id:
                    print(f"  [WARNING] Template step missing 'id' field, skipping")
                    continue

                # ç²å–æ­¥é©Ÿé…ç½®
                step_name = template_step.get('name', 'Unknown Step')
                step_type = template_step.get('type', 'REVIEWER')
                # âœ… ä½¿ç”¨ç´¢å¼• + 1 ä½œç‚º step_orderï¼ˆå› ç‚º order å¾ 1 é–‹å§‹ï¼‰
                step_order = template_step.get('order', idx + 1)

                # ç²å– candidates é…ç½®
                candidates = template_step.get('candidates', {})
                if isinstance(candidates, dict):
                    standardized_candidates = {
                        'users': candidates.get('users', []),
                        'roles': candidates.get('roles', []),
                        'companies': candidates.get('companies', [])
                    }
                else:
                    standardized_candidates = {'users': [], 'roles': [], 'companies': []}

                candidate_config = {
                    'review_id': review_id,
                    'step_id': template_step_id,  # âœ… ä½¿ç”¨ template step_id
                    'step_name': step_name,
                    'step_type': step_type,
                    'step_order': step_order,
                    'candidates': standardized_candidates,
                    'source': 'workflow_template'
                }

                candidates_batch.append(candidate_config)
                print(f"  [CANDIDATES]   Step {step_order}: {step_name} (template_id={template_step_id})")

            # æ‰¹é‡ UPSERT å€™é¸äººé…ç½®
            if candidates_batch:
                self._batch_upsert_review_step_candidates(candidates_batch)
                print(f"  [CANDIDATES] âœ“ Created {len(candidates_batch)} complete step candidate configurations from template")
            else:
                print(f"  [CANDIDATES] No valid steps to create candidate configurations")

        except Exception as e:
            print(f"âœ— [ERROR] Failed to create candidates from template: {e}")
            import traceback
            traceback.print_exc()

    def _sync_progress_with_template_mapping(self, review_id: int, steps: List[Dict], workflow_steps_config: List[Dict]) -> None:
        """
        åŒæ­¥ progressï¼Œä¸¦è‡ªå‹•é—œè¯åˆ° template step_id

        é€™æ˜¯æ–¹æ¡ˆ2çš„æ ¸å¿ƒæ–¹æ³•ï¼š
        - åŒæ­¥ ACC API çš„åŸ·è¡Œè¨˜éŒ„
        - é€šé step_order è‡ªå‹•é—œè¯åˆ° template_step_id
        - ä¿ç•™ ACC step_id ç”¨æ–¼æ­·å²è¿½è¹¤

        Args:
            review_id: è©•å¯©ID
            steps: ACC API è¿”å›çš„æ­¥é©ŸåŸ·è¡Œæ•¸æ“š
            workflow_steps_config: å·¥ä½œæµæ­¥é©Ÿé…ç½®ï¼ˆç”¨æ–¼æŸ¥æ‰¾ template_step_idï¼‰
        """
        try:
            batch_data = []

            print(f"  [PROGRESS] Syncing progress with template mapping (review_id={review_id}, {len(steps)} progress records)")

            for idx, step_data in enumerate(steps):
                # ACC step_idï¼ˆåŸ·è¡Œæ™‚çš„IDï¼‰
                acc_step_id = step_data.get('stepId') or step_data.get('id')
                if not acc_step_id:
                    print(f"  [WARNING] Progress step missing 'stepId', skipping")
                    continue

                # æ­¥é©Ÿé †åºï¼ˆç”¨æ–¼åŒ¹é… templateï¼‰
                # âœ… ä½¿ç”¨ API çš„ stepOrderï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ä½¿ç”¨ç´¢å¼• + 1
                step_order = step_data.get('stepOrder') or step_data.get('order') or (idx + 1)

                # âœ… é€šé step_order æŸ¥æ‰¾å°æ‡‰çš„ template step_id
                template_step_id = None
                if workflow_steps_config and 0 < step_order <= len(workflow_steps_config):
                    template_step = workflow_steps_config[step_order - 1]  # order å¾ 1 é–‹å§‹
                    template_step_id = template_step.get('id')
                    print(f"  [PROGRESS]   Step {step_order}: ACC_id={acc_step_id} â†’ template_id={template_step_id}")
                else:
                    print(f"  [WARNING] Cannot find template step for order {step_order} (total: {len(workflow_steps_config)})")

                # å¾ ACC API ç²å– candidatesï¼ˆç”¨æ–¼æ­·å²è¨˜éŒ„ï¼‰
                candidates = step_data.get('candidates', {})
                if not candidates or (isinstance(candidates, dict) and not any(candidates.values())):
                    # å¾å·¥ä½œæµé…ç½®è£œå…… candidates
                    if template_step_id and workflow_steps_config:
                        workflow_candidates = self._get_candidates_from_workflow_config(
                            workflow_steps_config, template_step_id
                        )
                        if workflow_candidates:
                            candidates = workflow_candidates

                progress_data = {
                    'review_id': review_id,
                    'step_id': acc_step_id,  # ACC step_idï¼ˆä¿ç•™ç”¨æ–¼æ­·å²è¨˜éŒ„ï¼‰
                    'template_step_id': template_step_id,  # âœ… æ–°å¢ï¼šé—œè¯åˆ° template
                    'step_name': step_data.get('stepName') or step_data.get('name'),
                    'step_type': self._map_step_type(step_data.get('type', 'reviewer')),
                    'step_order': step_order,
                    'status': self._map_step_status(step_data.get('status', 'pending')),
                    'assigned_to': step_data.get('assignedTo', []),
                    'claimed_by': step_data.get('claimedBy', {}),
                    'completed_by': step_data.get('completedBy', {}),
                    'action_by': step_data.get('actionBy', {}),
                    'candidates': candidates,
                    'decision': step_data.get('decision'),
                    'comments': step_data.get('comments'),
                    'notes': step_data.get('notes'),
                    'due_date': self._parse_timestamp(step_data.get('dueDate')),
                    'started_at': self._parse_timestamp(step_data.get('startedAt')),
                    'completed_at': self._parse_timestamp(step_data.get('completedAt')),
                    'end_time': self._parse_timestamp(step_data.get('endTime'))
                }

                batch_data.append(progress_data)

            # æ‰¹é‡UPSERT
            if batch_data:
                try:
                    # æª¢æŸ¥æ˜¯å¦æœ‰å¢å¼·ç‰ˆçš„ batch_upsert_review_steps æ–¹æ³•
                    if hasattr(self.da, 'batch_upsert_review_steps'):
                        inserted, updated = self.da.batch_upsert_review_steps(batch_data)
                        print(f"  [PROGRESS] âœ“ Batch UPSERT: {inserted} inserted, {updated} updated")
                    else:
                        # å›é€€åˆ°æ™®é€šæ’å…¥
                        inserted_count = self.da.batch_insert_review_steps(batch_data)
                        print(f"  [PROGRESS] âœ“ Batch INSERT: {inserted_count} records")
                except Exception as e:
                    print(f"âœ— [ERROR] Failed to batch upsert progress: {e}")
                    raise

            # âœ… æ–°å¢ï¼šç‚ºç•¶å‰æ´»èºæ­¥é©Ÿå‰µå»º progress è¨˜éŒ„
            # ACC API åªè¿”å›å·²å®Œæˆçš„æ­¥é©Ÿï¼Œéœ€è¦ç‚ºç•¶å‰å¾…è™•ç†æ­¥é©Ÿå‰µå»ºè¨˜éŒ„
            self._create_current_step_progress(review_id, steps, workflow_steps_config)

        except Exception as e:
            print(f"âœ— [ERROR] Failed to sync progress with template mapping: {e}")
            import traceback
            traceback.print_exc()

    def _create_current_step_progress(self, review_id: int, executed_steps: List[Dict], workflow_steps_config: List[Dict]) -> None:
        """
        ç‚ºç•¶å‰æ´»èºæ­¥é©Ÿå‰µå»º progress è¨˜éŒ„

        ACC API åªè¿”å›å·²å®Œæˆçš„æ­¥é©Ÿï¼Œä¸è¿”å›ç•¶å‰å¾…è™•ç†æ­¥é©Ÿã€‚
        é€™å€‹æ–¹æ³•æª¢æ¸¬ä¸¦å‰µå»ºç•¶å‰æ­¥é©Ÿçš„ progress è¨˜éŒ„ã€‚

        é‚è¼¯ï¼š
        1. æ‰¾å‡ºå·²åŸ·è¡Œæ­¥é©Ÿçš„æœ€å¤§ step_order
        2. ç•¶å‰æ­¥é©Ÿæ‡‰è©²æ˜¯ max_order + 1
        3. å¦‚æœè©²æ­¥é©Ÿåœ¨ workflow template ä¸­å­˜åœ¨ï¼Œä¸”æ²’æœ‰ progress è¨˜éŒ„ï¼Œå‰‡å‰µå»º

        Args:
            review_id: è©•å¯©ID
            executed_steps: ACC API è¿”å›çš„å·²åŸ·è¡Œæ­¥é©Ÿ
            workflow_steps_config: å·¥ä½œæµæ­¥é©Ÿé…ç½®
        """
        try:
            # âœ… è¨ºæ–·æ—¥èªŒï¼šç¢ºèªæ–¹æ³•è¢«èª¿ç”¨
            print(f"  [CURRENT_STEP] Processing review_id={review_id}, executed_steps={len(executed_steps)}, workflow_steps={len(workflow_steps_config) if workflow_steps_config else 0}")

            if not workflow_steps_config:
                print(f"  [CURRENT_STEP] Skipped - no workflow config for review {review_id}")
                return

            # 1. æ‰¾å‡ºå·²åŸ·è¡Œæ­¥é©Ÿçš„æœ€å¤§ step_order
            # âœ… å¾æ•¸æ“šåº«æŸ¥è©¢å·²æ’å…¥çš„ review_progressï¼Œæ‰¾å‡ºå¯¦éš›çš„æœ€å¤§ step_order
            # é€™æ¯”ä¾è³´ API è¿”å›çš„æ•¸æ“šæ›´å¯é 
            max_executed_order = 0
            try:
                with self.da.get_cursor() as cursor:
                    cursor.execute("""
                        SELECT COALESCE(MAX(step_order), 0) as max_order
                        FROM review_progress
                        WHERE review_id = %s
                    """, (review_id,))
                    result = cursor.fetchone()
                    if result:
                        max_executed_order = result['max_order'] if isinstance(result, dict) else result[0]
            except Exception as e:
                print(f"  [CURRENT_STEP] Error querying max step_order: {e}")
                # Fallback: å¾ API æ•¸æ“šè¨ˆç®—
                for step_data in executed_steps:
                    step_order = step_data.get('stepOrder') or step_data.get('order', 0)
                    if step_order > max_executed_order:
                        max_executed_order = step_order

            # 2. ç•¶å‰æ´»èºæ­¥é©Ÿæ‡‰è©²æ˜¯ä¸‹ä¸€å€‹
            current_step_order = max_executed_order + 1

            # âœ… è¨ºæ–·æ—¥èªŒï¼šé¡¯ç¤ºè¨ˆç®—çµæœ
            print(f"  [CURRENT_STEP] max_executed_order={max_executed_order}, current_step_order={current_step_order}")

            # 3. æª¢æŸ¥è©²æ­¥é©Ÿæ˜¯å¦åœ¨ workflow ä¸­å­˜åœ¨
            if current_step_order > len(workflow_steps_config):
                # å·²ç¶“å®Œæˆæ‰€æœ‰æ­¥é©Ÿ
                print(f"  [CURRENT_STEP] All steps completed (max_order={max_executed_order}, total={len(workflow_steps_config)})")
                return

            # 4. ç²å–è©²æ­¥é©Ÿçš„ template é…ç½®
            template_step = workflow_steps_config[current_step_order - 1]
            template_step_id = template_step.get('id')

            if not template_step_id:
                print(f"  [WARNING] Cannot find template step_id for order {current_step_order}")
                return

            # 5. æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰è©²æ­¥é©Ÿçš„ progress è¨˜éŒ„
            existing_progress = self.da.get_review_progress_by_template_step(review_id, template_step_id)
            if existing_progress:
                print(f"  [CURRENT_STEP] Progress already exists for step {current_step_order} (template_id={template_step_id})")
                return

            # 6. å‰µå»ºç•¶å‰æ­¥é©Ÿçš„ progress è¨˜éŒ„
            print(f"  [CURRENT_STEP] Creating progress for current step {current_step_order}: {template_step.get('name')} (template_id={template_step_id})")

            # å¾ template ç²å– candidates
            candidates = template_step.get('candidates', {})
            standardized_candidates = {
                'users': candidates.get('users', []) if isinstance(candidates.get('users'), list) else [],
                'roles': candidates.get('roles', []) if isinstance(candidates.get('roles'), list) else [],
                'companies': candidates.get('companies', []) if isinstance(candidates.get('companies'), list) else []
            }

            progress_data = {
                'review_id': review_id,
                'step_id': template_step_id,  # ä½¿ç”¨ template step_idï¼ˆå› ç‚º ACC æ²’æœ‰çµ¦ runtime IDï¼‰
                'template_step_id': template_step_id,
                'step_name': template_step.get('name', 'Unknown Step'),
                'step_type': self._map_step_type(template_step.get('type', 'REVIEWER')),
                'step_order': current_step_order,
                'status': 'PENDING',  # ç•¶å‰æ­¥é©Ÿç‹€æ…‹ç‚º PENDING
                'assigned_to': [],
                'claimed_by': {},
                'completed_by': {},
                'action_by': {},
                'candidates': standardized_candidates,
                'decision': None,
                'comments': None,
                'notes': '',
                'due_date': None,
                'started_at': None,
                'completed_at': None,
                'end_time': None
            }

            # æ’å…¥è¨˜éŒ„
            try:
                if hasattr(self.da, 'batch_upsert_review_steps'):
                    inserted, updated = self.da.batch_upsert_review_steps([progress_data])
                    print(f"  [CURRENT_STEP] âœ“ Created current step progress (inserted={inserted}, updated={updated})")
                else:
                    inserted_count = self.da.batch_insert_review_steps([progress_data])
                    print(f"  [CURRENT_STEP] âœ“ Created current step progress (inserted={inserted_count})")
            except Exception as e:
                print(f"âœ— [ERROR] Failed to create current step progress: {e}")
                raise

        except Exception as e:
            print(f"âœ— [ERROR] Failed in _create_current_step_progress: {e}")
            import traceback
            traceback.print_exc()

    def _create_review_step_candidates_from_cache(self, review_id: int, steps: List[Dict], workflow_steps_config: List[Dict]) -> None:
        """
        ä»ç¼“å­˜çš„å·¥ä½œæµæ•°æ®ä¸ºè¯„å®¡åˆ›å»ºæ­¥éª¤å€™é€‰äººé…ç½®
        
        Args:
            review_id: è¯„å®¡ID
            steps: ACC API è¿”å›çš„æ­¥éª¤æ•°æ®
            workflow_steps_config: ç¼“å­˜çš„å·¥ä½œæµæ­¥éª¤é…ç½®
        """
        try:
            candidates_batch = []
            processed_steps = set()  # è·Ÿè¸ªå·²å¤„ç†çš„æ­¥éª¤ï¼Œé¿å…é‡å¤
            
            for step_data in steps:
                step_id = step_data.get('stepId') or step_data.get('id')
                if not step_id:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡è¿™ä¸ªæ­¥éª¤ï¼ˆé¿å…é‡å¤ï¼‰
                step_key = (review_id, step_id)
                if step_key in processed_steps:
                    continue
                processed_steps.add(step_key)
                
                # ä¼˜å…ˆçº§1: nextActionBy (å½“å‰æ´»è·ƒæ­¥éª¤)
                candidates = {}
                source = 'acc_sync'  # é»˜è®¤ä¸º acc_syncï¼Œç¬¦åˆçº¦æŸ
                
                if step_data.get('status') in ['PENDING', 'CLAIMED']:
                    # è¿™æ˜¯å½“å‰æ´»è·ƒæ­¥éª¤ï¼Œå°è¯•ä» nextActionBy è·å–
                    next_action_by = step_data.get('nextActionBy', {})
                    if next_action_by and next_action_by.get('candidates'):
                        candidates = next_action_by.get('candidates', {})
                        source = 'acc_sync'  # nextActionBy æ¥æºä¹Ÿæ ‡è®°ä¸º acc_sync
                    else:
                        # ä¼˜å…ˆçº§2: æ­¥éª¤æœ¬èº«çš„ candidates
                        candidates = step_data.get('candidates', {})
                        source = 'acc_sync' if candidates else 'acc_sync'
                else:
                    # ä¼˜å…ˆçº§2: æ­¥éª¤æœ¬èº«çš„ candidates
                    candidates = step_data.get('candidates', {})
                    source = 'acc_sync' if candidates else 'acc_sync'
                
                # ä¼˜å…ˆçº§3: ä»å·¥ä½œæµé…ç½®è·å–
                if not candidates or (isinstance(candidates, dict) and not any(candidates.values())):
                    workflow_candidates = self._get_candidates_from_workflow_config(
                        workflow_steps_config, step_id
                    )
                    if workflow_candidates:
                        candidates = workflow_candidates
                        source = 'workflow_template'  # ä»å·¥ä½œæµé…ç½®è·å–çš„æ ‡è®°ä¸º workflow_template
                    else:
                        # è®¾ç½®ç©ºçš„å€™é€‰äººç»“æ„
                        candidates = {'users': [], 'roles': [], 'companies': []}
                        source = 'acc_sync'  # ç©ºçš„ä¹Ÿæ ‡è®°ä¸º acc_sync
                
                # æ ‡å‡†åŒ–å€™é€‰äººæ•°æ®æ ¼å¼
                if isinstance(candidates, dict):
                    standardized_candidates = {
                        'users': candidates.get('users', []),
                        'roles': candidates.get('roles', []),
                        'companies': candidates.get('companies', [])
                    }
                else:
                    standardized_candidates = {'users': [], 'roles': [], 'companies': []}
                
                candidate_config = {
                    'review_id': review_id,
                    'step_id': step_id,
                    'step_name': step_data.get('stepName') or step_data.get('name'),
                    'step_type': step_data.get('stepType') or step_data.get('type'),
                    'step_order': step_data.get('stepOrder') or step_data.get('order', 0),
                    'candidates': standardized_candidates,
                    'source': source
                }
                
                candidates_batch.append(candidate_config)
            
            # æ‰¹é‡æ’å…¥å€™é€‰äººé…ç½®
            if candidates_batch:
                self._batch_upsert_review_step_candidates(candidates_batch)
                print(f"  [CANDIDATES] Created {len(candidates_batch)} step candidate configurations")
                
        except Exception as e:
            print(f"[ERROR] Failed to create review step candidates: {e}")
    
    def _batch_upsert_review_step_candidates(self, candidates_data: List[Dict]) -> None:
        """
        æ‰¹é‡UPSERTè¯„å®¡æ­¥éª¤å€™é€‰äººé…ç½®
        
        Args:
            candidates_data: å€™é€‰äººé…ç½®æ•°æ®åˆ—è¡¨
        """
        if not candidates_data:
            return
        
        try:
            with self.da.get_connection() as conn:
                with conn.cursor() as cursor:
                    # æ„å»ºæ‰¹é‡UPSERT SQL
                    upsert_sql = """
                        INSERT INTO review_step_candidates (
                            review_id, step_id, step_name, step_type, step_order,
                            candidates, source
                        ) VALUES %s
                        ON CONFLICT (review_id, step_id) 
                        DO UPDATE SET
                            step_name = EXCLUDED.step_name,
                            step_type = EXCLUDED.step_type,
                            step_order = EXCLUDED.step_order,
                            candidates = EXCLUDED.candidates,
                            source = EXCLUDED.source,
                            updated_at = CURRENT_TIMESTAMP
                    """
                    
                    # å‡†å¤‡æ•°æ®
                    values_list = []
                    for config in candidates_data:
                        values_list.append((
                            config['review_id'],
                            config['step_id'],
                            config['step_name'],
                            config['step_type'],
                            config['step_order'],
                            psycopg2.extras.Json(config['candidates']),
                            config['source']
                        ))
                    
                    # æ‰§è¡Œæ‰¹é‡UPSERT
                    psycopg2.extras.execute_values(
                        cursor, upsert_sql, values_list,
                        template=None, page_size=100
                    )
                    
                    conn.commit()
                    
        except Exception as e:
            print(f"[ERROR] Failed to batch upsert review step candidates: {e}")
            raise


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def get_enhanced_sync_manager(
    data_access: Optional[ReviewDataAccess] = None,
    **kwargs
) -> EnhancedReviewSyncManager:
    """è·å–å¢å¼ºçš„åŒæ­¥ç®¡ç†å™¨å®ä¾‹"""
    return EnhancedReviewSyncManager(data_access, **kwargs)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("å¢å¼ºçš„å®¡æ‰¹ç³»ç»ŸåŒæ­¥ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 60)
    
    try:
        sync_manager = get_enhanced_sync_manager(
            max_concurrent=10,
            enable_cache=True,
            cache_ttl=3600,
            cache_max_size=5000,
            batch_size=100
        )
        print("SUCCESS: å¢å¼ºåŒæ­¥ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        cache_stats = sync_manager.cache.get_stats()
        print(f"\nç¼“å­˜ç»Ÿè®¡:")
        print(json.dumps(cache_stats, indent=2, ensure_ascii=False))
        
        # æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
        report = sync_manager.get_performance_report()
        print(f"\nå½“å‰æ€§èƒ½æŒ‡æ ‡:")
        print(json.dumps(report, indent=2, ensure_ascii=False, default=str))
        
    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

