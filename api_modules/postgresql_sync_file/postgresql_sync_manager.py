# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„PostgreSQLåŒæ­¥ç®¡ç†å™¨
åŸºäºäº”å±‚ä¼˜åŒ–ç­–ç•¥ï¼Œæ”¯æŒæ™ºèƒ½è·³è¿‡ã€æ‰¹é‡æ“ä½œã€å¹¶å‘å¤„ç†
"""

import asyncio
import time
import uuid
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import gc
import pytz

from database_sql.optimized_data_access import get_optimized_postgresql_dal
from database.data_sync_strategy import DataTransformer

logger = logging.getLogger(__name__)

# ä¸­å›½æ—¶åŒºå¸¸é‡
CHINA_TZ = pytz.timezone('Asia/Shanghai')

class OptimizedPostgreSQLSyncManager:
    """ä¼˜åŒ–çš„PostgreSQLåŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(self, batch_size: int = 100, api_delay: float = 0.02, max_workers: int = 8, memory_threshold_mb: int = 1024):
        self.batch_size = batch_size
        self.api_delay = api_delay
        self.max_workers = max_workers
        self.memory_threshold_mb = memory_threshold_mb
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'api_calls': 0,
            'api_calls_saved': 0,
            'smart_skips': 0,
            'batch_operations': 0,
            'concurrent_operations': 0,
            'memory_peak_mb': 0,
            'processing_time': 0
        }
        
        # æ•°æ®è½¬æ¢å™¨
        self.converter = DataTransformer()
        
        # å†…å­˜ç®¡ç† (å·²åœ¨__init__ä¸­è¨­ç½®)
        
        # å¹¶å‘æ§åˆ¶
        self.api_semaphore = asyncio.Semaphore(8)
        self.db_semaphore = asyncio.Semaphore(15)
        
        # ç»Ÿä¸€ä½¿ç”¨V2æ¶æ„
        pass
    
    # ============================================================================
    # ğŸŒ æ—¶åŒºè½¬æ¢å·¥å…·å‡½æ•°
    # ============================================================================
    
    def _convert_to_china_timezone(self, dt: datetime) -> datetime:
        """
        å°†datetimeè½¬æ¢ä¸ºä¸­å›½æ—¶åŒº
        
        Args:
            dt: è¾“å…¥çš„datetimeå¯¹è±¡
            
        Returns:
            è½¬æ¢ä¸ºä¸­å›½æ—¶åŒºçš„datetimeå¯¹è±¡
        """
        if not dt:
            return dt
            
        try:
            # å¦‚æœæ˜¯naive datetimeï¼Œå‡è®¾ä¸ºUTC
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            
            # è½¬æ¢ä¸ºä¸­å›½æ—¶åŒº
            china_dt = dt.astimezone(CHINA_TZ)
            return china_dt
            
        except Exception as e:
            logger.warning(f"æ—¶åŒºè½¬æ¢å¤±è´¥: {dt}, é”™è¯¯: {e}")
            return dt
    
    def _parse_datetime_to_china(self, datetime_str) -> Optional[datetime]:
        """
        è§£ædatetimeå­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºä¸­å›½æ—¶åŒº
        
        Args:
            datetime_str: æ—¶é—´å­—ç¬¦ä¸²æˆ–datetimeå¯¹è±¡
            
        Returns:
            è½¬æ¢ä¸ºä¸­å›½æ—¶åŒºçš„datetimeå¯¹è±¡
        """
        # å…ˆä½¿ç”¨åŸæœ‰çš„è§£ææ–¹æ³•
        parsed_dt = self._parse_datetime(datetime_str)
        
        if parsed_dt:
            # è½¬æ¢ä¸ºä¸­å›½æ—¶åŒº
            return self._convert_to_china_timezone(parsed_dt)
        
        return None
    
    def _batch_convert_timestamps_to_china(self, data_dict: Dict[str, Any], 
                                         timestamp_fields: List[str]) -> Dict[str, Any]:
        """
        æ‰¹é‡è½¬æ¢æ•°æ®å­—å…¸ä¸­çš„æ—¶é—´æˆ³å­—æ®µä¸ºä¸­å›½æ—¶åŒº
        
        Args:
            data_dict: åŒ…å«æ—¶é—´æˆ³çš„æ•°æ®å­—å…¸
            timestamp_fields: éœ€è¦è½¬æ¢çš„æ—¶é—´æˆ³å­—æ®µåˆ—è¡¨
            
        Returns:
            è½¬æ¢åçš„æ•°æ®å­—å…¸
        """
        converted_dict = data_dict.copy()
        
        for field in timestamp_fields:
            if field in converted_dict and converted_dict[field]:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆè§£æå†è½¬æ¢
                if isinstance(converted_dict[field], str):
                    converted_dict[field] = self._parse_datetime_to_china(converted_dict[field])
                # å¦‚æœæ˜¯datetimeå¯¹è±¡ï¼Œç›´æ¥è½¬æ¢
                elif isinstance(converted_dict[field], datetime):
                    converted_dict[field] = self._convert_to_china_timezone(converted_dict[field])
        
        return converted_dict
        
    # ============================================================================
    # ğŸš€ Layer 1: æ™ºèƒ½åˆ†æ”¯è·³è¿‡ä¼˜åŒ–
    # ============================================================================
    
    async def _smart_branch_filtering(self, project_id: str, last_sync_time: datetime, 
                                    headers: dict) -> List[Dict[str, Any]]:
        """æ™ºèƒ½åˆ†æ”¯è¿‡æ»¤ - æ ¸å¿ƒä¼˜åŒ–ï¼ŒåŒ…å«é¡¶å±‚rollupæ£€æŸ¥"""
        
        logger.info("ğŸ” å¼€å§‹æ™ºèƒ½åˆ†æ”¯è¿‡æ»¤...")
        start_time = time.time()
        
        try:
            dal = await get_optimized_postgresql_dal()
            
            # ğŸš€ CRITICAL OPTIMIZATION: é¡¶å±‚rollupæ—¶é—´æ£€æŸ¥
            # è¿™æ˜¯æœ€é‡è¦çš„ä¼˜åŒ– - å¯ä»¥è·³è¿‡æ•´ä¸ªé¡¹ç›®
            top_level_check = await self._check_project_top_level_rollup(project_id, last_sync_time, dal)
            
            if top_level_check.get('can_skip_entire_project'):
                logger.info("ğŸš€ TOP-LEVEL ROLLUP OPTIMIZATION: Entire project can be skipped!")
                logger.info(f"   Max rollup time: {top_level_check.get('max_rollup_time')}")
                logger.info(f"   Last sync time: {top_level_check.get('last_sync_time')}")
                logger.info(f"   Skip efficiency: {top_level_check.get('skip_efficiency_percentage', 0):.1f}%")
                
                self.stats['smart_skips'] += top_level_check.get('total_top_level_folders', 1)
                self.stats['api_calls_saved'] += top_level_check.get('total_top_level_folders', 1) * 10  # ä¼°ç®—èŠ‚çœçš„APIè°ƒç”¨
                self.stats['top_level_rollup_skips'] = 1
                
                return []  # æ•´ä¸ªé¡¹ç›®éƒ½å¯ä»¥è·³è¿‡
            
            logger.info(f"ğŸ” Top-level check: {top_level_check.get('folders_with_changes', 0)} folders need checking")
            
            # ğŸ”‘ ç¬¬äºŒæ­¥ï¼šè·å–å¯èƒ½æœ‰å˜åŒ–çš„æ–‡ä»¶å¤¹ï¼ˆç°åœ¨åªæ£€æŸ¥æœ‰å˜åŒ–çš„é¡¶å±‚æ–‡ä»¶å¤¹ï¼‰
            changed_folders = await dal.get_folders_for_smart_skip_check(project_id, last_sync_time)
            
            if not changed_folders:
                logger.info("âœ… æ™ºèƒ½è·³è¿‡ï¼šæ²¡æœ‰å‘ç°å˜åŒ–çš„æ–‡ä»¶å¤¹")
                self.stats['smart_skips'] += 1
                return []
            
            # ğŸ”‘ ç¬¬ä¸‰æ­¥ï¼šé€’å½’æ£€æŸ¥æ–‡ä»¶å¤¹æ ‘ï¼Œåº”ç”¨rollupæ—¶é—´ä¼˜åŒ–
            filtered_items = []
            
            for folder in changed_folders:
                # æ£€æŸ¥rollupæ—¶é—´
                rollup_time = self._parse_datetime(folder.get('last_modified_time_rollup'))
                
                if rollup_time and rollup_time <= last_sync_time:
                    # ğŸš€ æ™ºèƒ½è·³è¿‡ï¼šæ•´ä¸ªåˆ†æ”¯æ— å˜åŒ–
                    logger.debug(f"æ™ºèƒ½è·³è¿‡åˆ†æ”¯: {folder.get('name')} (rollup: {rollup_time} <= sync: {last_sync_time})")
                    self.stats['smart_skips'] += 1
                    self.stats['api_calls_saved'] += folder.get('object_count', 1) * 2  # ä¼°ç®—èŠ‚çœçš„APIè°ƒç”¨
                    continue
                
                # éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥çš„æ–‡ä»¶å¤¹
                filtered_items.append(folder)
                logger.debug(f"åˆ†æ”¯æœ‰å˜åŒ–ï¼Œç»§ç»­æ£€æŸ¥: {folder.get('name')} (rollup: {rollup_time} > sync: {last_sync_time})")
            
            processing_time = time.time() - start_time
            self.stats['processing_time'] += processing_time
            
            skip_efficiency = (self.stats['smart_skips'] / max(len(changed_folders), 1)) * 100
            logger.info(f"ğŸ¯ æ™ºèƒ½åˆ†æ”¯è¿‡æ»¤å®Œæˆ: {len(filtered_items)}/{len(changed_folders)} éœ€è¦å¤„ç†, è·³è¿‡æ•ˆç‡: {skip_efficiency:.1f}%")
            
            return filtered_items
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åˆ†æ”¯è¿‡æ»¤å¤±è´¥: {e}")
            return []
    
    async def _check_project_top_level_rollup(self, project_id: str, last_sync_time: datetime, 
                                         dal) -> Dict[str, Any]:
        """
        ğŸš€ CRITICAL OPTIMIZATION: æ£€æŸ¥é¡¹ç›®é¡¶å±‚rollupæ—¶é—´
        è¿™æ˜¯æœ€é‡è¦çš„ä¼˜åŒ– - å¯ä»¥åœ¨ä¸è°ƒç”¨ä»»ä½•APIçš„æƒ…å†µä¸‹åˆ¤æ–­æ•´ä¸ªé¡¹ç›®æ˜¯å¦éœ€è¦åŒæ­¥
        """
        try:
            async with dal.get_connection() as conn:
                query = """
                SELECT 
                    MAX(last_modified_time_rollup) as max_rollup_time,
                    COUNT(*) as total_top_level_folders,
                    COUNT(CASE WHEN last_modified_time_rollup > $2 THEN 1 END) as folders_with_changes,
                    AVG(object_count) as avg_objects_per_folder
                FROM folders 
                WHERE project_id = $1 
                  AND depth = 0
                  AND last_modified_time_rollup IS NOT NULL
                """
                
                result = await conn.fetchrow(query, project_id, last_sync_time)
                
                if not result or not result['max_rollup_time']:
                    return {
                        'can_skip_entire_project': False,
                        'reason': 'No top-level folders found or no rollup time available',
                        'recommendation': 'Perform incremental sync with folder-level checks'
                    }
                
                max_rollup_time = result['max_rollup_time']
                total_folders = result['total_top_level_folders']
                folders_with_changes = result['folders_with_changes']
                avg_objects = result['avg_objects_per_folder'] or 0
                
                # ğŸ¯ å…³é”®åˆ¤æ–­ï¼šå¦‚æœæœ€å¤§rollupæ—¶é—´ <= ä¸Šæ¬¡åŒæ­¥æ—¶é—´ï¼Œæ•´ä¸ªé¡¹ç›®éƒ½å¯ä»¥è·³è¿‡
                can_skip_entire_project = max_rollup_time <= last_sync_time
                
                skip_efficiency = 0.0
                if total_folders > 0:
                    skip_efficiency = ((total_folders - folders_with_changes) / total_folders) * 100
                
                # ä¼°ç®—èŠ‚çœçš„APIè°ƒç”¨æ¬¡æ•°
                estimated_api_calls_saved = 0
                if can_skip_entire_project:
                    # æ¯ä¸ªæ–‡ä»¶å¤¹è‡³å°‘èŠ‚çœ2ä¸ªAPIè°ƒç”¨ï¼ˆè·å–å†…å®¹ + è·å–è‡ªå®šä¹‰å±æ€§ï¼‰
                    estimated_api_calls_saved = int(total_folders * avg_objects * 2)
                
                return {
                    'can_skip_entire_project': can_skip_entire_project,
                    'max_rollup_time': max_rollup_time.isoformat(),
                    'last_sync_time': last_sync_time.isoformat(),
                    'total_top_level_folders': total_folders,
                    'folders_with_changes': folders_with_changes,
                    'skip_efficiency_percentage': skip_efficiency,
                    'estimated_api_calls_saved': estimated_api_calls_saved,
                    'recommendation': 'Skip entire project' if can_skip_entire_project else 'Perform incremental sync',
                    'optimization_level': 'project_level' if can_skip_entire_project else 'folder_level'
                }
                
        except Exception as e:
            logger.error(f"Top-level rollup check failed: {e}")
            return {
                'can_skip_entire_project': False,
                'reason': f'Check failed: {str(e)}',
                'recommendation': 'Perform incremental sync with error handling'
            }
    
    # ============================================================================
    # ğŸš€ Layer 2.5: æ–‡ä»¶çº§Timestampæ¯”å¯¹ä¼˜åŒ–
    # ============================================================================
    
    async def _identify_files_needing_updates(self, changed_files: List[Dict], project_id: str, 
                                            last_sync_time: datetime, dal) -> List[Dict]:
        """
        ğŸ¯ å…³é”®ä¼˜åŒ–ï¼šæ–‡ä»¶çº§timestampæ¯”å¯¹
        è¯†åˆ«éœ€è¦æ›´æ–°è‡ªå®šä¹‰å±æ€§å’Œç‰ˆæœ¬çš„æ–‡ä»¶ï¼Œè®°å½•æ–‡ä»¶IDç”¨äºæ‰¹é‡APIè°ƒç”¨
        """
        try:
            files_needing_updates = []
            
            logger.info(f"ğŸ” å¼€å§‹æ–‡ä»¶çº§timestampæ¯”å¯¹: {len(changed_files)} ä¸ªæ–‡ä»¶")
            
            for file_data in changed_files:
                file_id = file_data.get('id')
                if not file_id:
                    continue
                
                # è·å–æ–‡ä»¶çš„lastModifiedTime
                attributes = file_data.get('attributes', {})
                file_modified_time_str = attributes.get('lastModifiedTime')
                
                if not file_modified_time_str:
                    # æ²¡æœ‰ä¿®æ”¹æ—¶é—´ï¼Œä¿å®ˆç­–ç•¥ï¼šéœ€è¦æ›´æ–°
                    files_needing_updates.append({
                        'file_id': file_id,
                        'file_data': file_data,
                        'reason': 'no_modified_time',
                        'needs_custom_attributes': True,
                        'needs_version_update': True
                    })
                    continue
                
                # è§£ææ–‡ä»¶ä¿®æ”¹æ—¶é—´
                file_modified_time = self._parse_datetime(file_modified_time_str)
                
                if not file_modified_time:
                    # è§£æå¤±è´¥ï¼Œä¿å®ˆç­–ç•¥ï¼šéœ€è¦æ›´æ–°
                    files_needing_updates.append({
                        'file_id': file_id,
                        'file_data': file_data,
                        'reason': 'parse_time_failed',
                        'needs_custom_attributes': True,
                        'needs_version_update': True
                    })
                    continue
                
                # ğŸ¯ å…³é”®æ¯”è¾ƒï¼šæ–‡ä»¶ä¿®æ”¹æ—¶é—´ vs ä¸Šæ¬¡åŒæ­¥æ—¶é—´
                if self._is_file_modified_since_sync(file_modified_time, last_sync_time):
                    # æ–‡ä»¶æœ‰æ›´æ–°ï¼Œéœ€è¦æ£€æŸ¥è‡ªå®šä¹‰å±æ€§å’Œç‰ˆæœ¬
                    files_needing_updates.append({
                        'file_id': file_id,
                        'file_data': file_data,
                        'reason': 'file_modified',
                        'file_modified_time': file_modified_time.isoformat(),
                        'last_sync_time': last_sync_time.isoformat(),
                        'needs_custom_attributes': True,
                        'needs_version_update': True
                    })
                    
                    logger.debug(f"ğŸ“„ æ–‡ä»¶éœ€è¦æ›´æ–°: {attributes.get('displayName', file_id)} "
                               f"(ä¿®æ”¹æ—¶é—´: {file_modified_time} > åŒæ­¥æ—¶é—´: {last_sync_time})")
                else:
                    # æ–‡ä»¶æ— å˜åŒ–ï¼Œè·³è¿‡
                    logger.debug(f"â­ï¸ æ–‡ä»¶è·³è¿‡: {attributes.get('displayName', file_id)} "
                               f"(ä¿®æ”¹æ—¶é—´: {file_modified_time} <= åŒæ­¥æ—¶é—´: {last_sync_time})")
            
            # ç»Ÿè®¡
            self.stats['files_analyzed'] = len(changed_files)
            self.stats['files_needing_updates'] = len(files_needing_updates)
            self.stats['files_skipped'] = len(changed_files) - len(files_needing_updates)
            
            skip_efficiency = (self.stats['files_skipped'] / max(len(changed_files), 1)) * 100
            
            logger.info(f"ğŸ¯ æ–‡ä»¶çº§æ¯”å¯¹å®Œæˆ: {len(files_needing_updates)}/{len(changed_files)} éœ€è¦æ›´æ–°, "
                       f"è·³è¿‡æ•ˆç‡: {skip_efficiency:.1f}%")
            
            return files_needing_updates
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶çº§timestampæ¯”å¯¹å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›æ‰€æœ‰æ–‡ä»¶ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            return [{'file_id': f.get('id'), 'file_data': f, 'reason': 'comparison_failed', 
                    'needs_custom_attributes': True, 'needs_version_update': True} 
                   for f in changed_files if f.get('id')]
    
    def _is_file_modified_since_sync(self, file_modified_time: datetime, last_sync_time: datetime) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨ä¸Šæ¬¡åŒæ­¥åè¢«ä¿®æ”¹"""
        try:
            if not file_modified_time or not last_sync_time:
                return True  # ä¿å®ˆç­–ç•¥
            
            # å¤„ç†æ—¶åŒºé—®é¢˜
            if file_modified_time.tzinfo is None:
                file_modified_time = file_modified_time.replace(tzinfo=timezone.utc)
            if last_sync_time.tzinfo is None:
                last_sync_time = last_sync_time.replace(tzinfo=timezone.utc)
            
            return file_modified_time > last_sync_time
            
        except Exception as e:
            logger.warning(f"æ–‡ä»¶æ—¶é—´æ¯”è¾ƒå¤±è´¥: {e}")
            return True  # ä¿å®ˆç­–ç•¥
    
    def _parse_datetime(self, datetime_str):
        """è§£ædatetimeå­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        if not datetime_str:
            return None
        
        # å¦‚æœå·²ç»æ˜¯datetimeå¯¹è±¡ï¼Œç›´æ¥è¿”å›
        if isinstance(datetime_str, datetime):
            return datetime_str
        
        try:
            # åªå¤„ç†å­—ç¬¦ä¸²ç±»å‹
            if not isinstance(datetime_str, str):
                logger.warning(f"Expected string for datetime parsing, got {type(datetime_str)}: {datetime_str}")
                return None
            
            # å¤„ç†ACC APIè¿”å›çš„ç‰¹æ®Šæ ¼å¼ï¼š2025-10-20T02:32:52.0000000Z
            if 'T' in datetime_str:
                # å¤„ç†Zç»“å°¾çš„UTCæ—¶é—´
                if datetime_str.endswith('Z'):
                    # å¤„ç†è¶…è¿‡6ä½çš„å°æ•°ç§’ï¼ˆPythonçš„%fåªæ”¯æŒ6ä½ï¼‰
                    if '.' in datetime_str:
                        date_part, time_part = datetime_str.split('T')
                        if '.' in time_part:
                            time_base, microseconds_z = time_part.split('.')
                            microseconds = microseconds_z.rstrip('Z')
                            # æˆªæ–­æˆ–å¡«å……åˆ°6ä½
                            if len(microseconds) > 6:
                                microseconds = microseconds[:6]
                            else:
                                microseconds = microseconds.ljust(6, '0')
                            datetime_str = f"{date_part}T{time_base}.{microseconds}Z"
                    
                    # ä½¿ç”¨fromisoformatå¤„ç†
                    return datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                else:
                    return datetime.fromisoformat(datetime_str)
            
            # å°è¯•å…¶ä»–æ ¼å¼
            formats = [
                '%Y-%m-%dT%H:%M:%S.%fZ',
                '%Y-%m-%dT%H:%M:%SZ',
                '%Y-%m-%dT%H:%M:%S.%f',
                '%Y-%m-%dT%H:%M:%S',
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d'
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(datetime_str, fmt)
                    if 'Z' in fmt or 'T' in fmt:
                        dt = dt.replace(tzinfo=timezone.utc)
                    return dt
                except ValueError:
                    continue
            
            logger.warning(f"Cannot parse datetime: {datetime_str}")
            return None
            
        except Exception as e:
            logger.warning(f"Datetime parsing failed: {datetime_str}, error: {e}")
            return None

    # ============================================================================
    # ğŸš€ Layer 2: æ‰¹é‡APIè°ƒç”¨ä¼˜åŒ–
    # ============================================================================
    
    async def _batch_api_operations(self, project_id: str, folders_to_check: List[Dict[str, Any]], 
                                  headers: dict) -> Tuple[List[Dict], List[Dict]]:
        """æ‰¹é‡APIæ“ä½œä¼˜åŒ–"""
        
        logger.info(f"ğŸ“¡ å¼€å§‹æ‰¹é‡APIæ“ä½œ: {len(folders_to_check)} ä¸ªæ–‡ä»¶å¤¹")
        start_time = time.time()
        
        try:
            dal = await get_optimized_postgresql_dal()
            last_sync_time = await dal.get_project_last_sync_time(project_id)
            
            # ğŸ”‘ æ‰¹é‡è·å–æ–‡ä»¶å¤¹å†…å®¹
            folder_ids = [folder['id'] for folder in folders_to_check]
            
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…APIé™åˆ¶
            batch_size = min(20, len(folder_ids))  # APIæ‰¹é‡é™åˆ¶
            contents_batches = []
            
            # ä½¿ç”¨ç¾æœ‰çš„ç•°æ­¥æ–¹æ³•é€å€‹ç²å–æ–‡ä»¶å¤¾å…§å®¹
            try:
                import aiohttp
                
                async with aiohttp.ClientSession() as session:
                    contents_batches = []
                    
                    # åˆ†æ‰¹è™•ç†ä»¥é¿å…éå¤šä¸¦ç™¼
                    for i in range(0, len(folder_ids), batch_size):
                        batch_ids = folder_ids[i:i + batch_size]
                        
                        # ä¸¦ç™¼ç²å–ç•¶å‰æ‰¹æ¬¡çš„æ–‡ä»¶å¤¾å…§å®¹
                        tasks = []
                        for folder_id in batch_ids:
                            task = self._get_folder_contents_async(session, project_id, folder_id, headers)
                            tasks.append((folder_id, task))
                        
                        # ç­‰å¾…ç•¶å‰æ‰¹æ¬¡å®Œæˆ
                        batch_results = {}
                        for folder_id, task in tasks:
                            try:
                                content = await task
                                batch_results[folder_id] = content
                            except Exception as e:
                                logger.warning(f"Failed to get contents for folder {folder_id}: {e}")
                                batch_results[folder_id] = {}
                        
                        contents_batches.append(batch_results)
                        
                        # APIç¯€æµ
                        if i + batch_size < len(folder_ids):
                            await asyncio.sleep(self.api_delay)
                            
            except ImportError:
                logger.warning("aiohttp not available, skipping batch processing")
                contents_batches = []
            
            # ğŸ”‘ è§£ææ‰¹é‡ç»“æœï¼Œæå–å˜åŒ–çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
            changed_folders = []
            changed_files = []
            
            for batch_contents in contents_batches:
                for folder_id, contents_data in batch_contents.items():
                    if not contents_data or not contents_data.get('data'):
                        continue
                    
                    # æ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹ä¿¡æ¯
                    folder_info = next((f for f in folders_to_check if f['id'] == folder_id), None)
                    if not folder_info:
                        continue
                    
                    # å¤„ç†æ–‡ä»¶å¤¹å†…å®¹
                    for item in contents_data['data']:
                        item_type = item.get('type')
                        
                        if item_type == 'folders':
                            # å­æ–‡ä»¶å¤¹ - æ£€æŸ¥rollupæ—¶é—´
                            subfolder_rollup = self._parse_datetime(
                                item.get('attributes', {}).get('lastModifiedTimeRollup')
                            )
                            if not subfolder_rollup or (last_sync_time and subfolder_rollup > last_sync_time):
                                changed_folders.append(item)
                            else:
                                self.stats['smart_skips'] += 1
                        
                        elif item_type in ['items', 'files']:
                            # æ–‡ä»¶ - æ£€æŸ¥ä¿®æ”¹æ—¶é—´
                            file_modified = self._parse_datetime(
                                item.get('attributes', {}).get('lastModifiedTime')
                            )
                            if file_modified and last_sync_time and file_modified > last_sync_time:
                                changed_files.append(item)
            
            api_time = time.time() - start_time
            self.stats['processing_time'] += api_time
            
            logger.info(f"âœ… æ‰¹é‡APIæ“ä½œå®Œæˆ: {len(changed_folders)} æ–‡ä»¶å¤¹, {len(changed_files)} æ–‡ä»¶, è€—æ—¶: {api_time:.2f}s")
            
            return changed_folders, changed_files
            
        except Exception as e:
            logger.error(f"æ‰¹é‡APIæ“ä½œå¤±è´¥: {e}")
            return [], []
    
    async def _batch_get_file_versions_and_custom_attrs(self, project_id: str, 
                                                      changed_files: List[Dict[str, Any]], 
                                                      headers: dict) -> List[Dict[str, Any]]:
        """æ‰¹é‡è·å–æ–‡ä»¶ç‰ˆæœ¬å’Œè‡ªå®šä¹‰å±æ€§"""
        
        if not changed_files:
            return []
        
        logger.info(f"Batch retrieving file details: {len(changed_files)} files")
        
        try:
            # ğŸ”‘ æ‰¹é‡è·å–æ–‡ä»¶ç‰ˆæœ¬è¯¦ç»†ä¿¡æ¯ï¼ˆå‚è€ƒMongoDBå®ç°ï¼‰
            file_ids = [file_data['id'] for file_data in changed_files]
            
            # åˆ†æ‰¹å¤„ç†ç‰ˆæœ¬ä¿¡æ¯è·å–
            batch_size = 10  # ç‰ˆæœ¬APIå¹¶å‘é™åˆ¶æ›´ä¸¥æ ¼
            all_file_metadata = []
            
            # å¹¶å‘è·å–ç‰ˆæœ¬ä¿¡æ¯
            try:
                import aiohttp
                
                async with aiohttp.ClientSession() as session:
                    # åˆ†æ‰¹å¤„ç†æ–‡ä»¶
                    for i in range(0, len(changed_files), batch_size):
                        batch_files = changed_files[i:i + batch_size]
                        
                        # å¹¶å‘è·å–å½“å‰æ‰¹æ¬¡çš„ç‰ˆæœ¬ä¿¡æ¯
                        tasks = []
                        for file_data in batch_files:
                            file_id = file_data.get('id')
                            if file_id:
                                task = self._get_file_versions_async(session, project_id, file_id, headers)
                                tasks.append((file_data, task))
                        
                        # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
                        for file_data, task in tasks:
                            try:
                                versions_info = await task
                                # å°†ç‰ˆæœ¬ä¿¡æ¯åˆå¹¶åˆ°æ–‡ä»¶æ•°æ®ä¸­
                                file_data['versions_info'] = versions_info
                                all_file_metadata.append(file_data)
                                
                                self.stats['api_calls'] += 1
                            except Exception as e:
                                logger.warning(f"è·å–æ–‡ä»¶ç‰ˆæœ¬å¤±è´¥ {file_data.get('id')}: {e}")
                                # å³ä½¿ç‰ˆæœ¬è·å–å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
                                file_data['versions_info'] = []
                                all_file_metadata.append(file_data)
                        
                        # APIèŠ‚æµ
                        if i + batch_size < len(changed_files):
                            await asyncio.sleep(self.api_delay)
                        
                        self.stats['batch_operations'] += 1
                        
            except ImportError:
                logger.warning("aiohttp not available, using basic file metadata")
                all_file_metadata = changed_files
            
            # ğŸ”‘ æ‰¹é‡è·å–è‡ªå®šä¹‰å±æ€§å’Œå­˜å‚¨ä¿¡æ¯ (Custom Attributes API)
            version_urns = []
            file_to_version_map = {}
            
            for file_metadata in all_file_metadata:
                # ä»versions_infoä¸­è·å–ç‰ˆæœ¬URN
                versions_info = file_metadata.get('versions_info', [])
                if versions_info:
                    version_urn = versions_info[0].get('id')  # ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
                    if version_urn:
                        version_urns.append(version_urn)
                        file_to_version_map[version_urn] = file_metadata.get('id')
            
            custom_attributes_data = []
            if version_urns:
                # åˆ†æ‰¹è·å–è‡ªå®šä¹‰å±æ€§ (APIé™åˆ¶50ä¸ª)
                for i in range(0, len(version_urns), batch_size):
                    batch_urns = version_urns[i:i + batch_size]
                    
                    async with self.api_semaphore:
                        # è°ƒç”¨å®é™…çš„Custom Attributes API
                        custom_attrs_batch = await self._call_custom_attributes_api(project_id, batch_urns, headers)
                        
                        custom_attributes_data.extend(custom_attrs_batch)
                        self.stats['api_calls'] += 1
                        self.stats['batch_operations'] += 1
            
            # åˆå¹¶æ–‡ä»¶å…ƒæ•°æ®ã€ç‰ˆæœ¬ä¿¡æ¯å’Œè‡ªå®šä¹‰å±æ€§
            enriched_files = []
            for file_metadata in all_file_metadata:
                # æŸ¥æ‰¾å¯¹åº”çš„è‡ªå®šä¹‰å±æ€§å’Œå­˜å‚¨ä¿¡æ¯
                file_id = file_metadata.get('id')
                versions_info = file_metadata.get('versions_info', [])
                
                # ä»è‡ªå®šä¹‰å±æ€§APIè·å–é¢å¤–ä¿¡æ¯
                custom_attrs_info = None
                if versions_info:
                    version_urn = versions_info[0].get('id')
                    custom_attrs_info = next(
                        (attr for attr in custom_attributes_data if attr.get('urn') == version_urn),
                        None
                    )
                
                # æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯ä¸­çš„storageUrn
                if custom_attrs_info and versions_info:
                    storage_urn = custom_attrs_info.get('storageUrn')
                    if storage_urn:
                        versions_info[0]['detailed_attributes']['storageUrn'] = storage_urn
                
                # æ·»åŠ è‡ªå®šä¹‰å±æ€§
                file_metadata['custom_attributes'] = custom_attrs_info.get('customAttributes', []) if custom_attrs_info else []
                enriched_files.append(file_metadata)
            
            logger.info(f"âœ… æ–‡ä»¶è¯¦ç»†ä¿¡æ¯è·å–å®Œæˆ: {len(enriched_files)} ä¸ªæ–‡ä»¶")
            return enriched_files
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
            return changed_files  # è¿”å›åŸºæœ¬ä¿¡æ¯
    
    # ============================================================================
    # ğŸš€ Layer 3: æ•°æ®åº“æ‰¹é‡æ“ä½œä¼˜åŒ–
    # ============================================================================
    
    async def _batch_database_operations(self, project_id: str, 
                                       changed_folders: List[Dict[str, Any]], 
                                       changed_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰¹é‡æ•°æ®åº“æ“ä½œä¼˜åŒ–"""
        
        logger.info(f"ğŸ’¾ å¼€å§‹æ‰¹é‡æ•°æ®åº“æ“ä½œ: {len(changed_folders)} æ–‡ä»¶å¤¹, {len(changed_files)} æ–‡ä»¶")
        start_time = time.time()
        
        try:
            dal = await get_optimized_postgresql_dal()
            results = {
                'folders_synced': 0,
                'files_synced': 0,
                'custom_attrs_synced': 0,
                'errors': []
            }
            
            # ğŸ”‘ ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹
            if changed_folders:
                async with self.db_semaphore:
                    folders_data = []
                    for folder in changed_folders:
                        folder_record = self.converter.transform_folder_data(
                            folder, project_id, None, "", 0
                        )
                        folders_data.append(folder_record)
                    
                    folder_result = await dal.batch_upsert_folders(folders_data)
                    results['folders_synced'] = folder_result.get('upserted', 0)
                    results['errors'].extend(folder_result.get('errors', []))
                    
                    self.stats['batch_operations'] += 1
            
            # ğŸ”‘ ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡å¤„ç†æ–‡ä»¶
            if changed_files:
                async with self.db_semaphore:
                    files_data = []
                    versions_data = []
                    custom_attrs_definitions = []
                    custom_attrs_values = []
                    
                    for file_data in changed_files:
                        # è½¬æ¢æ–‡ä»¶æ•°æ®
                        file_record = self.converter.transform_file_data(
                            file_data, project_id, None, "", 0
                        )
                        files_data.append(file_record)
                        
                        # ğŸ”‘ å¤„ç†æ–‡ä»¶ç‰ˆæœ¬ä¿¡æ¯
                        tip_version_urn = file_record.get('tip_version_urn')
                        logger.info(f"DEBUG: File {file_record.get('name')} tip_version_urn: {tip_version_urn}")
                        if tip_version_urn:
                            logger.info(f"DEBUG: Creating version record for {file_record.get('name')}")
                            version_record = {
                                'id': tip_version_urn,
                                'project_id': project_id,
                                'file_id': file_record['id'],
                                'version_number': 1,  # é»˜è®¤ä¸º1ï¼Œå®é™…åº”ä»APIè·å–
                                'version_type': 'tip',
                                'create_time': file_record.get('create_time'),
                                'create_user_id': file_record.get('create_user_id'),
                                'create_user_name': file_record.get('create_user_name'),
                                'file_size': file_record.get('file_size', 0),
                                'storage_urn': file_record.get('storage_location'),
                                'mime_type': file_record.get('mime_type'),
                                'metadata': {
                                    'is_tip_version': True,
                                    'original_file_data': file_data
                                }
                            }
                            versions_data.append(version_record)
                            logger.info(f"DEBUG: Added version record, total versions_data: {len(versions_data)}")
                        else:
                            logger.info(f"DEBUG: No tip_version_urn for file {file_record.get('name')}")
                        
                        # ğŸ”‘ æå–è‡ªå®šä¹‰å±æ€§ (åˆ†ç¦»è¡¨è®¾è®¡)
                        custom_attrs = file_data.get('custom_attributes', [])
                        for attr in custom_attrs:
                            # å±æ€§å®šä¹‰
                            attr_def = {
                                'attr_id': attr.get('id'),
                                'project_id': project_id,
                                'name': attr.get('name'),
                                'type': attr.get('type'),
                                'array_values': attr.get('arrayValues', [])
                            }
                            custom_attrs_definitions.append(attr_def)
                            
                            # å±æ€§å€¼
                            attr_value = {
                                'file_id': file_data.get('id'),
                                'attr_id': attr.get('id'),
                                'project_id': project_id,
                                'value': attr.get('value'),
                                'value_date': self._parse_date_value(attr) if attr.get('type') == 'date' else None,
                                'value_number': self._parse_number_value(attr) if attr.get('type') == 'number' else None,
                                'value_boolean': self._parse_boolean_value(attr) if attr.get('type') == 'boolean' else None
                            }
                            custom_attrs_values.append(attr_value)
                    
                    # æ‰¹é‡æ’å…¥æ–‡ä»¶
                    file_result = await dal.batch_upsert_files(files_data)
                    results['files_synced'] = file_result.get('upserted', 0)
                    results['errors'].extend(file_result.get('errors', []))
                    
                    # ğŸ”‘ æ‰¹é‡æ’å…¥æ–‡ä»¶ç‰ˆæœ¬
                    logger.info(f"DEBUG: versions_data length: {len(versions_data)}")
                    if versions_data:
                        try:
                            logger.info(f"DEBUG: Creating {len(versions_data)} file versions...")
                            versions_result = await dal.batch_upsert_file_versions(versions_data)
                            logger.info(f"âœ… æ–‡ä»¶ç‰ˆæœ¬åŒæ­¥å®Œæˆ: {versions_result.get('upserted', 0)} ä¸ªç‰ˆæœ¬")
                        except Exception as e:
                            logger.error(f"æ–‡ä»¶ç‰ˆæœ¬åŒæ­¥å¤±è´¥: {e}")
                            results['errors'].append(f"File versions sync failed: {e}")
                    else:
                        logger.info("DEBUG: No versions_data to process")
                    
                    # ğŸ”‘ ç¬¬ä¸‰é˜¶æ®µï¼šæ‰¹é‡å¤„ç†è‡ªå®šä¹‰å±æ€§ (åˆ†ç¦»è¡¨)
                    if custom_attrs_definitions:
                        # å»é‡å±æ€§å®šä¹‰
                        unique_definitions = self._deduplicate_definitions(custom_attrs_definitions)
                        def_result = await dal.batch_upsert_custom_attribute_definitions(unique_definitions)
                        
                        # æ‰¹é‡æ’å…¥å±æ€§å€¼
                        if custom_attrs_values:
                            value_result = await dal.batch_upsert_custom_attribute_values(custom_attrs_values)
                            results['custom_attrs_synced'] = value_result.get('upserted', 0)
                            results['errors'].extend(value_result.get('errors', []))
                    
                    self.stats['batch_operations'] += 2  # æ–‡ä»¶ + è‡ªå®šä¹‰å±æ€§
            
            db_time = time.time() - start_time
            self.stats['processing_time'] += db_time
            
            logger.info(f"âœ… æ‰¹é‡æ•°æ®åº“æ“ä½œå®Œæˆ: è€—æ—¶ {db_time:.2f}s")
            logger.info(f"   - æ–‡ä»¶å¤¹: {results['folders_synced']}")
            logger.info(f"   - æ–‡ä»¶: {results['files_synced']}")
            logger.info(f"   - è‡ªå®šä¹‰å±æ€§: {results['custom_attrs_synced']}")
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            return {'folders_synced': 0, 'files_synced': 0, 'custom_attrs_synced': 0, 'errors': [str(e)]}
    
    # ============================================================================
    # ğŸš€ Layer 4: å¹¶å‘å¤„ç†ä¼˜åŒ–
    # ============================================================================
    
    async def _concurrent_processing_with_memory_management(self, project_id: str, 
                                                          sync_items: List[Dict[str, Any]], 
                                                          headers: dict) -> Dict[str, Any]:
        """å¹¶å‘å¤„ç†ä¸å†…å­˜ç®¡ç†"""
        
        logger.info(f"âš¡ å¼€å§‹å¹¶å‘å¤„ç†: {len(sync_items)} ä¸ªé¡¹ç›®")
        start_time = time.time()
        
        try:
            # ğŸ”‘ æ™ºèƒ½ä»»åŠ¡åˆ†ç»„
            task_groups = self._create_intelligent_task_groups(sync_items)
            
            results = {
                'high_priority': {'count': 0, 'errors': []},
                'medium_priority': {'count': 0, 'errors': []},
                'low_priority': {'count': 0, 'errors': []},
                'total_processed': 0
            }
            
            # ğŸ”‘ åˆ†é˜¶æ®µå¹¶å‘æ‰§è¡Œ
            # é˜¶æ®µ1ï¼šé«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆæœ€å¤§å¹¶å‘ï¼‰
            if task_groups['high_priority']:
                logger.info(f"ğŸ”¥ å¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡: {len(task_groups['high_priority'])} ä¸ª")
                high_results = await self._process_priority_group(
                    project_id, task_groups['high_priority'], headers, max_concurrency=self.max_workers
                )
                results['high_priority'] = high_results
                results['total_processed'] += high_results['count']
                
                # å†…å­˜æ¸…ç†
                await self._memory_cleanup()
            
            # é˜¶æ®µ2ï¼šä¸­ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆä¸­ç­‰å¹¶å‘ï¼‰
            if task_groups['medium_priority']:
                logger.info(f"ğŸ”¶ å¤„ç†ä¸­ä¼˜å…ˆçº§ä»»åŠ¡: {len(task_groups['medium_priority'])} ä¸ª")
                medium_results = await self._process_priority_group(
                    project_id, task_groups['medium_priority'], headers, max_concurrency=self.max_workers // 2
                )
                results['medium_priority'] = medium_results
                results['total_processed'] += medium_results['count']
                
                # å†…å­˜æ¸…ç†
                await self._memory_cleanup()
            
            # é˜¶æ®µ3ï¼šä½ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆä½å¹¶å‘ï¼‰
            if task_groups['low_priority']:
                logger.info(f"ğŸ”· å¤„ç†ä½ä¼˜å…ˆçº§ä»»åŠ¡: {len(task_groups['low_priority'])} ä¸ª")
                low_results = await self._process_priority_group(
                    project_id, task_groups['low_priority'], headers, max_concurrency=self.max_workers // 4
                )
                results['low_priority'] = low_results
                results['total_processed'] += low_results['count']
            
            concurrent_time = time.time() - start_time
            self.stats['processing_time'] += concurrent_time
            
            logger.info(f"âœ… å¹¶å‘å¤„ç†å®Œæˆ: {results['total_processed']} ä¸ªé¡¹ç›®, è€—æ—¶: {concurrent_time:.2f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"å¹¶å‘å¤„ç†å¤±è´¥: {e}")
            return {'total_processed': 0, 'errors': [str(e)]}
    
    def _create_intelligent_task_groups(self, items: List[Dict[str, Any]]) -> Dict[str, List]:
        """æ™ºèƒ½ä»»åŠ¡åˆ†ç»„"""
        
        groups = {
            'high_priority': [],    # é«˜ä¼˜å…ˆçº§ï¼šå°æ–‡ä»¶ã€é‡è¦æ–‡ä»¶å¤¹
            'medium_priority': [],  # ä¸­ä¼˜å…ˆçº§ï¼šæ™®é€šæ–‡ä»¶
            'low_priority': []      # ä½ä¼˜å…ˆçº§ï¼šå¤§æ–‡ä»¶ã€å¤æ‚å±æ€§
        }
        
        for item in items:
            priority = self._calculate_item_priority(item)
            groups[priority].append(item)
        
        logger.info(f"ğŸ“Š ä»»åŠ¡åˆ†ç»„å®Œæˆ: é«˜ä¼˜å…ˆçº§ {len(groups['high_priority'])}, "
                   f"ä¸­ä¼˜å…ˆçº§ {len(groups['medium_priority'])}, ä½ä¼˜å…ˆçº§ {len(groups['low_priority'])}")
        
        return groups
    
    def _calculate_item_priority(self, item: Dict[str, Any]) -> str:
        """è®¡ç®—é¡¹ç›®ä¼˜å…ˆçº§"""
        
        # åŸºç¡€ä¼˜å…ˆçº§
        if item.get('type') == 'folders':
            base_priority = 'high_priority'
        else:
            base_priority = 'medium_priority'
        
        # æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´
        file_size = item.get('attributes', {}).get('fileSize', 0)
        if file_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Š
            return 'low_priority'
        
        # æ ¹æ®è‡ªå®šä¹‰å±æ€§æ•°é‡è°ƒæ•´
        custom_attrs_count = len(item.get('custom_attributes', []))
        if custom_attrs_count > 10:
            return 'low_priority'
        
        # æ ¹æ®ä¿®æ”¹æ—¶é—´è°ƒæ•´ï¼ˆæœ€è¿‘ä¿®æ”¹çš„ä¼˜å…ˆï¼‰
        last_modified = item.get('attributes', {}).get('lastModifiedTime')
        if last_modified:
            modified_time = self._parse_datetime(last_modified)
            if modified_time and (datetime.utcnow() - modified_time).days < 1:
                return 'high_priority'
        
        return base_priority
    
    async def _process_priority_group(self, project_id: str, items: List[Dict[str, Any]], 
                                    headers: dict, max_concurrency: int) -> Dict[str, Any]:
        """å¤„ç†ä¼˜å…ˆçº§ç»„"""
        
        semaphore = asyncio.Semaphore(max_concurrency)
        
        async def process_item_with_semaphore(item):
            async with semaphore:
                return await self._process_single_item_optimized(project_id, item, headers)
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [process_item_with_semaphore(item) for item in items]
        
        # æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = 0
        errors = []
        
        for result in results:
            if isinstance(result, Exception):
                errors.append(str(result))
            else:
                success_count += 1
        
        self.stats['concurrent_operations'] += len(tasks)
        
        return {'count': success_count, 'errors': errors}
    
    async def _process_single_item_optimized(self, project_id: str, item: Dict[str, Any], 
                                           headers: dict) -> bool:
        """ä¼˜åŒ–çš„å•é¡¹ç›®å¤„ç†"""
        
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ å•é¡¹ç›®çš„å…·ä½“å¤„ç†é€»è¾‘
            # ä¾‹å¦‚ï¼šè·å–è¯¦ç»†ä¿¡æ¯ã€è½¬æ¢æ•°æ®ã€å­˜å‚¨åˆ°æ•°æ®åº“ç­‰
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            await asyncio.sleep(0.01)
            
            return True
            
        except Exception as e:
            logger.error(f"å•é¡¹ç›®å¤„ç†å¤±è´¥: {e}")
            raise
    
    # ============================================================================
    # ğŸš€ Layer 5: å†…å­˜ç®¡ç†ä¸æ€§èƒ½ç›‘æ§
    # ============================================================================
    
    async def _memory_cleanup(self):
        """å†…å­˜æ¸…ç†"""
        
        current_memory = self._get_memory_usage_mb()
        
        if current_memory > self.memory_threshold_mb * 0.8:
            logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {current_memory}MB, æ‰§è¡Œæ¸…ç†")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # ç­‰å¾…å†…å­˜é‡Šæ”¾
            await asyncio.sleep(0.1)
            
            new_memory = self._get_memory_usage_mb()
            logger.info(f"å†…å­˜æ¸…ç†å®Œæˆ: {current_memory}MB -> {new_memory}MB")
        
        # æ›´æ–°å³°å€¼å†…å­˜ä½¿ç”¨
        self.stats['memory_peak_mb'] = max(self.stats['memory_peak_mb'], current_memory)
    
    def _get_memory_usage_mb(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0
    
    def _calculate_optimization_efficiency(self) -> float:
        """è®¡ç®—ä¼˜åŒ–æ•ˆç‡"""
        try:
            total_operations = self.stats.get('concurrent_operations', 0) + self.stats.get('smart_skips', 0)
            smart_skips = self.stats.get('smart_skips', 0)
            
            if total_operations == 0:
                return 0.0
            
            # è®¡ç®—è·³è¿‡çš„æ¯”ä¾‹ä½œä¸ºä¼˜åŒ–æ•ˆç‡
            efficiency = (smart_skips / total_operations) * 100
            return round(efficiency, 2)
            
        except Exception as e:
            logger.warning(f"è®¡ç®—ä¼˜åŒ–æ•ˆç‡å¤±è´¥: {str(e)}")
            return 0.0
    
    # ============================================================================
    # ğŸš€ ä¸»è¦åŒæ­¥æ–¹æ³•
    # ============================================================================
    
    async def optimized_incremental_sync(self, project_id: str, max_depth: int = 10, 
                                       include_custom_attributes: bool = True, 
                                       task_uuid: str = None, headers: dict = None) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„å¢é‡åŒæ­¥ - ç»Ÿä¸€ä½¿ç”¨V2æ¶æ„"""
        
        logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–å¢é‡åŒæ­¥: é¡¹ç›® {project_id}")
        
        # ç›´æ¥ä½¿ç”¨V2æ¶æ„çš„å¢é‡åŒæ­¥
        return await self._optimized_incremental_sync_v2(project_id, max_depth, include_custom_attributes, task_uuid, headers)
            
    async def optimized_full_sync(self, project_id: str, max_depth: int = 10, 
                                include_custom_attributes: bool = True, 
                                task_uuid: str = None, headers: dict = None) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„å…¨é‡åŒæ­¥ - ç»Ÿä¸€ä½¿ç”¨V2æ¶æ„"""
        
        logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–å…¨é‡åŒæ­¥: é¡¹ç›® {project_id}")
            
        # ç›´æ¥ä½¿ç”¨V2æ¶æ„çš„å…¨é‡åŒæ­¥
        return await self._optimized_full_sync_v2(project_id, max_depth, include_custom_attributes, task_uuid, headers)
    
    # ============================================================================
    # è¾…åŠ©æ–¹æ³• - å¼‚æ­¥APIè°ƒç”¨
    # ============================================================================
    
    async def _get_top_folders_async(self, project_id: str, headers: dict) -> dict:
        """å¼‚æ­¥è·å–é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹ - ä½¿ç”¨æ­£ç¡®çš„Hub-based API"""
        try:
            import aiohttp
            
            async with aiohttp.ClientSession() as session:
                # Step 1: Get hubs to find the hub containing this project
                hubs_url = "https://developer.api.autodesk.com/project/v1/hubs"
                
                async with session.get(hubs_url, headers=headers) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"Failed to get hubs: {response.status} - {error_text}")
                    
                    hubs_data = await response.json()
                    self.stats['api_calls'] += 1
                
                # Step 2: Find the hub containing our project
                hub_id = None
                for hub in hubs_data.get('data', []):
                    hub_id_candidate = hub.get('id')
                    
                    # Check if project exists in this hub
                    projects_url = f"https://developer.api.autodesk.com/project/v1/hubs/{hub_id_candidate}/projects"
                    async with session.get(projects_url, headers=headers) as proj_response:
                        if proj_response.status == 200:
                            projects_data = await proj_response.json()
                            self.stats['api_calls'] += 1
                            
                            for project in projects_data.get('data', []):
                                if project.get('id') == project_id:
                                    hub_id = hub_id_candidate
                                    break
                    
                    if hub_id:
                        break
                
                if not hub_id:
                    raise Exception(f"Project {project_id} not found in any accessible hub")
                
                # Step 3: Get top folders for the project
                top_folders_url = f"https://developer.api.autodesk.com/project/v1/hubs/{hub_id}/projects/{project_id}/topFolders"
                
                async with session.get(top_folders_url, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.stats['api_calls'] += 1
                        return data
                    else:
                        error_text = await response.text()
                        raise Exception(f"Failed to get top folders: {response.status} - {error_text}")
                        
        except ImportError:
            # Fallback to synchronous requests if aiohttp not available
            import requests
            
            # Step 1: Get hubs
            hubs_url = "https://developer.api.autodesk.com/project/v1/hubs"
            response = requests.get(hubs_url, headers=headers)
            
            if response.status_code != 200:
                raise Exception(f"Failed to get hubs: {response.status_code} - {response.text}")
            
            hubs_data = response.json()
            self.stats['api_calls'] += 1
            
            # Step 2: Find hub containing project
            hub_id = None
            for hub in hubs_data.get('data', []):
                hub_id_candidate = hub.get('id')
                
                projects_url = f"https://developer.api.autodesk.com/project/v1/hubs/{hub_id_candidate}/projects"
                proj_response = requests.get(projects_url, headers=headers)
                
                if proj_response.status_code == 200:
                    projects_data = proj_response.json()
                    self.stats['api_calls'] += 1
                    
                    for project in projects_data.get('data', []):
                        if project.get('id') == project_id:
                            hub_id = hub_id_candidate
                            break
                
                if hub_id:
                    break
            
            if not hub_id:
                raise Exception(f"Project {project_id} not found in any accessible hub")
            
            # Step 3: Get top folders
            top_folders_url = f"https://developer.api.autodesk.com/project/v1/hubs/{hub_id}/projects/{project_id}/topFolders"
            response = requests.get(top_folders_url, headers=headers)
            
            if response.status_code == 200:
                self.stats['api_calls'] += 1
                return response.json()
            else:
                raise Exception(f"Failed to get top folders: {response.status_code} - {response.text}")
    
    async def _collect_all_items_recursive_async(self, project_id: str, top_folders: list,
                                               headers: dict, max_depth: int) -> Tuple[List, List]:
        """å¼‚æ­¥é€’å½’æ”¶é›†æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶"""
        all_folders = []
        all_files = []
        
        # BFS queue: (folder_data, depth, parent_path)
        queue = [(folder, 0, "") for folder in top_folders]
        
        try:
            import aiohttp
            
            async with aiohttp.ClientSession() as session:
                while queue and len(queue) > 0:
                    current_batch = queue[:self.batch_size]
                    queue = queue[self.batch_size:]
                    
                    # å¹¶å‘è·å–å½“å‰æ‰¹æ¬¡çš„æ–‡ä»¶å¤¹å†…å®¹
                    tasks = []
                    for folder_data, depth, parent_path in current_batch:
                        if depth >= max_depth:
                            continue
                            
                        folder_id = folder_data.get('id')
                        if folder_id:
                            task = self._get_folder_contents_async(session, project_id, folder_id, headers)
                            tasks.append((folder_data, depth, parent_path, task))
                
                    # å¤„ç†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
                    for folder_data, depth, parent_path, task in tasks:
                        try:
                            contents = await task
                            all_folders.append(folder_data)
                            
                            # å¤„ç†æ–‡ä»¶å¤¹å†…å®¹
                            for item in contents.get('data', []):
                                item_type = item.get('type')
                                
                                if item_type == 'folders':
                                    # å­æ–‡ä»¶å¤¹åŠ å…¥é˜Ÿåˆ—
                                    new_path = f"{parent_path}/{folder_data.get('attributes', {}).get('name', '')}"
                                    queue.append((item, depth + 1, new_path))
                                elif item_type in ['items', 'files']:
                                    # æ–‡ä»¶ç›´æ¥æ·»åŠ 
                                    all_files.append(item)
                                    
                        except Exception as e:
                            logger.warning(f"è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥ {folder_data.get('id')}: {e}")
                    
                    # APIèŠ‚æµ
                    if queue:
                        await asyncio.sleep(self.api_delay)
                        
        except ImportError:
            logger.warning("aiohttp not available, using basic collection")
            # ç®€å•çš„åŒæ­¥æ”¶é›†
            for folder in top_folders:
                all_folders.append(folder)
        
        logger.info(f"BFSæ”¶é›†å®Œæˆ: {len(all_folders)} æ–‡ä»¶å¤¹, {len(all_files)} æ–‡ä»¶")
        return all_folders, all_files
    
    async def _get_folder_contents_async(self, session, project_id: str, folder_id: str, headers: dict) -> dict:
        """å¼‚æ­¥è·å–æ–‡ä»¶å¤¹å†…å®¹"""
        url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/folders/{folder_id}/contents"
        
        try:
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    self.stats['api_calls'] += 1
                    return data
                else:
                    logger.warning(f"Failed to get folder contents: {response.status}")
                    return {}
        except Exception as e:
            logger.error(f"Error getting folder contents: {e}")
            return {}
    
    async def _get_file_versions_async(self, session, project_id: str, item_id: str, headers: dict) -> List[Dict]:
        """å¼‚æ­¥è·å–æ–‡ä»¶ç‰ˆæœ¬ä¿¡æ¯"""
        url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/items/{item_id}/versions"
        
        try:
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    self.stats['api_calls'] += 1
                    return data.get('data', [])
                else:
                    logger.warning(f"Failed to get file versions: {response.status}")
                    return []
        except Exception as e:
            logger.error(f"Error getting file versions: {e}")
            return []
    
    # ============================================================================
    # è¾…åŠ©æ–¹æ³• - å¼‚æ­¥APIè°ƒç”¨
    # ============================================================================
    
    async def _get_top_folders_async(self, project_id: str, headers: dict) -> dict:
        """å¼‚æ­¥è·å–é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹ - ä½¿ç”¨æ­£ç¡®çš„Hub-based API"""
        try:
            import aiohttp
            
            async with aiohttp.ClientSession() as session:
                # Step 1: Get hubs to find the hub containing this project
                hubs_url = "https://developer.api.autodesk.com/project/v1/hubs"
                
                async with session.get(hubs_url, headers=headers) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"Failed to get hubs: {response.status} - {error_text}")
                    
                    hubs_data = await response.json()
                    self.stats['api_calls'] += 1
                
                # Step 2: Find the hub containing our project
                hub_id = None
                for hub in hubs_data.get('data', []):
                    hub_id_candidate = hub.get('id')
                    
                    # Check if project exists in this hub
                    projects_url = f"https://developer.api.autodesk.com/project/v1/hubs/{hub_id_candidate}/projects"
                    async with session.get(projects_url, headers=headers) as proj_response:
                        if proj_response.status == 200:
                            projects_data = await proj_response.json()
                            self.stats['api_calls'] += 1
                            
                            for project in projects_data.get('data', []):
                                if project.get('id') == project_id:
                                    hub_id = hub_id_candidate
                                    break
                    
                    if hub_id:
                        break
                
                if not hub_id:
                    raise Exception(f"Project {project_id} not found in any accessible hub")
                
                # Step 3: Get top folders for the project
                top_folders_url = f"https://developer.api.autodesk.com/project/v1/hubs/{hub_id}/projects/{project_id}/topFolders"
                
                async with session.get(top_folders_url, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.stats['api_calls'] += 1
                        return data
                    else:
                        error_text = await response.text()
                        raise Exception(f"Failed to get top folders: {response.status} - {error_text}")
                        
        except ImportError:
            # Fallback to synchronous requests if aiohttp not available
            import requests
            
            # Step 1: Get hubs
            hubs_url = "https://developer.api.autodesk.com/project/v1/hubs"
            response = requests.get(hubs_url, headers=headers)
            
            if response.status_code != 200:
                raise Exception(f"Failed to get hubs: {response.status_code} - {response.text}")
            
            hubs_data = response.json()
            self.stats['api_calls'] += 1
            
            # Step 2: Find hub containing project
            hub_id = None
            for hub in hubs_data.get('data', []):
                hub_id_candidate = hub.get('id')
                
                projects_url = f"https://developer.api.autodesk.com/project/v1/hubs/{hub_id_candidate}/projects"
                proj_response = requests.get(projects_url, headers=headers)
                
                if proj_response.status_code == 200:
                    projects_data = proj_response.json()
                    self.stats['api_calls'] += 1
                    
                    for project in projects_data.get('data', []):
                        if project.get('id') == project_id:
                            hub_id = hub_id_candidate
                            break
                
                if hub_id:
                    break
            
            if not hub_id:
                raise Exception(f"Project {project_id} not found in any accessible hub")
            
            # Step 3: Get top folders
            top_folders_url = f"https://developer.api.autodesk.com/project/v1/hubs/{hub_id}/projects/{project_id}/topFolders"
            response = requests.get(top_folders_url, headers=headers)
            
            if response.status_code == 200:
                self.stats['api_calls'] += 1
                return response.json()
            else:
                raise Exception(f"Failed to get top folders: {response.status_code} - {response.text}")
    
    async def _collect_all_items_recursive_async(self, project_id: str, top_folders: list, 
                                               headers: dict, max_depth: int) -> Tuple[List, List]:
        """å¼‚æ­¥é€’å½’æ”¶é›†æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶"""
        all_folders = []
        all_files = []
        
        # BFS queue: (folder_data, depth, parent_path)
        queue = [(folder, 0, "") for folder in top_folders]
        
        try:
            import aiohttp
            
            async with aiohttp.ClientSession() as session:
                while queue:
                    current_batch = queue[:self.batch_size]  # Process in batches
                    queue = queue[self.batch_size:]
                    
                    # Process batch of folders concurrently
                    tasks = []
                    for folder_data, depth, parent_path in current_batch:
                        if depth >= max_depth:
                            continue
                            
                        all_folders.append(folder_data)
                        folder_id = folder_data['id']
                        
                        # Create task for getting folder contents
                        task = self._get_folder_contents_async(session, project_id, folder_id, headers)
                        tasks.append((task, folder_data, depth, parent_path))
                    
                    # Wait for all tasks in batch to complete
                    for task, folder_data, depth, parent_path in tasks:
                        try:
                            contents = await task
                            
                            if contents and contents.get('data'):
                                for item in contents['data']:
                                    if item['type'] == 'folders':
                                        # Add subfolder to queue
                                        queue.append((item, depth + 1, parent_path))
                                    elif item['type'] == 'items':
                                        # Add file to collection
                                        all_files.append(item)
                                        
                        except Exception as e:
                            logger.warning(f"Failed to get contents for folder {folder_data.get('id')}: {e}")
                            continue
                    
                    # Add small delay to avoid rate limiting
                    if queue:  # Only delay if there are more items to process
                        await asyncio.sleep(self.api_delay)
                        
        except ImportError:
            # Fallback to synchronous processing
            logger.warning("aiohttp not available, using synchronous processing")
            return await self._collect_all_items_sync_fallback(project_id, top_folders, headers, max_depth)
        
        return all_folders, all_files
    
    async def _get_folder_contents_async(self, session, project_id: str, folder_id: str, headers: dict) -> dict:
        """å¼‚æ­¥è·å–æ–‡ä»¶å¤¹å†…å®¹"""
        url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/folders/{folder_id}/contents"
        
        try:
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    self.stats['api_calls'] += 1
                    return await response.json()
                else:
                    error_text = await response.text()
                    raise Exception(f"Failed to get folder contents: {response.status} - {error_text}")
        except Exception as e:
            logger.warning(f"Error getting folder contents for {folder_id}: {e}")
            return {}
    
    async def _get_file_versions_async(self, session, project_id: str, item_id: str, headers: dict) -> List[Dict]:
        """è·å–æ–‡ä»¶çš„æ‰€æœ‰ç‰ˆæœ¬ä¿¡æ¯ï¼ˆå‚è€ƒMongoDBå®ç°ï¼‰"""
        try:
            url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/items/{item_id}/versions"
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    versions = data.get('data', [])
                    
                    # ç›´æ¥ä½¿ç”¨ç‰ˆæœ¬APIè¿”å›çš„ä¿¡æ¯ï¼Œä¸éœ€è¦é¢å¤–çš„è¯¦æƒ…API
                    detailed_versions = []
                    for version in versions:
                        # ä»ç‰ˆæœ¬APIå“åº”ä¸­æå–è¯¦ç»†ä¿¡æ¯
                        version_attrs = version.get('attributes', {})
                        extension_data = version_attrs.get('extension', {}).get('data', {})
                        
                        # æ·»åŠ è¯¦ç»†å±æ€§åˆ°ç‰ˆæœ¬å¯¹è±¡
                        version['detailed_attributes'] = {
                            'storageSize': version_attrs.get('storageSize', 0),
                            'mimeType': version_attrs.get('fileType'),  # ä½¿ç”¨fileTypeä½œä¸ºmimeType
                            'processState': extension_data.get('processState'),
                            'downloadUrl': None,  # éœ€è¦ä»å…¶ä»–APIè·å–
                            'storageUrn': None  # å°†ä»è‡ªå®šä¹‰å±æ€§APIè·å–
                        }
                        detailed_versions.append(version)
                    
                    return detailed_versions
                else:
                    logger.warning(f"è·å–æ–‡ä»¶ç‰ˆæœ¬å¤±è´¥: {response.status} - {await response.text()}")
                    return []
                    
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶ç‰ˆæœ¬å¼‚å¸¸ {item_id}: {e}")
            return []
    
    async def _get_version_detail_async(self, session, project_id: str, version_id: str, headers: dict) -> Dict:
        """è·å–ç‰ˆæœ¬è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ–‡ä»¶å¤§å°ã€MIMEç±»å‹ç­‰ï¼‰"""
        try:
            url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/versions/{version_id}"
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    version_data = data.get('data', {})
                    attributes = version_data.get('attributes', {})
                    relationships = version_data.get('relationships', {})
                    
                    # æå–å…³é”®ä¿¡æ¯ï¼ˆå‚è€ƒMongoDBçš„åšæ³•ï¼‰
                    return {
                        'detailed_attributes': {
                            'storageSize': attributes.get('storageSize', 0),
                            'mimeType': attributes.get('mimeType'),
                            'processState': attributes.get('processState'),
                            'downloadUrl': relationships.get('downloadFormats', {}).get('links', {}).get('related'),
                            'storageUrn': relationships.get('storage', {}).get('data', {}).get('id')
                        }
                    }
                else:
                    logger.debug(f"è·å–ç‰ˆæœ¬è¯¦æƒ…å¤±è´¥: {response.status}")
                    return {}
                    
        except Exception as e:
            logger.debug(f"è·å–ç‰ˆæœ¬è¯¦æƒ…å¼‚å¸¸ {version_id}: {e}")
            return {}

    async def _call_custom_attributes_api(self, project_id: str, version_urns: List[str], headers: dict) -> List[Dict]:
        """èª¿ç”¨ACCè‡ªå®šç¾©å±¬æ€§API"""
        try:
            import aiohttp
            
            # ç§»é™¤ 'b.' å‰ç¶´ä»¥ç”¨æ–¼BIM360 API
            bim360_project_id = project_id.replace('b.', '') if project_id.startswith('b.') else project_id
            
            url = f"https://developer.api.autodesk.com/bim360/docs/v1/projects/{bim360_project_id}/versions:batch-get"
            
            payload = {
                "urns": version_urns
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get('results', [])
                    elif response.status == 404:
                        # é …ç›®å¯èƒ½ä¸æ”¯æŒè‡ªå®šç¾©å±¬æ€§
                        logger.info(f"Custom attributes not available for project {project_id}")
                        return []
                    else:
                        error_text = await response.text()
                        logger.warning(f"Custom attributes API failed: {response.status} - {error_text}")
                        return []
                        
        except ImportError:
            # Fallback to synchronous requests
            import requests
            
            bim360_project_id = project_id.replace('b.', '') if project_id.startswith('b.') else project_id
            url = f"https://developer.api.autodesk.com/bim360/docs/v1/projects/{bim360_project_id}/versions:batch-get"
            
            payload = {
                "urns": version_urns
            }
            
            response = requests.post(url, headers=headers, json=payload)
            
            if response.status_code == 200:
                data = response.json()
                return data.get('results', [])
            elif response.status_code == 404:
                logger.info(f"Custom attributes not available for project {project_id}")
                return []
            else:
                logger.warning(f"Custom attributes API failed: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            logger.error(f"Custom attributes API call failed: {e}")
            return []
    
    async def _collect_all_items_sync_fallback(self, project_id: str, top_folders: list, 
                                             headers: dict, max_depth: int) -> Tuple[List, List]:
        """åŒæ­¥æ–¹å¼æ”¶é›†æ•°æ®çš„åå¤‡æ–¹æ¡ˆ"""
        import requests
        
        all_folders = []
        all_files = []
        
        queue = [(folder, 0, "") for folder in top_folders]
        
        while queue:
            folder_data, depth, parent_path = queue.pop(0)
            
            if depth >= max_depth:
                continue
                
            all_folders.append(folder_data)
            folder_id = folder_data['id']
            
            try:
                url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/folders/{folder_id}/contents"
                response = requests.get(url, headers=headers)
                
                if response.status_code == 200:
                    self.stats['api_calls'] += 1
                    contents = response.json()
                    
                    if contents and contents.get('data'):
                        for item in contents['data']:
                            if item['type'] == 'folders':
                                queue.append((item, depth + 1, parent_path))
                            elif item['type'] == 'items':
                                all_files.append(item)
                else:
                    logger.warning(f"Failed to get contents for folder {folder_id}: {response.status_code}")
                    
            except Exception as e:
                logger.warning(f"Error processing folder {folder_id}: {e}")
                continue
            
            # Rate limiting
            time.sleep(self.api_delay)
        
        return all_folders, all_files
    
    async def _sync_folder_custom_attribute_definitions(self, project_id: str, folder_id: str, headers: dict) -> List[Dict[str, Any]]:
        """åŒæ­¥æ–‡ä»¶å¤¾çš„è‡ªå®šç¾©å±¬æ€§å®šç¾©"""
        try:
            import aiohttp
            import urllib.parse
            
            # Remove 'b.' prefix for BIM360 API
            bim360_project_id = project_id.replace('b.', '') if project_id.startswith('b.') else project_id
            
            # URL encode folder ID
            encoded_folder_id = urllib.parse.quote(folder_id, safe='')
            
            url = f"https://developer.api.autodesk.com/bim360/docs/v1/projects/{bim360_project_id}/folders/{encoded_folder_id}/custom-attribute-definitions"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        definitions = data.get('results', [])
                        
                        logger.info(f"Found {len(definitions)} custom attribute definitions for folder {folder_id}")
                        
                        # Transform to our format
                        folder_definitions = []
                        for definition in definitions:
                            attr_def = {
                                'attr_id': definition.get('id'),
                                'project_id': project_id,
                                'scope_type': 'folder',  # è®¾ç½®ä½œç”¨åŸŸç±»å‹ä¸ºæ–‡ä»¶å¤¹
                                'scope_folder_id': folder_id,  # å…³è”åˆ°å…·ä½“æ–‡ä»¶å¤¹
                                'name': definition.get('name'),
                                'type': definition.get('type'),
                                'array_values': definition.get('arrayValues', []),
                                'description': None,  # Not provided by API
                                'is_required': False,  # Default
                                'default_value': None,  # Not provided by API
                                'inherit_to_subfolders': False  # ä¸ç»§æ‰¿åˆ°å­æ–‡ä»¶å¤¹ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹ç‹¬ç«‹è®¾ç½®
                            }
                            folder_definitions.append(attr_def)
                        
                        return folder_definitions
                        
                    elif response.status == 404:
                        # Folder may not have custom attributes defined
                        logger.debug(f"No custom attributes defined for folder {folder_id}")
                        return []
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to get folder custom attributes: {response.status} - {error_text}")
                        return []
                        
        except ImportError:
            # Fallback to synchronous requests
            import requests
            import urllib.parse
            
            bim360_project_id = project_id.replace('b.', '') if project_id.startswith('b.') else project_id
            encoded_folder_id = urllib.parse.quote(folder_id, safe='')
            
            url = f"https://developer.api.autodesk.com/bim360/docs/v1/projects/{bim360_project_id}/folders/{encoded_folder_id}/custom-attribute-definitions"
            
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                definitions = data.get('results', [])

                folder_definitions = []
                for definition in definitions:
                    attr_def = {
                        'attr_id': definition.get('id'),
                        'project_id': project_id,
                        'scope_type': 'folder',  # è®¾ç½®ä½œç”¨åŸŸç±»å‹ä¸ºæ–‡ä»¶å¤¹
                        'scope_folder_id': folder_id,  # å…³è”åˆ°å…·ä½“æ–‡ä»¶å¤¹
                        'name': definition.get('name'),
                        'type': definition.get('type'),
                        'array_values': definition.get('arrayValues', []),
                        'description': None,
                        'is_required': False,
                        'default_value': None,
                        'inherit_to_subfolders': False  # ä¸ç»§æ‰¿åˆ°å­æ–‡ä»¶å¤¹
                    }
                    folder_definitions.append(attr_def)

                return folder_definitions
            else:
                logger.debug(f"No custom attributes for folder {folder_id}: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"Error getting folder custom attributes for {folder_id}: {e}")
            return []

    async def _batch_process_folders_async(self, folders: List, project_id: str, headers: dict = None, include_custom_attributes: bool = False) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹"""
        try:
            from database.data_sync_strategy import DataTransformer
            
            dal = await get_optimized_postgresql_dal()
            transformer = DataTransformer()
            
            # Transform folder data
            folders_data = []
            all_folder_definitions = []
            
            for folder_api_data in folders:
                try:
                    folder_record = transformer.transform_folder_data(
                        folder_api_data, project_id, None, "", 0
                    )
                    folders_data.append(folder_record)
                    
                    # ğŸ”‘ Get folder custom attribute definitions if requested
                    if include_custom_attributes and headers:
                        folder_id = folder_api_data.get('id')
                        if folder_id:
                            try:
                                folder_definitions = await self._sync_folder_custom_attribute_definitions(
                                    project_id, folder_id, headers
                                )
                                all_folder_definitions.extend(folder_definitions)
                                
                                if folder_definitions:
                                    logger.info(f"Found {len(folder_definitions)} custom attribute definitions for folder {folder_record.get('name')}")
                                    
                            except Exception as e:
                                logger.warning(f"Failed to get custom attributes for folder {folder_id}: {e}")
                    
                except Exception as e:
                    logger.warning(f"Failed to transform folder data: {e}")
                    continue
            
            # Batch insert folders to database
            folders_synced = 0
            if folders_data:
                result = await dal.batch_upsert_folders(folders_data)
                folders_synced = result.get('upserted', 0)
                self.stats['batch_operations'] += 1
            
            # ğŸ”‘ Batch insert folder custom attribute definitions
            folder_definitions_synced = 0
            if all_folder_definitions:
                logger.info(f"Processing {len(all_folder_definitions)} folder custom attribute definitions...")
                try:
                    # Remove duplicates
                    unique_definitions = self._deduplicate_definitions(all_folder_definitions)
                    def_result = await dal.batch_upsert_custom_attribute_definitions(unique_definitions)
                    folder_definitions_synced = def_result.get('upserted', 0)
                    logger.info(f"âœ… Folder custom attribute definitions synced: {folder_definitions_synced}")
                    
                except Exception as e:
                    logger.error(f"Failed to sync folder custom attribute definitions: {e}")
            
            return {
                'synced': folders_synced,
                'folder_custom_attrs_synced': folder_definitions_synced
            }
            
        except Exception as e:
            logger.error(f"Error in batch folder processing: {e}")
            return {'synced': 0}
    
    async def _batch_process_files_async(self, files: List, project_id: str) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰¹é‡å¤„ç†æ–‡ä»¶"""
        try:
            from database.data_sync_strategy import DataTransformer
            
            dal = await get_optimized_postgresql_dal()
            transformer = DataTransformer()
            
            # Transform file data
            files_data = []
            versions_data = []
            custom_attrs_definitions = []
            custom_attrs_values = []
            
            for file_api_data in files:
                try:
                    file_record = transformer.transform_file_data(
                        file_api_data, project_id, None, "", 0
                    )
                    files_data.append(file_record)
                    
                    # ğŸ”‘ Process custom attributes from enriched file data
                    custom_attrs = file_api_data.get('custom_attributes', [])
                    if custom_attrs:
                        logger.info(f"DEBUG: Processing {len(custom_attrs)} custom attributes for {file_record.get('name')}")
                        
                        for attr in custom_attrs:
                            # Attribute definition
                            attr_def = {
                                'attr_id': attr.get('id'),
                                'project_id': project_id,
                                'name': attr.get('name'),
                                'type': attr.get('type'),
                                'array_values': attr.get('arrayValues', [])
                            }
                            custom_attrs_definitions.append(attr_def)
                            
                            # Attribute value
                            attr_value = {
                                'file_id': file_record['id'],
                                'attr_id': attr.get('id'),
                                'project_id': project_id,
                                'value': attr.get('value'),
                                'value_date': self._parse_date_value(attr) if attr.get('type') == 'date' else None,
                                'value_number': self._parse_number_value(attr) if attr.get('type') == 'number' else None,
                                'value_boolean': self._parse_boolean_value(attr) if attr.get('type') == 'boolean' else None
                            }
                            custom_attrs_values.append(attr_value)
                    
                    # ğŸ”‘ Create file version records from detailed version info
                    versions_info = file_api_data.get('versions_info', [])
                    if versions_info:
                        logger.info(f"DEBUG: Creating {len(versions_info)} versions for {file_record.get('name')}")
                        
                        for version in versions_info:
                            version_id = version.get('id')
                            if version_id:
                                # è·å–è¯¦ç»†å±æ€§ï¼ˆå‚è€ƒMongoDBå®ç°ï¼‰
                                version_attrs = version.get('attributes', {})
                                detailed_attrs = version.get('detailed_attributes', {})
                                
                                # ä¼˜å…ˆä½¿ç”¨è¯¦ç»†ä¿¡æ¯ï¼Œå›é€€åˆ°åŸºæœ¬ä¿¡æ¯
                                file_size = (
                                    detailed_attrs.get('storageSize', 0) or
                                    version_attrs.get('storageSize', 0) or
                                    0
                                )
                                
                                mime_type = (
                                    detailed_attrs.get('mimeType') or
                                    version_attrs.get('mimeType')
                                )
                                
                                storage_urn = (
                                    detailed_attrs.get('storageUrn') or
                                    version_attrs.get('storageUrn')
                                )
                                
                                version_record = {
                                    'id': version_id,
                                    'project_id': project_id,
                                    'file_id': file_record['id'],
                                    'version_number': version_attrs.get('versionNumber', 1),
                                    'version_type': 'tip' if version == versions_info[0] else 'historical',
                                    'create_time': self._parse_datetime(version_attrs.get('createTime')),
                                    'create_user_id': version_attrs.get('createUserId'),
                                    'create_user_name': version_attrs.get('createUserName'),
                                    'file_size': file_size,
                                    'storage_urn': storage_urn,
                                    'mime_type': mime_type,
                                    'metadata': {
                                        'process_state': detailed_attrs.get('processState'),
                                        'download_url': detailed_attrs.get('downloadUrl'),
                                        'enhanced_version': True
                                    }
                                }
                                versions_data.append(version_record)
                                
                                # æ›´æ–°æ–‡ä»¶è®°å½•çš„ä¿¡æ¯ï¼ˆä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ä¿¡æ¯ï¼‰
                                if version == versions_info[0]:  # æœ€æ–°ç‰ˆæœ¬
                                    file_record.update({
                                        'file_size': file_size,
                                        'mime_type': mime_type,
                                        'storage_urn': storage_urn,
                                        'process_state': detailed_attrs.get('processState'),
                                        'download_url': detailed_attrs.get('downloadUrl')
                                    })
                    else:
                        # å›é€€åˆ°åŸæœ‰é€»è¾‘ï¼ˆå¦‚æœæ²¡æœ‰è¯¦ç»†ç‰ˆæœ¬ä¿¡æ¯ï¼‰
                        tip_version_urn = file_record.get('tip_version_urn')
                        if tip_version_urn:
                            logger.info(f"DEBUG: Creating fallback version for {file_record.get('name')}")
                            version_record = {
                                'id': tip_version_urn,
                                'project_id': project_id,
                                'file_id': file_record['id'],
                                'version_number': 1,
                                'version_type': 'tip',
                                'create_time': file_record.get('create_time'),
                                'create_user_id': file_record.get('create_user_id'),
                                'create_user_name': file_record.get('create_user_name'),
                                'file_size': file_record.get('file_size', 0),
                                'storage_urn': file_record.get('storage_location'),
                                'mime_type': file_record.get('mime_type'),
                                'metadata': {
                                    'is_tip_version': True,
                                    'fallback_version': True
                                }
                            }
                            versions_data.append(version_record)
                    
                except Exception as e:
                    logger.warning(f"Failed to transform file data: {e}")
                    continue
            
            # Batch insert files to database
            if files_data:
                result = await dal.batch_upsert_files(files_data)
                self.stats['batch_operations'] += 1
                
                # ğŸ”‘ Batch insert file versions
                if versions_data:
                    logger.info(f"DEBUG: Creating {len(versions_data)} file versions...")
                    try:
                        versions_result = await dal.batch_upsert_file_versions(versions_data)
                        logger.info(f"âœ… File versions created: {versions_result.get('upserted', 0)}")
                    except Exception as e:
                        logger.error(f"Failed to create file versions: {e}")
                
                # ğŸ”‘ Batch insert custom attributes
                custom_attrs_synced = 0
                if custom_attrs_definitions:
                    logger.info(f"DEBUG: Processing {len(custom_attrs_definitions)} custom attribute definitions...")
                    try:
                        # Remove duplicates from definitions
                        unique_definitions = self._deduplicate_definitions(custom_attrs_definitions)
                        def_result = await dal.batch_upsert_custom_attribute_definitions(unique_definitions)
                        logger.info(f"âœ… Custom attribute definitions created: {def_result.get('upserted', 0)}")
                        
                        # Insert attribute values
                        if custom_attrs_values:
                            logger.info(f"DEBUG: Creating {len(custom_attrs_values)} custom attribute values...")
                            value_result = await dal.batch_upsert_custom_attribute_values(custom_attrs_values)
                            custom_attrs_synced = value_result.get('upserted', 0)
                            logger.info(f"âœ… Custom attribute values created: {custom_attrs_synced}")
                        
                    except Exception as e:
                        logger.error(f"Failed to create custom attributes: {e}")
                
                return {
                    'synced': result.get('upserted', 0),
                    'custom_attrs_synced': custom_attrs_synced
                }
            
            return {'synced': 0}
            
        except Exception as e:
            logger.error(f"Error in batch file processing: {e}")
            return {'synced': 0}
    
    async def _batch_process_custom_attributes_async(self, files: List, project_id: str, headers: dict) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰¹é‡å¤„ç†è‡ªå®šä¹‰å±æ€§"""
        try:
            # For now, return 0 as custom attributes require additional API setup
            # In a full implementation, this would:
            # 1. Extract file IDs from the files list
            # 2. Call Custom Attributes API in batches
            # 3. Transform and store the attributes
            logger.info("Custom attributes processing skipped (requires additional API setup)")
            return {'synced': 0}
            
        except Exception as e:
            logger.error(f"Error in custom attributes processing: {e}")
            return {'synced': 0}
    
    def _parse_datetime(self, datetime_str):
        """è§£æAPIè¿”å›çš„æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
        if not datetime_str:
            return None
        
        try:
            from datetime import datetime
            # ACC APIé€šå¸¸è¿”å›ISOæ ¼å¼çš„æ—¶é—´æˆ³
            if datetime_str.endswith('Z'):
                # UTCæ—¶é—´æˆ³
                return datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
            else:
                # å°è¯•ç›´æ¥è§£æ
                return datetime.fromisoformat(datetime_str)
        except Exception as e:
            logger.warning(f"è§£ææ—¥æœŸæ—¶é—´å¤±è´¥: {datetime_str}, {str(e)}")
            return None

    def _calculate_optimization_efficiency(self, folders_count: int, files_count: int, duration: float) -> float:
        """è®¡ç®—ä¼˜åŒ–æ•ˆç‡"""
        try:
            # Simple efficiency calculation based on items per second
            total_items = folders_count + files_count
            if total_items == 0 or duration == 0:
                return 100.0
            
            items_per_second = total_items / duration
            
            # Efficiency scale: 
            # > 10 items/sec = 95%+
            # > 5 items/sec = 85%+  
            # > 1 item/sec = 70%+
            # < 1 item/sec = lower
            
            if items_per_second >= 10:
                return min(95.0 + (items_per_second - 10) * 0.5, 100.0)
            elif items_per_second >= 5:
                return 85.0 + (items_per_second - 5) * 2.0
            elif items_per_second >= 1:
                return 70.0 + (items_per_second - 1) * 3.75
            else:
                return max(50.0, 70.0 * items_per_second)
                
        except Exception:
            return 75.0  # Default efficiency
    
    # ============================================================================
    # è¾…åŠ©æ–¹æ³• - å…¶ä»–
    # ============================================================================
    
    def _get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            'api_calls': self.stats['api_calls'],
            'api_calls_saved': self.stats['api_calls_saved'],
            'smart_skips': self.stats['smart_skips'],
            'batch_operations': self.stats['batch_operations'],
            'concurrent_operations': self.stats['concurrent_operations'],
            'memory_peak_mb': self.stats['memory_peak_mb'],
            'processing_time_seconds': round(self.stats['processing_time'], 2)
        }
    
    
    def _get_last_sync_time(self, project_id: str) -> datetime:
        """è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰"""
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–ï¼Œæš‚æ—¶è¿”å›ä¸€ä¸ªé»˜è®¤å€¼
        return datetime.utcnow() - timedelta(hours=1)
    
    def _deduplicate_definitions(self, definitions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡å±æ€§å®šä¹‰"""
        seen = set()
        unique_definitions = []

        for definition in definitions:
            # Use attr_id, project_id, and scope_folder_id for deduplication
            # scope_folder_id can be None for project-level attributes
            key = (
                definition.get('attr_id'),
                definition.get('project_id'),
                definition.get('scope_folder_id')  # åŒ…å«æ–‡ä»¶å¤¹IDä»¥åŒºåˆ†ä¸åŒæ–‡ä»¶å¤¹çš„å±æ€§å®šä¹‰
            )
            if key not in seen:
                seen.add(key)
                unique_definitions.append(definition)

        return unique_definitions
    
    async def _get_attr_definition_id_mapping(self, definitions: List[Dict], dal) -> Dict[int, int]:
        """è·å–ACC APIå±æ€§IDåˆ°æ•°æ®åº“å®šä¹‰IDçš„æ˜ å°„"""
        mapping = {}
        
        try:
            async with dal.get_connection() as conn:
                for definition in definitions:
                    acc_attr_id = definition.get('attr_id')
                    project_id = definition.get('project_id')
                    
                    if acc_attr_id and project_id:
                        # æŸ¥è¯¢æ•°æ®åº“ä¸­å¯¹åº”çš„å®šä¹‰ID
                        result = await conn.fetchrow("""
                            SELECT id FROM custom_attribute_definitions 
                            WHERE attr_id = $1 AND project_id = $2
                            LIMIT 1
                        """, acc_attr_id, project_id)
                        
                        if result:
                            mapping[acc_attr_id] = result['id']
                            
        except Exception as e:
            logger.error(f"Failed to get attribute definition ID mapping: {e}")
            
        return mapping
    
    # ============================================================================
    # ğŸš€ V2æ¶æ„ä¸“ç”¨åŒæ­¥æ–¹æ³•
    # ============================================================================
    
    async def _optimized_incremental_sync_v2(self, project_id: str, max_depth: int = 10, 
                                           include_custom_attributes: bool = True, 
                                           task_uuid: str = None, headers: dict = None) -> Dict[str, Any]:
        """V2æ¶æ„çš„ä¼˜åŒ–å¢é‡åŒæ­¥"""
        
        logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–å¢é‡åŒæ­¥ V2: é¡¹ç›® {project_id}")
        start_time = time.time()
        
        try:
            # é‡ç½®ç»Ÿè®¡
            self.stats = {key: 0 for key in self.stats}
            
            dal = await get_optimized_postgresql_dal()
            
            # ğŸ”‘ è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            last_sync_time = await dal.get_project_last_sync_time(project_id)
            if not last_sync_time:
                logger.info("æœªæ‰¾åˆ°ä¸Šæ¬¡åŒæ­¥æ—¶é—´ï¼Œæ‰§è¡Œå…¨é‡åŒæ­¥")
                return await self._optimized_full_sync_v2(project_id, max_depth, include_custom_attributes, task_uuid, headers)
            
            logger.info(f"ä¸Šæ¬¡åŒæ­¥æ—¶é—´: {last_sync_time}")
            
            # ğŸš€ Layer 1: æ™ºèƒ½åˆ†æ”¯è·³è¿‡ (V2ç‰ˆæœ¬)
            folders_to_check = await self._smart_branch_filtering_v2(project_id, last_sync_time, headers)
            
            if not folders_to_check:
                logger.info("âœ… æ™ºèƒ½è·³è¿‡ï¼šé¡¹ç›®æ— å˜åŒ–")
                return {
                    'status': 'no_changes',
                    'folders_synced': 0,
                    'files_synced': 0,
                    'custom_attrs_synced': 0,
                    'performance_stats': self._get_performance_stats(),
                    'optimization_efficiency': 100.0,
                    'architecture_version': 'v2'
                }
            
            # ğŸš€ Layer 2: æ‰¹é‡APIè°ƒç”¨ (V2ç‰ˆæœ¬)
            changed_folders, changed_files = await self._batch_api_operations_v2(project_id, folders_to_check, headers)
            
            logger.info(f"ğŸ“Š APIæ‰¹é‡æ“ä½œå®Œæˆ: {len(changed_folders)} æ–‡ä»¶å¤¹, {len(changed_files)} æ–‡ä»¶")
            
            # ğŸš€ Layer 3: æ–‡ä»¶çº§timestampæ¯”å¯¹å’Œæ‰¹é‡æ ‡è®° (V2ç‰ˆæœ¬)
            files_needing_updates = await self._identify_files_needing_updates_v2(
                changed_files, project_id, last_sync_time, dal
            )
            
            logger.info(f"ğŸ¯ æ–‡ä»¶æ›´æ–°åˆ†æ: {len(files_needing_updates)} ä¸ªæ–‡ä»¶éœ€è¦æ›´æ–°")
            
            # è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬è‡ªå®šä¹‰å±æ€§ï¼‰- V2ç‰ˆæœ¬
            # åªå¤„ç†éœ€è¦æ›´æ–°çš„æ–‡ä»¶
            files_to_process = []
            if files_needing_updates:
                # ä»éœ€è¦æ›´æ–°çš„æ–‡ä»¶ä¸­æå–æ–‡ä»¶æ•°æ®
                files_to_process = [item['file_data'] for item in files_needing_updates if 'file_data' in item]
                
                if include_custom_attributes and files_to_process:
                    files_to_process = await self._batch_get_file_versions_and_custom_attrs_v2(project_id, files_to_process, headers)
            
            # ğŸš€ Layer 4: æ‰¹é‡æ•°æ®åº“æ“ä½œ (V2ç‰ˆæœ¬)
            folders_synced = 0
            files_synced = 0
            custom_attrs_synced = 0
            
            if changed_folders:
                folders_synced = await self._batch_insert_folders_v2(changed_folders, dal)
            
            if files_to_process:
                # è½¬æ¢æ–‡ä»¶æ•°æ®ä¸ºV2æ ¼å¼
                v2_files_data = []
                for file_data in files_to_process:
                    custom_attrs = file_data.get('custom_attributes', {})
                    v2_file = {
                        'id': file_data.get('id'),
                        'project_id': file_data.get('project_id'),
                        'name': file_data.get('name') or custom_attrs.get('name'),
                        'display_name': file_data.get('display_name') or custom_attrs.get('title'),
                        'parent_folder_id': file_data.get('parent_folder_id'),
                        'folder_path': file_data.get('folder_path', ''),
                        'full_path': file_data.get('full_path', ''),
                        'path_segments': file_data.get('path_segments', []),
                        'depth': file_data.get('depth', 0),
                        'create_time': custom_attrs.get('createTime'),
                        'create_user_id': custom_attrs.get('createUserId'),
                        'create_user_name': custom_attrs.get('createUserName'),
                        'last_modified_time': custom_attrs.get('lastModifiedTime'),
                        'last_modified_user_id': custom_attrs.get('lastModifiedUserId'),
                        'last_modified_user_name': custom_attrs.get('lastModifiedUserName'),
                        'file_type': custom_attrs.get('name', '').split('.')[-1] if custom_attrs.get('name') else '',
                        'mime_type': '',
                        'reserved': False,
                        'hidden': False,
                        'metadata': {},
                        'file_permissions': {},
                        'file_settings': {},
                        'review_info': {},
                        'sync_info': {'synced_at': datetime.now().isoformat()}
                    }
                    
                    # è½¬æ¢æ—¶é—´æˆ³å­—æ®µä¸ºä¸­å›½æ—¶åŒº
                    timestamp_fields = ['create_time', 'last_modified_time']
                    v2_file = self._batch_convert_timestamps_to_china(v2_file, timestamp_fields)
                    v2_files_data.append(v2_file)
                
                files_synced = await self._batch_insert_files_v2(v2_files_data, dal)
                
                # æ’å…¥æ–‡ä»¶ç‰ˆæœ¬ (V2æ¶æ„)
                await self._batch_insert_file_versions_v2(files_to_process, dal)
            
            if include_custom_attributes and files_to_process:
                custom_attrs_synced = await self._batch_insert_custom_attributes_v2(files_to_process, dal)
            
            # åªæœ‰åœ¨å®é™…åŒæ­¥äº†å†…å®¹æ—¶æ‰æ›´æ–°åŒæ­¥çŠ¶æ€
            if folders_synced > 0 or files_synced > 0 or custom_attrs_synced > 0:
                await self._update_project_sync_status(project_id, dal)
            
            # è®¡ç®—ç»“æœ
            duration = time.time() - start_time
            optimization_efficiency = self._calculate_optimization_efficiency(folders_synced, files_synced, duration)
            
            result = {
                'status': 'success',
                'message': 'V2 Incremental sync completed successfully',
                'folders_synced': folders_synced,
                'files_synced': files_synced,
                'custom_attrs_synced': custom_attrs_synced,
                'files_needing_updates': len(files_needing_updates),
                'duration_seconds': round(duration, 2),
                'optimization_efficiency': optimization_efficiency,
                'performance_stats': self._get_performance_stats(),
                'architecture_version': 'v2'
            }
            
            logger.info(f"âœ… V2å¢é‡åŒæ­¥å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ V2å¢é‡åŒæ­¥å¤±è´¥: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'duration_seconds': time.time() - start_time,
                'architecture_version': 'v2'
            }
    
    async def _smart_branch_filtering_v2(self, project_id: str, last_sync_time: datetime, 
                                       headers: dict) -> List[Dict[str, Any]]:
        """V2æ¶æ„çš„æ™ºèƒ½åˆ†æ”¯è¿‡æ»¤"""
        return await self._smart_branch_filtering(project_id, last_sync_time, headers)
    
    async def _batch_api_operations_v2(self, project_id: str, folders_to_check: List[Dict], 
                                     headers: dict) -> tuple:
        """V2æ¶æ„çš„æ‰¹é‡APIæ“ä½œ"""
        return await self._batch_api_operations(project_id, folders_to_check, headers)
    
    async def _identify_files_needing_updates_v2(self, changed_files: List[Dict], 
                                               project_id: str, last_sync_time: datetime, 
                                               dal) -> List[Dict]:
        """V2æ¶æ„çš„æ–‡ä»¶æ›´æ–°è¯†åˆ«"""
        return await self._identify_files_needing_updates(changed_files, project_id, last_sync_time, dal)
    
    async def _optimized_full_sync_v2(self, project_id: str, max_depth: int = 10, 
                                    include_custom_attributes: bool = True, 
                                    task_uuid: str = None, headers: dict = None) -> Dict[str, Any]:
        """V2æ¶æ„çš„ä¼˜åŒ–å…¨é‡åŒæ­¥"""
        
        logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–å…¨é‡åŒæ­¥ V2: é¡¹ç›® {project_id}")
        start_time = time.time()
        
        try:
            # 1. æ¸…é™¤ç°æœ‰é¡¹ç›®æ•°æ®
            dal = await get_optimized_postgresql_dal()
            
            async with dal.get_connection() as conn:
                # Clear existing project data - first count, then delete
                deleted_attrs = await conn.fetchval("SELECT COUNT(*) FROM custom_attribute_values WHERE project_id = $1", project_id) or 0
                await conn.execute("DELETE FROM custom_attribute_values WHERE project_id = $1", project_id)
                
                deleted_defs = await conn.fetchval("SELECT COUNT(*) FROM custom_attribute_definitions WHERE project_id = $1", project_id) or 0
                await conn.execute("DELETE FROM custom_attribute_definitions WHERE project_id = $1", project_id)
                
                deleted_versions = await conn.fetchval("SELECT COUNT(*) FROM file_versions WHERE project_id = $1", project_id) or 0
                await conn.execute("DELETE FROM file_versions WHERE project_id = $1", project_id)
                
                deleted_files = await conn.fetchval("SELECT COUNT(*) FROM files WHERE project_id = $1", project_id) or 0
                await conn.execute("DELETE FROM files WHERE project_id = $1", project_id)
                
                deleted_folders = await conn.fetchval("SELECT COUNT(*) FROM folders WHERE project_id = $1", project_id) or 0
                await conn.execute("DELETE FROM folders WHERE project_id = $1", project_id)
                
            logger.info(f"ğŸ§¹ æ•°æ®æ¸…ç†å®Œæˆ: æ–‡ä»¶å¤¹({deleted_folders}), æ–‡ä»¶({deleted_files}), ç‰ˆæœ¬({deleted_versions}), å±æ€§å®šä¹‰({deleted_defs}), å±æ€§å€¼({deleted_attrs})")
            
            # 2. æ£€æŸ¥è®¤è¯å¤´
            if not headers:
                logger.error("Missing authentication headers")
                return {
                    'status': 'error',
                    'error': 'Missing authentication headers'
                }
            
            # 3. è·å–é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹
            logger.info(f"ğŸ“ è·å–é¡¹ç›®é¡¶çº§æ–‡ä»¶å¤¹...")
            
            try:
                # Call real ACC API to get top-level folders
                top_folders_data = await self._get_top_folders_async(project_id, headers)
                if not top_folders_data or not top_folders_data.get('data'):
                    logger.warning(f"No top-level folders found for project {project_id}")
                    return {
                        'status': 'success',
                        'message': 'No folders found in project',
                        'folders_synced': 0,
                        'files_synced': 0,
                        'custom_attrs_synced': 0,
                        'performance_stats': self.stats
                    }
                
                top_folders = top_folders_data.get('data', [])
                logger.info(f"ğŸ“‚ æ‰¾åˆ° {len(top_folders)} ä¸ªé¡¶çº§æ–‡ä»¶å¤¹")
                
                # 4. BFSé€’å½’æ”¶é›†æ‰€æœ‰æ•°æ®
                logger.info("ğŸ”„ å¼€å§‹BFSé€’å½’æ”¶é›†æ•°æ®...")
                all_folders, all_files, all_custom_attrs = await self._bfs_collect_all_data_v2(
                    project_id, top_folders, max_depth, include_custom_attributes, headers
                )
                
                # 5. è·å–æ–‡ä»¶ç‰ˆæœ¬å’Œè‡ªå®šä¹‰å±æ€§è¯¦ç»†ä¿¡æ¯
                if all_files:
                    logger.info("Getting file versions and custom attributes...")
                    enriched_files = await self._batch_get_file_versions_and_custom_attrs_v2(
                        project_id, all_files, headers
                    )
                    all_files = enriched_files
                
                # 5. æ‰¹é‡æ’å…¥æ•°æ® - ä½¿ç”¨V2æ¶æ„
                logger.info("ğŸ’¾ å¼€å§‹æ‰¹é‡æ•°æ®åº“æ“ä½œ...")
                
                # 5.1 æ’å…¥æ–‡ä»¶å¤¹
                folders_synced = await self._batch_insert_folders_v2(all_folders, dal)
                
                # 5.2 æ’å…¥æ–‡ä»¶ (ä½¿ç”¨V2å­—æ®µ)
                # è½¬æ¢æ–‡ä»¶æ•°æ®ä¸ºV2æ ¼å¼
                v2_files_data = []
                for file_data in all_files:
                    # ä½¿ç”¨è‡ªå®šä¹‰å±æ€§ä¸­çš„ä¿¡æ¯æ¥å¡«å……V2å­—æ®µ
                    custom_attrs = file_data.get('custom_attributes', {})
                    
                    v2_file = {
                        'id': file_data.get('id'),
                        'project_id': file_data.get('project_id'),
                        'name': file_data.get('name') or custom_attrs.get('name'),
                        'display_name': file_data.get('display_name') or custom_attrs.get('title'),
                        'parent_folder_id': file_data.get('parent_folder_id'),
                        'folder_path': file_data.get('folder_path', ''),
                        'full_path': file_data.get('full_path', ''),
                        'path_segments': file_data.get('path_segments', []),
                        'depth': file_data.get('depth', 0),
                        'create_time': custom_attrs.get('createTime'),
                        'create_user_id': custom_attrs.get('createUserId'),
                        'create_user_name': custom_attrs.get('createUserName'),
                        'last_modified_time': custom_attrs.get('lastModifiedTime'),
                        'last_modified_user_id': custom_attrs.get('lastModifiedUserId'),
                        'last_modified_user_name': custom_attrs.get('lastModifiedUserName'),
                        'file_type': custom_attrs.get('name', '').split('.')[-1] if custom_attrs.get('name') else '',
                        'mime_type': '',
                        'reserved': False,
                        'hidden': False,
                        'metadata': {},
                        'file_permissions': {},
                        'file_settings': {},
                        'review_info': {},
                        'sync_info': {'synced_at': datetime.now().isoformat()}
                    }
                    
                    # è½¬æ¢æ—¶é—´æˆ³å­—æ®µä¸ºä¸­å›½æ—¶åŒº
                    timestamp_fields = ['create_time', 'last_modified_time']
                    v2_file = self._batch_convert_timestamps_to_china(v2_file, timestamp_fields)
                    v2_files_data.append(v2_file)
                
                files_synced = await self._batch_insert_files_v2(v2_files_data, dal)
                
                # 5.3 æ’å…¥æ–‡ä»¶ç‰ˆæœ¬ (é›†ä¸­ç®¡ç†ç‰ˆæœ¬ä¿¡æ¯)
                versions_synced = await self._batch_insert_file_versions_v2(all_files, dal)
                
                # 5.4 æ’å…¥è‡ªå®šä¹‰å±æ€§ (ä½¿ç”¨V2å…³è”è®¾è®¡)
                custom_attrs_synced = 0
                if include_custom_attributes and all_files:
                    custom_attrs_synced = await self._batch_insert_custom_attributes_v2(all_files, dal)

                # 5.5 æ’å…¥æ–‡ä»¶å¤¹è‡ªå®šä¹‰å±æ€§å®šä¹‰
                folder_attr_defs_synced = 0
                logger.info(f"ğŸ“‹ Total folder custom attribute definitions collected: {len(all_custom_attrs.get('definitions', []))}")
                if include_custom_attributes and all_custom_attrs.get('definitions'):
                    try:
                        definitions = all_custom_attrs['definitions']
                        logger.info(f"ğŸ“‹ Processing {len(definitions)} folder custom attribute definitions...")
                        # å»é‡
                        unique_definitions = self._deduplicate_definitions(definitions)
                        logger.info(f"ğŸ“‹ After deduplication: {len(unique_definitions)} unique definitions")
                        if unique_definitions:
                            result = await dal.batch_upsert_custom_attribute_definitions(unique_definitions)
                            folder_attr_defs_synced = result.get('upserted', 0)
                            logger.info(f"âœ… Folder custom attribute definitions synced: {folder_attr_defs_synced}")
                    except Exception as e:
                        logger.error(f"Failed to sync folder custom attribute definitions: {e}")
                        import traceback
                        logger.error(traceback.format_exc())

                # 6. æ›´æ–°é¡¹ç›®åŒæ­¥çŠ¶æ€
                await self._update_project_sync_status(project_id, dal)
                
                end_time = time.time()
                duration = end_time - start_time
                
                # 7. ç”Ÿæˆç»“æœæŠ¥å‘Š
                result = {
                    'status': 'success',
                    'folders_synced': folders_synced,
                    'files_synced': files_synced,
                    'versions_synced': versions_synced,
                    'custom_attrs_synced': custom_attrs_synced,
                    'total_time_seconds': round(duration, 2),
                    'performance_stats': self.stats,
                    'architecture_version': 'v2'
                }
                
                logger.info(f"âœ… V2å…¨é‡åŒæ­¥å®Œæˆ: {result}")
                return result
                
            except Exception as e:
                logger.error(f"V2åŒæ­¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                return {
                    'status': 'error',
                    'error': str(e),
                    'folders_synced': 0,
                    'files_synced': 0,
                    'custom_attrs_synced': 0,
                    'architecture_version': 'v2'
                }
                
        except Exception as e:
            logger.error(f"âŒ V2å…¨é‡åŒæ­¥å¤±è´¥: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'folders_synced': 0,
                'files_synced': 0,
                'custom_attrs_synced': 0,
                'architecture_version': 'v2'
            }
    
    async def _bfs_collect_all_data_v2(self, project_id: str, top_folders: List[Dict], 
                                     max_depth: int, include_custom_attributes: bool, 
                                     headers: dict) -> Tuple[List[Dict], List[Dict], Dict]:
        """BFSæ”¶é›†æ‰€æœ‰æ•°æ® - V2ä¼˜åŒ–ç‰ˆæœ¬"""
        
        all_folders = []
        all_files = []
        all_custom_attrs = {'definitions': [], 'values': []}
        
        try:
            import aiohttp
            
            async with aiohttp.ClientSession() as session:
                # BFSé˜Ÿåˆ—ï¼š(folder_data, depth, parent_path)
                queue = [(folder, 0, "") for folder in top_folders]
                
                while queue and len(queue) > 0:
                    current_folder, depth, parent_path = queue.pop(0)
                    
                    if depth >= max_depth:
                        continue
                    
                    folder_id = current_folder.get('id')
                    folder_name = current_folder.get('attributes', {}).get('name', 'Unknown')
                    current_path = f"{parent_path}/{folder_name}".strip('/')
                    
                    # è½¬æ¢æ–‡ä»¶å¤¹æ•°æ®ä¸ºV2æ ¼å¼
                    folder_data = self._transform_folder_data_v2(current_folder, project_id, parent_path, depth)
                    all_folders.append(folder_data)

                    # ğŸ”‘ æ”¶é›†æ–‡ä»¶å¤¹è‡ªå®šä¹‰å±æ€§å®šä¹‰
                    if include_custom_attributes:
                        try:
                            folder_defs = await self._get_folder_custom_attr_definitions_v2(
                                project_id, folder_id, headers, session
                            )
                            if folder_defs:
                                all_custom_attrs['definitions'].extend(folder_defs)
                                logger.info(f"ğŸ“‹ Found {len(folder_defs)} custom attribute definitions for folder {folder_name}")
                        except Exception as e:
                            logger.warning(f"Failed to get custom attributes for folder {folder_id}: {e}")
                    
                    # è·å–æ–‡ä»¶å¤¹å†…å®¹
                    contents_url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/folders/{folder_id}/contents"
                    
                    try:
                        async with session.get(contents_url, headers=headers) as response:
                            if response.status == 200:
                                contents_data = await response.json()
                                self.stats['api_calls'] += 1
                                
                                for item in contents_data.get('data', []):
                                    item_type = item.get('type')
                                    
                                    if item_type == 'folders':
                                        # æ·»åŠ å­æ–‡ä»¶å¤¹åˆ°é˜Ÿåˆ—
                                        queue.append((item, depth + 1, current_path))
                                    
                                    elif item_type == 'items':
                                        # è½¬æ¢æ–‡ä»¶æ•°æ®ä¸ºV2æ ¼å¼
                                        file_data = self._transform_file_data_v2(item, project_id, current_path, depth + 1)
                                        all_files.append(file_data)
                            
                            else:
                                logger.warning(f"Failed to get folder contents: {response.status}")
                                
                    except Exception as e:
                        logger.error(f"Error getting folder contents for {folder_name}: {e}")
                        continue
                
                # æ–‡ä»¶å¤¹è‡ªå®šä¹‰å±æ€§å·²åœ¨BFSéå†ä¸­æ”¶é›†åˆ°all_custom_attrs['definitions']
                logger.info(f"ğŸ“Š BFSæ”¶é›†å®Œæˆ: {len(all_folders)} æ–‡ä»¶å¤¹, {len(all_files)} æ–‡ä»¶, {len(all_custom_attrs.get('definitions', []))} æ–‡ä»¶å¤¹å±æ€§å®šä¹‰")
                
                return all_folders, all_files, all_custom_attrs
                
        except ImportError:
            logger.error("aiohttp not available, cannot make API calls")
            return [], [], {}
        except Exception as e:
            logger.error(f"BFSæ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return [], [], {}
    
    def _transform_folder_data_v2(self, folder_data: Dict, project_id: str, parent_path: str, depth: int) -> Dict:
        """è½¬æ¢æ–‡ä»¶å¤¹æ•°æ®ä¸ºV2æ ¼å¼ï¼Œå¹¶è½¬æ¢æ—¶åŒºä¸ºä¸­å›½æ—¶åŒº"""
        attributes = folder_data.get('attributes', {})
        
        # åŸºç¡€æ•°æ®
        folder_record = {
            'id': folder_data.get('id'),
            'project_id': project_id,
            'name': attributes.get('name'),
            'display_name': attributes.get('displayName'),
            'parent_id': folder_data.get('relationships', {}).get('parent', {}).get('data', {}).get('id'),
            'path': f"{parent_path}/{attributes.get('name')}".strip('/'),
            'path_segments': f"{parent_path}/{attributes.get('name')}".strip('/').split('/'),
            'depth': depth,
            'create_time': attributes.get('createTime'),
            'create_user_id': attributes.get('createUserId'),
            'create_user_name': attributes.get('createUserName'),
            'last_modified_time': attributes.get('lastModifiedTime'),
            'last_modified_user_id': attributes.get('lastModifiedUserId'),
            'last_modified_user_name': attributes.get('lastModifiedUserName'),
            'last_modified_time_rollup': attributes.get('lastModifiedTime'),  # åˆå§‹è®¾ä¸ºç›¸åŒå€¼
            'object_count': attributes.get('objectCount', 0),
            'total_size': 0,
            'hidden': attributes.get('hidden', False),
            'metadata': attributes.get('extension', {}),
            'extension': attributes.get('extension', {}),
            'children_stats': {},
            'sync_info': {'synced_at': datetime.now().isoformat()}
        }
        
        # æ‰¹é‡è½¬æ¢æ—¶é—´æˆ³å­—æ®µä¸ºä¸­å›½æ—¶åŒº
        timestamp_fields = ['create_time', 'last_modified_time', 'last_modified_time_rollup']
        return self._batch_convert_timestamps_to_china(folder_record, timestamp_fields)
    
    def _transform_file_data_v2(self, file_data: Dict, project_id: str, folder_path: str, depth: int) -> Dict:
        """è½¬æ¢æ–‡ä»¶æ•°æ®ä¸ºV2æ ¼å¼ï¼Œå¹¶è½¬æ¢æ—¶åŒºä¸ºä¸­å›½æ—¶åŒº"""
        attributes = file_data.get('attributes', {})
        
        # åŸºç¡€æ•°æ®
        file_record = {
            'id': file_data.get('id'),
            'project_id': project_id,
            'name': attributes.get('name'),
            'display_name': attributes.get('displayName'),
            'parent_folder_id': file_data.get('relationships', {}).get('parent', {}).get('data', {}).get('id'),
            'folder_path': folder_path,
            'full_path': f"{folder_path}/{attributes.get('name')}".strip('/'),
            'path_segments': f"{folder_path}/{attributes.get('name')}".strip('/').split('/'),
            'depth': depth,
            'create_time': attributes.get('createTime'),
            'create_user_id': attributes.get('createUserId'),
            'create_user_name': attributes.get('createUserName'),
            'last_modified_time': attributes.get('lastModifiedTime'),
            'last_modified_user_id': attributes.get('lastModifiedUserId'),
            'last_modified_user_name': attributes.get('lastModifiedUserName'),
            'file_type': attributes.get('extension'),
            'mime_type': attributes.get('mimeType'),
            'reserved': attributes.get('reserved', False),
            'hidden': attributes.get('hidden', False),
            'metadata': attributes.get('extension', {}),
            'file_permissions': {},  # V2æ–°å­—æ®µ
            'file_settings': {},     # V2æ–°å­—æ®µ
            'review_info': {},       # V2æ–°å­—æ®µ
            'sync_info': {'synced_at': datetime.now().isoformat()}
        }
        
        # æ‰¹é‡è½¬æ¢æ—¶é—´æˆ³å­—æ®µä¸ºä¸­å›½æ—¶åŒº
        timestamp_fields = ['create_time', 'last_modified_time']
        return self._batch_convert_timestamps_to_china(file_record, timestamp_fields)
    
    async def _batch_insert_folders_v2(self, folders_data: List[Dict], dal) -> int:
        """æ‰¹é‡æ’å…¥æ–‡ä»¶å¤¹ - V2æ¶æ„"""
        if not folders_data:
            return 0
        
        try:
            result = await dal.batch_upsert_folders(folders_data)
            return result.get('upserted', 0)
        except Exception as e:
            logger.error(f"V2 folder batch insert failed: {e}")
            return 0
    
    async def _batch_insert_files_v2(self, files_data: List[Dict], dal) -> int:
        """æ‰¹é‡æ’å…¥æ–‡ä»¶ - V2æ¶æ„"""
        if not files_data:
            return 0
        
        try:
            result = await dal.batch_upsert_files(files_data)
            return result.get('upserted', 0)
        except Exception as e:
            logger.error(f"V2 file batch insert failed: {e}")
            return 0
    
    async def _batch_insert_file_versions_v2(self, files_data: List[Dict], dal) -> int:
        """æ‰¹é‡æ’å…¥æ–‡ä»¶ç‰ˆæœ¬ - V2æ¶æ„"""
        if not files_data:
            return 0
        
        try:
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‰ˆæœ¬è®°å½•
            versions_data = []
            for file_data in files_data:
                versions_info = file_data.get('versions_info', [])
                
                # å¤„ç†æ–‡ä»¶ç‰ˆæœ¬ä¿¡æ¯
                
                if versions_info:
                    # å¤„ç†ä»APIè·å–çš„çœŸå®ç‰ˆæœ¬ä¿¡æ¯
                    for i, version in enumerate(versions_info):
                        version_attrs = version.get('attributes', {})
                        detailed_attrs = version.get('detailed_attributes', {})
                        
                        # åŸºç¡€ç‰ˆæœ¬æ•°æ®
                        version_data = {
                            'id': version.get('id'),
                            'file_id': file_data.get('id'),
                            'project_id': file_data.get('project_id'),
                            'version_number': version_attrs.get('versionNumber', i + 1),
                            'urn': version.get('id'),
                            'item_urn': version.get('relationships', {}).get('item', {}).get('data', {}).get('id'),
                            'storage_urn': detailed_attrs.get('storageUrn'),
                            'lineage_urn': version_attrs.get('lineageUrn'),
                            'create_time': version_attrs.get('createTime'),
                            'create_user_id': version_attrs.get('createUserId'),
                            'create_user_name': version_attrs.get('createUserName'),
                            'last_modified_time': version_attrs.get('lastModifiedTime'),
                            'last_modified_user_id': version_attrs.get('lastModifiedUserId'),
                            'last_modified_user_name': version_attrs.get('lastModifiedUserName'),
                            'file_size': version_attrs.get('storageSize', 0),
                            'storage_size': detailed_attrs.get('storageSize', 0),
                            'mime_type': detailed_attrs.get('mimeType') or version_attrs.get('fileType'),
                            'process_state': detailed_attrs.get('processState'),
                            'download_url': detailed_attrs.get('downloadUrl'),
                            'is_current_version': i == 0,  # ç¬¬ä¸€ä¸ªç‰ˆæœ¬æ˜¯æœ€æ–°ç‰ˆæœ¬
                            'version_status': 'active',
                            'metadata': version_attrs.get('extension', {}),
                            'review_info': {},
                            'extension': version_attrs.get('extension', {}),
                            'download_info': {'downloadUrl': detailed_attrs.get('downloadUrl')},
                            'sync_info': {'synced_at': datetime.now().isoformat()}
                        }
                        
                        # è½¬æ¢æ—¶é—´æˆ³å­—æ®µä¸ºä¸­å›½æ—¶åŒº
                        timestamp_fields = ['create_time', 'last_modified_time']
                        version_data = self._batch_convert_timestamps_to_china(version_data, timestamp_fields)
                        versions_data.append(version_data)
                else:
                    # å¦‚æœæ²¡æœ‰ç‰ˆæœ¬ä¿¡æ¯ï¼Œåˆ›å»ºé»˜è®¤ç‰ˆæœ¬
                    version_data = {
                        'id': f"{file_data.get('id')}_v1",
                        'file_id': file_data.get('id'),
                        'project_id': file_data.get('project_id'),
                        'version_number': 1,
                        'urn': f"{file_data.get('id')}_v1",
                        'create_time': file_data.get('create_time'),
                        'create_user_id': file_data.get('create_user_id'),
                        'create_user_name': file_data.get('create_user_name'),
                        'last_modified_time': file_data.get('last_modified_time'),
                        'last_modified_user_id': file_data.get('last_modified_user_id'),
                        'last_modified_user_name': file_data.get('last_modified_user_name'),
                        'mime_type': file_data.get('mime_type'),
                        'is_current_version': True,
                        'version_status': 'active',
                        'metadata': file_data.get('metadata', {}),
                        'review_info': file_data.get('review_info', {}),
                        'sync_info': file_data.get('sync_info', {})
                    }
                    
                    # è½¬æ¢æ—¶é—´æˆ³å­—æ®µä¸ºä¸­å›½æ—¶åŒº
                    timestamp_fields = ['create_time', 'last_modified_time']
                    version_data = self._batch_convert_timestamps_to_china(version_data, timestamp_fields)
                    versions_data.append(version_data)
            
            if versions_data:
                result = await dal.batch_upsert_file_versions(versions_data)
                logger.info(f"File versions sync completed: {result.get('upserted', 0)} versions")
                return result.get('upserted', 0)
            else:
                return 0
        except Exception as e:
            logger.error(f"V2 file versions batch insert failed: {e}")
            return 0
    
    async def _batch_insert_custom_attributes_v2(self, files_data: List[Dict], dal) -> int:
        """æ‰¹é‡æ’å…¥è‡ªå®šä¹‰å±æ€§ - V2æ¶æ„"""
        if not files_data:
            return 0
        
        try:
            custom_attrs_definitions = []
            custom_attrs_values = []
            
            # ä»æ–‡ä»¶æ•°æ®ä¸­æå–è‡ªå®šä¹‰å±æ€§
            for file_data in files_data:
                custom_attrs_info = file_data.get('custom_attributes')
                
                # å¤„ç†è‡ªå®šä¹‰å±æ€§ä¿¡æ¯
                
                if custom_attrs_info and custom_attrs_info.get('customAttributes'):
                    custom_attrs = custom_attrs_info.get('customAttributes', [])
                    
                    for attr in custom_attrs:
                        # å±æ€§å®šä¹‰ - æ¸…ç†ç¼–ç é—®é¢˜å¹¶æ·»åŠ V2å­—æ®µ
                        attr_def = {
                            'attr_id': attr.get('id'),
                            'project_id': file_data.get('project_id'),
                            'name': str(attr.get('name', '')).encode('ascii', errors='ignore').decode('ascii') if attr.get('name') else '',
                            'type': attr.get('type'),
                            'array_values': attr.get('arrayValues', []),
                            'description': str(attr.get('description', '')).encode('ascii', errors='ignore').decode('ascii') if attr.get('description') else None,
                            'is_required': attr.get('isRequired', False),
                            'default_value': str(attr.get('defaultValue', '')).encode('ascii', errors='ignore').decode('ascii') if attr.get('defaultValue') else None,
                            'scope_type': 'project',  # V2 field
                            'scope_folder_id': None,  # V2 field
                            'inherit_to_subfolders': True,  # V2 field
                            'validation_rules': {},  # V2 field
                            'sync_info': {'synced_at': datetime.now().isoformat()}
                        }
                        custom_attrs_definitions.append(attr_def)
                        
                        # å±æ€§å€¼ - ä½¿ç”¨V2å­—æ®µæ˜ å°„
                        attr_value = {
                            'file_id': file_data.get('id'),
                            'attr_definition_id': attr.get('id'),  # V2 field (renamed from attr_id)
                            'project_id': file_data.get('project_id'),
                            'value': str(attr.get('value', '')).encode('ascii', errors='ignore').decode('ascii') if attr.get('value') else None,
                            'value_date': self._parse_date_value(attr) if attr.get('type') == 'date' else None,
                            'value_number': self._parse_number_value(attr) if attr.get('type') == 'number' else None,
                            'value_boolean': self._parse_boolean_value(attr) if attr.get('type') == 'boolean' else None,
                            'value_array': attr.get('arrayValues') if attr.get('type') == 'array' else None,  # V2 field
                            'validation_status': 'valid',  # V2 field
                            'validation_errors': [],  # V2 field
                            'updated_at': datetime.now(),
                            'sync_info': {'synced_at': datetime.now().isoformat()}
                        }
                        custom_attrs_values.append(attr_value)
            
            total_synced = 0
            
            logger.info(f"Found {len(custom_attrs_definitions)} attribute definitions and {len(custom_attrs_values)} attribute values")
            
            # æ’å…¥å±æ€§å®šä¹‰å¹¶è·å–IDæ˜ å°„
            attr_id_mapping = {}  # ACC API attr_id -> database definition id
            if custom_attrs_definitions:
                # å»é‡å±æ€§å®šä¹‰
                unique_definitions = self._deduplicate_definitions(custom_attrs_definitions)
                def_result = await dal.batch_upsert_custom_attribute_definitions(unique_definitions)
                logger.info(f"Custom attribute definitions sync completed: {def_result.get('upserted', 0)} definitions")
                
                # è·å–å±æ€§å®šä¹‰çš„IDæ˜ å°„
                attr_id_mapping = await self._get_attr_definition_id_mapping(unique_definitions, dal)
                logger.info(f"Retrieved {len(attr_id_mapping)} attribute ID mappings")
            
            # æ›´æ–°å±æ€§å€¼ä¸­çš„attr_definition_idå¹¶æ’å…¥
            if custom_attrs_values and attr_id_mapping:
                # æ›´æ–°å±æ€§å€¼ä¸­çš„attr_definition_id
                for attr_value in custom_attrs_values:
                    acc_attr_id = attr_value.get('attr_definition_id')  # è¿™æ˜¯ACC APIçš„ID
                    db_definition_id = attr_id_mapping.get(acc_attr_id)
                    if db_definition_id:
                        attr_value['attr_definition_id'] = db_definition_id
                    else:
                        logger.warning(f"No database ID found for ACC attribute ID: {acc_attr_id}")
                
                # è¿‡æ»¤æ‰æ²¡æœ‰æœ‰æ•ˆattr_definition_idçš„å€¼
                valid_values = [v for v in custom_attrs_values if v.get('attr_definition_id')]
                logger.info(f"Processing {len(valid_values)} valid attribute values out of {len(custom_attrs_values)}")
                
                if valid_values:
                    value_result = await dal.batch_upsert_custom_attribute_values(valid_values)
                    total_synced = value_result.get('upserted', 0)
                    logger.info(f"Custom attribute values sync completed: {total_synced} values")
            elif custom_attrs_values and not attr_id_mapping:
                logger.warning(f"Found {len(custom_attrs_values)} attribute values but no ID mappings available")
            
            return total_synced
            
        except Exception as e:
            logger.error(f"V2 custom attributes batch insert failed: {str(e).encode('ascii', errors='ignore').decode('ascii')}")
            return 0
    
    async def _batch_get_file_versions_and_custom_attrs_v2(self, project_id: str, 
                                                         changed_files: List[Dict[str, Any]], 
                                                         headers: dict) -> List[Dict[str, Any]]:
        """æ‰¹é‡è·å–æ–‡ä»¶ç‰ˆæœ¬å’Œè‡ªå®šä¹‰å±æ€§ - V2ç‰ˆæœ¬"""
        
        if not changed_files:
            return []
        
        logger.info(f"Batch retrieving file details: {len(changed_files)} files")
        
        try:
            # ğŸ”‘ æ‰¹é‡è·å–æ–‡ä»¶ç‰ˆæœ¬è¯¦ç»†ä¿¡æ¯ï¼ˆå‚è€ƒV1å®ç°ï¼‰
            file_ids = [file_data['id'] for file_data in changed_files]
            
            # åˆ†æ‰¹å¤„ç†ç‰ˆæœ¬ä¿¡æ¯è·å–
            batch_size = 10  # ç‰ˆæœ¬APIå¹¶å‘é™åˆ¶æ›´ä¸¥æ ¼
            all_file_metadata = []
            
            # å¹¶å‘è·å–ç‰ˆæœ¬ä¿¡æ¯
            try:
                import aiohttp
                
                async with aiohttp.ClientSession() as session:
                    # åˆ†æ‰¹å¤„ç†æ–‡ä»¶
                    for i in range(0, len(changed_files), batch_size):
                        batch_files = changed_files[i:i + batch_size]
                        
                        # å¹¶å‘è·å–å½“å‰æ‰¹æ¬¡çš„ç‰ˆæœ¬ä¿¡æ¯
                        tasks = []
                        for file_data in batch_files:
                            file_id = file_data.get('id')
                            if file_id:
                                task = self._get_file_versions_async(session, project_id, file_id, headers)
                                tasks.append((file_data, task))
                        
                        # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
                        for file_data, task in tasks:
                            try:
                                versions_info = await task
                                # å°†ç‰ˆæœ¬ä¿¡æ¯åˆå¹¶åˆ°æ–‡ä»¶æ•°æ®ä¸­
                                file_data['versions_info'] = versions_info
                                all_file_metadata.append(file_data)
                                
                                self.stats['api_calls'] += 1
                            except Exception as e:
                                logger.warning(f"è·å–æ–‡ä»¶ç‰ˆæœ¬å¤±è´¥ {file_data.get('id')}: {e}")
                                # å³ä½¿ç‰ˆæœ¬è·å–å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
                                file_data['versions_info'] = []
                                all_file_metadata.append(file_data)
                        
                        # APIèŠ‚æµ
                        if i + batch_size < len(changed_files):
                            await asyncio.sleep(self.api_delay)
                        
                        self.stats['batch_operations'] += 1
                        
            except ImportError:
                logger.warning("aiohttp not available, using basic file metadata")
                all_file_metadata = changed_files
            
            # ğŸ”‘ æ‰¹é‡è·å–è‡ªå®šä¹‰å±æ€§
            version_urns = []
            file_to_version_map = {}
            
            for file_metadata in all_file_metadata:
                # ä»versions_infoä¸­è·å–ç‰ˆæœ¬URN
                versions_info = file_metadata.get('versions_info', [])
                if versions_info:
                    version_urn = versions_info[0].get('id')  # ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
                    if version_urn:
                        version_urns.append(version_urn)
                        file_to_version_map[version_urn] = file_metadata.get('id')
            
            custom_attributes_data = []
            if version_urns:
                # åˆ†æ‰¹è·å–è‡ªå®šä¹‰å±æ€§ (APIé™åˆ¶50ä¸ª)
                for i in range(0, len(version_urns), batch_size):
                    batch_urns = version_urns[i:i + batch_size]
                    
                    async with self.api_semaphore:
                        # è°ƒç”¨å®é™…çš„Custom Attributes API
                        custom_attrs_batch = await self._call_custom_attributes_api(project_id, batch_urns, headers)
                        
                        custom_attributes_data.extend(custom_attrs_batch)
                        self.stats['api_calls'] += 1
                        self.stats['batch_operations'] += 1
            
            # åˆå¹¶æ–‡ä»¶å…ƒæ•°æ®ã€ç‰ˆæœ¬ä¿¡æ¯å’Œè‡ªå®šä¹‰å±æ€§
            enriched_files = []
            for file_metadata in all_file_metadata:
                # æŸ¥æ‰¾å¯¹åº”çš„è‡ªå®šä¹‰å±æ€§
                file_id = file_metadata.get('id')
                versions_info = file_metadata.get('versions_info', [])
                
                # ä»è‡ªå®šä¹‰å±æ€§APIè·å–é¢å¤–ä¿¡æ¯
                custom_attrs_info = None
                if versions_info:
                    version_urn = versions_info[0].get('id')
                    custom_attrs_info = next(
                        (attr for attr in custom_attributes_data if attr.get('urn') == version_urn),
                        None
                    )
                
                # åˆå¹¶æ‰€æœ‰ä¿¡æ¯åˆ°æ–‡ä»¶æ•°æ®ä¸­
                enriched_file = file_metadata.copy()
                enriched_file['custom_attributes'] = custom_attrs_info
                
                # å¦‚æœæ–‡ä»¶åä¸ºç©ºï¼Œå°è¯•ä»è‡ªå®šä¹‰å±æ€§ä¸­è·å–
                if not enriched_file.get('name') and custom_attrs_info:
                    enriched_file['name'] = custom_attrs_info.get('name') or custom_attrs_info.get('title')
                
                enriched_files.append(enriched_file)
            
            logger.info(f"File details retrieval completed: {len(enriched_files)} files")
            return enriched_files
            
        except Exception as e:
            logger.error(f"Batch file details retrieval failed: {e}")
            return changed_files  # è¿”å›åŸºæœ¬ä¿¡æ¯
    
    async def _get_folder_custom_attr_definitions_v2(self, project_id: str, folder_id: str,
                                                     headers: dict, session) -> List[Dict]:
        """è·å–å•ä¸ªæ–‡ä»¶å¤¹çš„è‡ªå®šä¹‰å±æ€§å®šä¹‰ - V2ç‰ˆæœ¬"""
        import urllib.parse

        definitions = []

        try:
            # Remove 'b.' prefix for BIM360 API
            bim360_project_id = project_id.replace('b.', '') if project_id.startswith('b.') else project_id

            # URL encode folder ID
            encoded_folder_id = urllib.parse.quote(folder_id, safe='')

            url = f"https://developer.api.autodesk.com/bim360/docs/v1/projects/{bim360_project_id}/folders/{encoded_folder_id}/custom-attribute-definitions"

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    self.stats['api_calls'] += 1

                    results = data.get('results', [])
                    for definition in results:
                        attr_def = {
                            'attr_id': definition.get('id'),
                            'project_id': project_id,
                            'scope_type': 'folder',
                            'scope_folder_id': folder_id,
                            'name': definition.get('name'),
                            'type': definition.get('type'),
                            'array_values': definition.get('arrayValues', []),
                            'description': None,
                            'is_required': False,
                            'default_value': None,
                            'inherit_to_subfolders': False
                        }
                        definitions.append(attr_def)

                elif response.status == 404:
                    # æ–‡ä»¶å¤¹æ²¡æœ‰è‡ªå®šä¹‰å±æ€§å®šä¹‰æ˜¯æ­£å¸¸çš„
                    pass
                else:
                    logger.debug(f"Failed to get custom attr definitions for folder {folder_id}: {response.status}")

        except Exception as e:
            logger.warning(f"Error getting custom attr definitions for folder {folder_id}: {e}")

        return definitions

    async def _collect_custom_attributes_v2(self, project_id: str, headers: dict, session) -> Dict:
        """æ”¶é›†è‡ªå®šä¹‰å±æ€§ - V2ç‰ˆæœ¬"""
        # æ–‡ä»¶å¤¹è‡ªå®šä¹‰å±æ€§å·²åœ¨BFSéå†ä¸­æ”¶é›†
        # è¿™é‡Œå¯ä»¥æ”¶é›†å…¶ä»–ç±»å‹çš„è‡ªå®šä¹‰å±æ€§ï¼ˆå¦‚é¡¹ç›®çº§åˆ«çš„ï¼‰
        return {'definitions': [], 'values': []}
    
    async def _update_project_sync_status(self, project_id: str, dal):
        """æ›´æ–°é¡¹ç›®åŒæ­¥çŠ¶æ€"""
        try:
            async with dal.get_connection() as conn:
                await conn.execute("""
                    INSERT INTO projects (id, name, last_sync_time, last_full_sync_time, sync_status)
                    VALUES ($1, $2, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, 'completed')
                    ON CONFLICT (id) DO UPDATE SET
                        last_sync_time = CURRENT_TIMESTAMP,
                        last_full_sync_time = CURRENT_TIMESTAMP,
                        sync_status = 'completed',
                        updated_at = CURRENT_TIMESTAMP
                """, project_id, f"Project {project_id}")
        except Exception as e:
            logger.error(f"æ›´æ–°é¡¹ç›®åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
    
    def _parse_date_value(self, attr: Dict[str, Any]) -> Optional[datetime]:
        """è§£ææ—¥æœŸç±»å‹å±æ€§å€¼"""
        if attr.get('type') == 'date' and attr.get('value'):
            return self._parse_datetime(attr['value'])
        return None
    
    def _parse_number_value(self, attr: Dict[str, Any]) -> Optional[float]:
        """è§£ææ•°å€¼ç±»å‹å±æ€§å€¼"""
        if attr.get('type') == 'number' and attr.get('value') is not None:
            try:
                return float(attr['value'])
            except (ValueError, TypeError):
                return None
        return None
    
    def _parse_boolean_value(self, attr: Dict[str, Any]) -> Optional[bool]:
        """è§£æå¸ƒå°”ç±»å‹å±æ€§å€¼"""
        if attr.get('type') == 'boolean' and attr.get('value') is not None:
            value = attr['value']
            if isinstance(value, bool):
                return value
            elif isinstance(value, str):
                return value.lower() in ('true', '1', 'yes', 'on')
            elif isinstance(value, (int, float)):
                return bool(value)
        return None

# å…¨å±€ä¼˜åŒ–åŒæ­¥ç®¡ç†å™¨å®ä¾‹
optimized_postgresql_sync_manager = OptimizedPostgreSQLSyncManager(
    batch_size=100,
    api_delay=0.02,
    max_workers=8
)
