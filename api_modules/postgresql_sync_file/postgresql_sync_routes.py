# -*- coding: utf-8 -*-
"""
PostgreSQLåŒæ­¥è·¯ç”± - HTTP APIå±¤
æä¾›RESTful APIæ¥å£ï¼Œä½¿ç”¨é‡æ§‹å¾Œçš„æœå‹™å±¤
å°ˆæ³¨æ–¼HTTPè«‹æ±‚è™•ç†å’ŒéŸ¿æ‡‰æ ¼å¼åŒ–
"""

import asyncio
import logging
from datetime import datetime
from flask import Blueprint, jsonify, request
from typing import Dict, List, Any, Optional

from .postgresql_sync_service import postgresql_sync_service, create_sync_service
from .postgresql_sync_utils import (
    SyncManagerFactory, AuthUtils, RollupCheckUtils, 
    ResponseUtils, PerformanceUtils
)
from database_sql.optimized_data_access import get_optimized_postgresql_dal

logger = logging.getLogger(__name__)

# å‰µå»ºè—åœ–
postgresql_sync_bp = Blueprint('postgresql_sync', __name__)

# ============================================================================
# ğŸš€ æ ¸å¿ƒåŒæ­¥APIç«¯ç‚¹
# ============================================================================

@postgresql_sync_bp.route('/api/postgresql-sync/project/<project_id>/sync', methods=['POST'])
def unified_sync_api(project_id):
    """
    çµ±ä¸€çš„PostgreSQLé …ç›®åŒæ­¥API - é‡æ§‹ç‰ˆæœ¬
    
    POST /api/postgresql-sync/project/{project_id}/sync
    
    åƒæ•¸:
    - syncType: åŒæ­¥é¡å‹ ("full_sync" | "incremental_sync") (é»˜èª: "incremental_sync")
    - performanceMode: æ€§èƒ½æ¨¡å¼ ("standard" | "high_performance" | "memory_optimized") (é»˜èª: "standard")
    - maxDepth: æœ€å¤§éæ­·æ·±åº¦ (é»˜èª: 10)
    - includeCustomAttributes: æ˜¯å¦åŒ…å«è‡ªå®šç¾©å±¬æ€§ (é»˜èª: true)
    - enableTopLevelRollupCheck: æ˜¯å¦å•Ÿç”¨é ‚å±¤rollupæª¢æŸ¥ (é»˜èª: true)
    """
    try:
        # ç²å–å’Œé©—è­‰åƒæ•¸
        request_data = request.json or {}
        sync_type = request_data.get('syncType', 'incremental_sync')
        performance_mode = request_data.get('performanceMode', 'standard')
        max_depth = request_data.get('maxDepth', 10)
        include_custom_attributes = request_data.get('includeCustomAttributes', True)
        enable_top_level_rollup_check = request_data.get('enableTopLevelRollupCheck', True)
        
        # ä½¿ç”¨å·¥å…·é¡é©—è­‰åƒæ•¸
        validation = AuthUtils.validate_sync_parameters(
            sync_type, performance_mode, max_depth, include_custom_attributes
        )
        
        if not validation['valid']:
            return jsonify(ResponseUtils.create_error_response(
                f"åƒæ•¸é©—è­‰å¤±æ•—: {', '.join(validation['errors'])}", 
                "INVALID_PARAMETERS"
            )), 400
        
        logger.info(f"Starting {sync_type} for project {project_id} with {performance_mode} mode")
        
        # åŸ·è¡ŒåŒæ­¥ - ä½¿ç”¨æœå‹™å±¤
        def run_sync():
            if sync_type == 'full_sync':
                return asyncio.run(postgresql_sync_service.start_full_sync(
                    project_id=project_id,
                    max_depth=max_depth,
                    include_custom_attributes=include_custom_attributes,
                    performance_mode=performance_mode
                ))
            else:  # incremental_sync
                return asyncio.run(postgresql_sync_service.start_incremental_sync(
                    project_id=project_id,
                    max_depth=max_depth,
                    include_custom_attributes=include_custom_attributes,
                    performance_mode=performance_mode,
                    enable_top_level_rollup_check=enable_top_level_rollup_check
                ))
        
        # åœ¨å¾Œå°åŸ·è¡ŒåŒæ­¥
        import threading
        
        def background_sync():
            try:
                result = run_sync()
                logger.info(f"Sync completed: {result.get('status', 'unknown')}")
            except Exception as e:
                logger.error(f"Background sync failed: {e}")
        
        # ç«‹å³åŸ·è¡Œä»¥ç²å–ä»»å‹™IDå’Œåˆå§‹ç‹€æ…‹
        result = run_sync()
        
        # å¦‚æœæ˜¯no_changesç‹€æ…‹ï¼Œç›´æ¥è¿”å›
        if result.get('status') == 'no_changes':
            return jsonify(ResponseUtils.create_success_response(
                result, "Project skipped due to optimization"
            )), 200
        
        # å¦‚æœæœ‰éŒ¯èª¤ï¼Œç›´æ¥è¿”å›
        if result.get('status') == 'error':
            return jsonify(ResponseUtils.create_error_response(
                result.get('error', 'Unknown error'), "SYNC_ERROR"
            )), 500
        
        # è¿”å›æˆåŠŸéŸ¿æ‡‰
        response_data = ResponseUtils.create_sync_response(
            task_id=result.get('task_uuid', 'unknown'),
            sync_type=sync_type,
            performance_mode=performance_mode,
            status="started",
            message=f"PostgreSQL {sync_type} started successfully",
            top_level_rollup_check=enable_top_level_rollup_check
        )
        
        return jsonify(response_data), 202
        
    except Exception as e:
        logger.error(f"PostgreSQL sync API error: {e}")
        return jsonify(ResponseUtils.create_error_response(
            str(e), "API_ERROR"
        )), 500

@postgresql_sync_bp.route('/api/postgresql-sync/project/<project_id>/status/<task_id>', methods=['GET'])
def get_sync_status(project_id, task_id):
    """ç²å–PostgreSQLåŒæ­¥ä»»å‹™ç‹€æ…‹ - é‡æ§‹ç‰ˆæœ¬"""
    try:
        # ä½¿ç”¨æœå‹™å±¤ç²å–ç‹€æ…‹
        def get_status():
            return asyncio.run(postgresql_sync_service.get_sync_status(task_id))
        
        result = get_status()
        
        if result.get('status') == 'error':
            return jsonify(ResponseUtils.create_error_response(
                result.get('error', 'Task not found'), "TASK_NOT_FOUND"
            )), 404
        
        return jsonify(ResponseUtils.create_success_response(
            result, "Task status retrieved successfully"
        )), 200
        
    except Exception as e:
        logger.error(f"Get sync status error: {e}")
        return jsonify(ResponseUtils.create_error_response(
            str(e), "STATUS_ERROR"
        )), 500

@postgresql_sync_bp.route('/api/postgresql-sync/project/<project_id>/rollup-check', methods=['GET'])
def check_rollup_status(project_id):
    """
    ğŸš€ æ–°å¢ï¼šæ£€æŸ¥é¡¹ç›®é¡¶å±‚rollupçŠ¶æ€
    è¿™æ˜¯å…³é”®çš„ä¼˜åŒ–ç«¯ç‚¹ï¼Œå¯ä»¥åœ¨åŒæ­¥å‰å¿«é€Ÿåˆ¤æ–­æ˜¯å¦éœ€è¦åŒæ­¥
    """
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        last_sync_time_str = request.args.get('lastSyncTime')
        
        if not last_sync_time_str:
            return jsonify({
                "success": False,
                "error": "lastSyncTime parameter is required"
            }), 400
        
        # è§£ææ—¶é—´
        try:
            last_sync_time = datetime.fromisoformat(last_sync_time_str.replace('Z', '+00:00'))
        except ValueError:
            return jsonify({
                "success": False,
                "error": "Invalid lastSyncTime format. Use ISO format."
            }), 400
        
        # æ‰§è¡Œé¡¶å±‚rollupæ£€æŸ¥
        def check_rollup():
            sync_manager = sync_managers['standard']
            return asyncio.run(_check_project_top_level_rollup(project_id, sync_manager, last_sync_time))
        
        rollup_result = check_rollup()
        
        return jsonify({
            "success": True,
            "project_id": project_id,
            "last_sync_time": last_sync_time_str,
            "rollup_check_result": rollup_result
        }), 200
        
    except Exception as e:
        logger.error(f"Rollup check error: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

# ============================================================================
# ğŸš€ æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡API
# ============================================================================

@optimized_postgresql_sync_bp.route('/api/optimized-postgresql-sync/project/<project_id>/performance-stats', methods=['GET'])
def get_postgresql_performance_stats(project_id):
    """è·å–PostgreSQLåŒæ­¥æ€§èƒ½ç»Ÿè®¡"""
    try:
        def get_stats():
            return asyncio.run(_get_postgresql_performance_stats(project_id))
        
        stats = get_stats()
        
        return jsonify({
            "success": True,
            "project_id": project_id,
            "performance_stats": stats
        }), 200
        
    except Exception as e:
        logger.error(f"Get performance stats error: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@optimized_postgresql_sync_bp.route('/api/optimized-postgresql-sync/project/<project_id>/optimization-report', methods=['GET'])
def get_optimization_report(project_id):
    """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
    try:
        def get_report():
            return asyncio.run(_generate_optimization_report(project_id))
        
        report = get_report()
        
        return jsonify({
            "success": True,
            "project_id": project_id,
            "optimization_report": report
        }), 200
        
    except Exception as e:
        logger.error(f"Get optimization report error: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

# ============================================================================
# ğŸš€ è¾…åŠ©å‡½æ•°
# ============================================================================

async def _check_project_top_level_rollup(project_id: str, sync_manager: OptimizedPostgreSQLSyncManager, 
                                        last_sync_time: Optional[datetime] = None) -> Dict[str, Any]:
    """
    ğŸ”‘ å…³é”®ä¼˜åŒ–ï¼šæ£€æŸ¥é¡¹ç›®é¡¶å±‚rollupæ—¶é—´
    è¿™æ˜¯æœ€é‡è¦çš„ä¼˜åŒ– - å¯ä»¥åœ¨ä¸è°ƒç”¨ä»»ä½•APIçš„æƒ…å†µä¸‹åˆ¤æ–­æ•´ä¸ªé¡¹ç›®æ˜¯å¦éœ€è¦åŒæ­¥
    """
    try:
        dal = await get_optimized_postgresql_dal()
        
        # å¦‚æœæ²¡æœ‰æä¾›last_sync_timeï¼Œä»æ•°æ®åº“è·å–
        if not last_sync_time:
            last_sync_time = await dal.get_project_last_sync_time(project_id)
            
        if not last_sync_time:
            return {
                'can_skip_entire_project': False,
                'reason': 'No previous sync time found',
                'recommendation': 'Perform full sync'
            }
        
        # ğŸš€ æ ¸å¿ƒä¼˜åŒ–ï¼šè·å–é¡¹ç›®é¡¶å±‚æ–‡ä»¶å¤¹çš„æœ€å¤§rollupæ—¶é—´
        async with dal.get_connection() as conn:
            query = """
            SELECT 
                MAX(last_modified_time_rollup) as max_rollup_time,
                COUNT(*) as total_top_level_folders,
                COUNT(CASE WHEN last_modified_time_rollup > $2 THEN 1 END) as folders_with_changes
            FROM folders 
            WHERE project_id = $1 
              AND (parent_id IS NULL OR parent_id = '')
              AND last_modified_time_rollup IS NOT NULL
            """
            
            result = await conn.fetchrow(query, project_id, last_sync_time)
            
            if not result or not result['max_rollup_time']:
                return {
                    'can_skip_entire_project': False,
                    'reason': 'No top-level folders found or no rollup time available',
                    'recommendation': 'Perform incremental sync with folder-level checks'
                }
            
            max_rollup_time = result['max_rollup_time']
            total_folders = result['total_top_level_folders']
            folders_with_changes = result['folders_with_changes']
            
            # ğŸ¯ å…³é”®åˆ¤æ–­ï¼šå¦‚æœæœ€å¤§rollupæ—¶é—´ <= ä¸Šæ¬¡åŒæ­¥æ—¶é—´ï¼Œæ•´ä¸ªé¡¹ç›®éƒ½å¯ä»¥è·³è¿‡
            can_skip_entire_project = max_rollup_time <= last_sync_time
            
            skip_efficiency = 0.0
            if total_folders > 0:
                skip_efficiency = ((total_folders - folders_with_changes) / total_folders) * 100
            
            return {
                'can_skip_entire_project': can_skip_entire_project,
                'max_rollup_time': max_rollup_time.isoformat(),
                'last_sync_time': last_sync_time.isoformat(),
                'total_top_level_folders': total_folders,
                'folders_with_changes': folders_with_changes,
                'skip_efficiency_percentage': skip_efficiency,
                'recommendation': 'Skip entire project' if can_skip_entire_project else 'Perform incremental sync',
                'optimization_level': 'project_level' if can_skip_entire_project else 'folder_level'
            }
            
    except Exception as e:
        logger.error(f"Top-level rollup check failed: {e}")
        return {
            'can_skip_entire_project': False,
            'reason': f'Check failed: {str(e)}',
            'recommendation': 'Perform incremental sync with error handling'
        }

async def _execute_postgresql_sync(sync_manager: OptimizedPostgreSQLSyncManager, sync_type: str, 
                                 project_id: str, max_depth: int, include_custom_attributes: bool, 
                                 task_id: str) -> Dict[str, Any]:
    """æ‰§è¡ŒPostgreSQLåŒæ­¥"""
    try:
        headers = get_auth_headers()
        
        if sync_type == 'full_sync':
            result = await sync_manager.optimized_full_sync(
                project_id=project_id,
                max_depth=max_depth,
                include_custom_attributes=include_custom_attributes,
                task_uuid=task_id,
                headers=headers
            )
        else:  # incremental_sync
            result = await sync_manager.optimized_incremental_sync(
                project_id=project_id,
                max_depth=max_depth,
                include_custom_attributes=include_custom_attributes,
                task_uuid=task_id,
                headers=headers
            )
        
        return result
        
    except Exception as e:
        logger.error(f"PostgreSQL sync execution failed: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'task_id': task_id
        }

async def _get_postgresql_performance_stats(project_id: str) -> Dict[str, Any]:
    """è·å–PostgreSQLæ€§èƒ½ç»Ÿè®¡"""
    try:
        dal = await get_optimized_postgresql_dal()
        
        async with dal.get_connection() as conn:
            # è·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯
            stats_query = """
            SELECT 
                COUNT(DISTINCT f.id) as total_folders,
                COUNT(DISTINCT fi.id) as total_files,
                COUNT(DISTINCT cav.id) as total_custom_attributes,
                MAX(f.last_modified_time_rollup) as latest_rollup_time,
                MAX(f.updated_at) as latest_sync_time
            FROM folders f
            LEFT JOIN files fi ON fi.project_id = f.project_id
            LEFT JOIN custom_attribute_values cav ON cav.project_id = f.project_id
            WHERE f.project_id = $1
            """
            
            stats = await conn.fetchrow(stats_query, project_id)
            
            return {
                'project_id': project_id,
                'total_folders': stats['total_folders'] or 0,
                'total_files': stats['total_files'] or 0,
                'total_custom_attributes': stats['total_custom_attributes'] or 0,
                'latest_rollup_time': stats['latest_rollup_time'].isoformat() if stats['latest_rollup_time'] else None,
                'latest_sync_time': stats['latest_sync_time'].isoformat() if stats['latest_sync_time'] else None,
                'database_type': 'PostgreSQL',
                'optimization_features': [
                    'separated_custom_attributes',
                    'batch_upsert_operations',
                    'smart_branch_skipping',
                    'top_level_rollup_optimization',
                    'concurrent_processing'
                ]
            }
            
    except Exception as e:
        logger.error(f"Get PostgreSQL performance stats failed: {e}")
        return {
            'error': str(e),
            'project_id': project_id
        }

async def _generate_optimization_report(project_id: str) -> Dict[str, Any]:
    """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
    try:
        dal = await get_optimized_postgresql_dal()
        
        # è·å–æœ€è¿‘çš„åŒæ­¥ä»»åŠ¡ç»Ÿè®¡
        async with dal.get_connection() as conn:
            recent_tasks_query = """
            SELECT 
                task_type,
                performance_mode,
                results,
                performance_stats,
                duration_seconds,
                created_at
            FROM sync_tasks 
            WHERE project_id = $1 
              AND task_status = 'completed'
            ORDER BY created_at DESC 
            LIMIT 10
            """
            
            recent_tasks = await conn.fetch(recent_tasks_query, project_id)
            
            # è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            total_tasks = len(recent_tasks)
            if total_tasks == 0:
                return {
                    'project_id': project_id,
                    'message': 'No completed sync tasks found',
                    'recommendation': 'Run a sync to generate optimization report'
                }
            
            # åˆ†ææ€§èƒ½è¶‹åŠ¿
            avg_duration = sum(task['duration_seconds'] or 0 for task in recent_tasks) / total_tasks
            
            # åˆ†æä¼˜åŒ–æ•ˆç‡
            optimization_efficiencies = []
            for task in recent_tasks:
                perf_stats = task.get('performance_stats', {})
                if isinstance(perf_stats, dict):
                    efficiency = perf_stats.get('optimization_efficiency', 0)
                    if efficiency > 0:
                        optimization_efficiencies.append(efficiency)
            
            avg_optimization_efficiency = sum(optimization_efficiencies) / len(optimization_efficiencies) if optimization_efficiencies else 0
            
            return {
                'project_id': project_id,
                'analysis_period': f'Last {total_tasks} completed syncs',
                'performance_summary': {
                    'average_sync_duration_seconds': round(avg_duration, 2),
                    'average_optimization_efficiency': round(avg_optimization_efficiency, 1),
                    'total_completed_syncs': total_tasks
                },
                'optimization_features_active': [
                    'Top-level rollup time checking',
                    'Smart branch skipping',
                    'Batch API operations',
                    'Separated custom attributes tables',
                    'Concurrent processing with priority scheduling',
                    'Memory management and monitoring'
                ],
                'recommendations': [
                    'Enable top-level rollup checking for maximum efficiency',
                    'Use high_performance mode for large projects',
                    'Monitor optimization efficiency trends',
                    'Consider full sync if efficiency drops below 50%'
                ],
                'database_optimization_status': 'PostgreSQL with separated tables - Optimal'
            }
            
    except Exception as e:
        logger.error(f"Generate optimization report failed: {e}")
        return {
            'project_id': project_id,
            'error': str(e)
        }

# ============================================================================
# ğŸš€ é¡å¤–çš„ä¾¿æ·ç«¯é»
# ============================================================================

@postgresql_sync_bp.route('/api/postgresql-sync/performance-modes', methods=['GET'])
def get_performance_modes():
    """ç²å–å¯ç”¨çš„æ€§èƒ½æ¨¡å¼"""
    try:
        modes = SyncManagerFactory.get_available_modes()
        configs = {mode: SyncManagerFactory.PERFORMANCE_CONFIGS[mode] for mode in modes}
        
        return jsonify(ResponseUtils.create_success_response({
            'available_modes': modes,
            'configurations': configs
        }, "Performance modes retrieved successfully")), 200
        
    except Exception as e:
        logger.error(f"Get performance modes error: {e}")
        return jsonify(ResponseUtils.create_error_response(
            str(e), "MODES_ERROR"
        )), 500

@postgresql_sync_bp.route('/api/postgresql-sync/project/<project_id>/performance-stats', methods=['GET'])
def get_performance_stats(project_id):
    """ç²å–é …ç›®æ€§èƒ½çµ±è¨ˆ"""
    try:
        def get_stats():
            return asyncio.run(postgresql_sync_service.get_sync_performance_stats(project_id))
        
        result = get_stats()
        
        return jsonify(ResponseUtils.create_success_response(
            result, "Performance stats retrieved successfully"
        )), 200
        
    except Exception as e:
        logger.error(f"Get performance stats error: {e}")
        return jsonify(ResponseUtils.create_error_response(
            str(e), "STATS_ERROR"
        )), 500

# ============================================================================
# ğŸš€ å°å‡ºè—åœ–
# ============================================================================

def register_postgresql_sync_routes(app):
    """è¨»å†ŠPostgreSQLåŒæ­¥è·¯ç”± - é‡æ§‹ç‰ˆæœ¬"""
    app.register_blueprint(postgresql_sync_bp)
    logger.info("PostgreSQL sync routes registered successfully")

# å‘å¾Œå…¼å®¹çš„å‡½æ•¸å
def register_optimized_postgresql_sync_routes(app):
    """å‘å¾Œå…¼å®¹çš„è¨»å†Šå‡½æ•¸"""
    register_postgresql_sync_routes(app)
    logger.warning("Using deprecated function name. Please use register_postgresql_sync_routes instead.")
