# PostgreSQLå„ªåŒ–åŒæ­¥æ–¹æ¡ˆå®Œæ•´æ–‡æª”

## ğŸ“‹ ç›®éŒ„
- [1. æ–¹æ¡ˆæ¦‚è¿°](#1-æ–¹æ¡ˆæ¦‚è¿°)
- [2. ç³»çµ±æ¶æ§‹](#2-ç³»çµ±æ¶æ§‹)
- [3. æ•¸æ“šåº«è¨­è¨ˆ](#3-æ•¸æ“šåº«è¨­è¨ˆ)
- [4. å…¨é‡åŒæ­¥æµç¨‹](#4-å…¨é‡åŒæ­¥æµç¨‹)
- [5. å¢é‡åŒæ­¥æµç¨‹](#5-å¢é‡åŒæ­¥æµç¨‹)
- [6. æ ¸å¿ƒå„ªåŒ–ç­–ç•¥](#6-æ ¸å¿ƒå„ªåŒ–ç­–ç•¥)
- [7. APIèª¿ç”¨å„ªåŒ–](#7-apièª¿ç”¨å„ªåŒ–)
- [8. æ€§èƒ½é…ç½®](#8-æ€§èƒ½é…ç½®)
- [9. éƒ¨ç½²æŒ‡å—](#9-éƒ¨ç½²æŒ‡å—)
- [10. ç›£æ§å’Œç¶­è­·](#10-ç›£æ§å’Œç¶­è­·)

---

## 1. æ–¹æ¡ˆæ¦‚è¿°

### 1.1 èƒŒæ™¯
æœ¬æ–¹æ¡ˆæ˜¯é‡å°Autodesk Construction Cloud (ACC) é …ç›®æ•¸æ“šåŒæ­¥çš„PostgreSQLå„ªåŒ–è§£æ±ºæ–¹æ¡ˆï¼Œé€šéäº”å±¤å„ªåŒ–ç­–ç•¥å¯¦ç¾é«˜æ•ˆçš„æ•¸æ“šåŒæ­¥ï¼Œæ”¯æŒå…¨é‡å’Œå¢é‡åŒæ­¥æ¨¡å¼ã€‚

### 1.2 æ ¸å¿ƒç‰¹æ€§
- ğŸš€ **äº”å±¤å„ªåŒ–ç­–ç•¥**ï¼šæ™ºèƒ½åˆ†æ”¯è·³éã€æ‰¹é‡APIèª¿ç”¨ã€æ–‡ä»¶ç´šæ™‚é–“æˆ³æ¯”å°ã€æ‰¹é‡æ•¸æ“šåº«æ“ä½œã€å…§å­˜ç®¡ç†
- ğŸ”„ **é›™æ¨¡å¼åŒæ­¥**ï¼šå…¨é‡åŒæ­¥å’Œå¢é‡åŒæ­¥
- ğŸ“Š **å¯¦æ™‚ç›£æ§**ï¼šæ€§èƒ½çµ±è¨ˆã€å„ªåŒ–æ•ˆç‡è¿½è¹¤
- ğŸ¯ **æ™ºèƒ½è·³é**ï¼šé ‚å±¤rollupæª¢æŸ¥å¯å¯¦ç¾é …ç›®ç´šè·³é
- ğŸ”§ **éˆæ´»é…ç½®**ï¼šä¸‰ç¨®æ€§èƒ½æ¨¡å¼é©æ‡‰ä¸åŒå ´æ™¯

### 1.3 æ€§èƒ½æŒ‡æ¨™
- **å¢é‡åŒæ­¥å„ªåŒ–æ•ˆç‡**ï¼š70-100%
- **APIèª¿ç”¨æ¸›å°‘**ï¼šå¹³å‡ç¯€çœ80%ä»¥ä¸Š
- **åŒæ­¥é€Ÿåº¦æå‡**ï¼šæ¯”å‚³çµ±æ–¹æ¡ˆå¿«5-10å€
- **å…§å­˜ä½¿ç”¨å„ªåŒ–**ï¼šæ”¯æŒå¤§å‹é …ç›®ï¼ˆ10è¬+æ–‡ä»¶ï¼‰

---

## 2. ç³»çµ±æ¶æ§‹

### 2.1 æ•´é«”æ¶æ§‹åœ–
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PostgreSQLå„ªåŒ–åŒæ­¥ç³»çµ±                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HTTP APIå±¤ (postgresql_sync_routes.py)                    â”‚
â”‚  â”œâ”€â”€ çµ±ä¸€åŒæ­¥ç«¯é»                                            â”‚
â”‚  â”œâ”€â”€ æ€§èƒ½ç›£æ§ç«¯é»                                            â”‚
â”‚  â””â”€â”€ Rollupæª¢æŸ¥ç«¯é»                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ¥­å‹™é‚è¼¯å±¤ (postgresql_sync_service.py)                    â”‚
â”‚  â”œâ”€â”€ PostgreSQLSyncService                                 â”‚
â”‚  â”œâ”€â”€ å…¨é‡åŒæ­¥é‚è¼¯                                            â”‚
â”‚  â””â”€â”€ å¢é‡åŒæ­¥é‚è¼¯                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å·¥å…·æ¨¡çµ„å±¤ (postgresql_sync_utils.py)                      â”‚
â”‚  â”œâ”€â”€ SyncManagerFactory                                    â”‚
â”‚  â”œâ”€â”€ TaskManager                                           â”‚
â”‚  â”œâ”€â”€ AuthUtils                                             â”‚
â”‚  â”œâ”€â”€ RollupCheckUtils                                      â”‚
â”‚  â”œâ”€â”€ PerformanceUtils                                      â”‚
â”‚  â””â”€â”€ ResponseUtils                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åŒæ­¥ç®¡ç†å™¨å±¤ (postgresql_sync_manager.py)                   â”‚
â”‚  â”œâ”€â”€ OptimizedPostgreSQLSyncManager                        â”‚
â”‚  â”œâ”€â”€ äº”å±¤å„ªåŒ–ç­–ç•¥                                            â”‚
â”‚  â””â”€â”€ æ‰¹é‡APIèª¿ç”¨                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ•¸æ“šè¨ªå•å±¤ (database_sql/optimized_data_access.py)         â”‚
â”‚  â”œâ”€â”€ å„ªåŒ–çš„PostgreSQL DAL                                   â”‚
â”‚  â”œâ”€â”€ æ‰¹é‡æ“ä½œæ”¯æŒ                                            â”‚
â”‚  â””â”€â”€ é€£æ¥æ± ç®¡ç†                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒçµ„ä»¶

#### 2.2.1 HTTP APIå±¤
- **çµ±ä¸€ç«¯é»**ï¼š`/api/postgresql-sync/project/{project_id}/sync`
- **åƒæ•¸é©—è­‰**ï¼šçµ±ä¸€çš„è«‹æ±‚åƒæ•¸é©—è­‰
- **éŸ¿æ‡‰æ ¼å¼åŒ–**ï¼šæ¨™æº–åŒ–çš„æˆåŠŸ/éŒ¯èª¤éŸ¿æ‡‰
- **æ€§èƒ½ç›£æ§**ï¼šå¯¦æ™‚æ€§èƒ½çµ±è¨ˆç«¯é»

#### 2.2.2 æ¥­å‹™é‚è¼¯å±¤
- **PostgreSQLSyncService**ï¼šä¸»æœå‹™é¡
- **å‹•æ…‹æ€§èƒ½æ¨¡å¼**ï¼šé‹è¡Œæ™‚åˆ‡æ›æ€§èƒ½é…ç½®
- **é ‚å±¤rollupæª¢æŸ¥**ï¼šé …ç›®ç´šå„ªåŒ–åˆ¤æ–·
- **ä»»å‹™ç®¡ç†**ï¼šUUIDç”Ÿæˆå’Œç‹€æ…‹è¿½è¹¤

#### 2.2.3 åŒæ­¥ç®¡ç†å™¨å±¤
- **äº”å±¤å„ªåŒ–ç­–ç•¥**ï¼šæ ¸å¿ƒå„ªåŒ–ç®—æ³•
- **æ‰¹é‡APIèª¿ç”¨**ï¼šä¸¦ç™¼æ§åˆ¶å’Œç¯€æµ
- **æ•¸æ“šè½‰æ›**ï¼šACC APIåˆ°PostgreSQLæ ¼å¼
- **éŒ¯èª¤æ¢å¾©**ï¼šå¥å£¯çš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶

---

## 3. æ•¸æ“šåº«è¨­è¨ˆ

### 3.1 æ ¸å¿ƒè¡¨çµæ§‹

#### 3.1.1 é …ç›®è¡¨ (projects)
```sql
CREATE TABLE projects (
    id VARCHAR(255) PRIMARY KEY,
    name VARCHAR(500) NOT NULL,
    description TEXT,
    hub_id VARCHAR(255),
    account_id VARCHAR(255),
    status VARCHAR(50) DEFAULT 'active',
    last_sync_time TIMESTAMP WITH TIME ZONE,
    last_full_sync_time TIMESTAMP WITH TIME ZONE,
    sync_status VARCHAR(50) DEFAULT 'never_synced',
    sync_stats JSONB DEFAULT '{}'::jsonb,
    metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

#### 3.1.2 æ–‡ä»¶å¤¾è¡¨ (folders)
```sql
CREATE TABLE folders (
    id VARCHAR(500) PRIMARY KEY,
    project_id VARCHAR(255) NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    name VARCHAR(500) NOT NULL,
    display_name VARCHAR(500),
    parent_id VARCHAR(500),
    path TEXT NOT NULL,
    path_segments TEXT[] DEFAULT '{}',
    depth INTEGER DEFAULT 0,
    create_time TIMESTAMP WITH TIME ZONE,
    create_user_id VARCHAR(100),
    create_user_name VARCHAR(255),
    last_modified_time TIMESTAMP WITH TIME ZONE,
    last_modified_user_id VARCHAR(100),
    last_modified_user_name VARCHAR(255),
    last_modified_time_rollup TIMESTAMP WITH TIME ZONE,  -- ğŸ”‘ é—œéµå„ªåŒ–å­—æ®µ
    object_count INTEGER DEFAULT 0,
    total_size BIGINT DEFAULT 0,
    hidden BOOLEAN DEFAULT FALSE,
    metadata JSONB DEFAULT '{}'::jsonb,
    extension JSONB DEFAULT '{}'::jsonb,
    children_stats JSONB DEFAULT '{}'::jsonb,
    sync_info JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

#### 3.1.3 æ–‡ä»¶è¡¨ (files)
```sql
CREATE TABLE files (
    id VARCHAR(500) PRIMARY KEY,
    project_id VARCHAR(255) NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    name VARCHAR(500) NOT NULL,
    display_name VARCHAR(500),
    parent_folder_id VARCHAR(500),
    folder_path TEXT,
    full_path TEXT,
    path_segments TEXT[] DEFAULT '{}',
    depth INTEGER DEFAULT 0,
    create_time TIMESTAMP WITH TIME ZONE,
    create_user_id VARCHAR(100),
    create_user_name VARCHAR(255),
    last_modified_time TIMESTAMP WITH TIME ZONE,
    last_modified_user_id VARCHAR(100),
    last_modified_user_name VARCHAR(255),
    current_version_id VARCHAR(500),
    version_number INTEGER DEFAULT 1,
    file_size BIGINT DEFAULT 0,
    storage_size BIGINT DEFAULT 0,
    storage_urn TEXT,
    download_url TEXT,
    process_state VARCHAR(100),
    file_type VARCHAR(100),
    mime_type VARCHAR(200),
    reserved BOOLEAN DEFAULT FALSE,
    reserved_time TIMESTAMP WITH TIME ZONE,
    reserved_user_id VARCHAR(100),
    reserved_user_name VARCHAR(255),
    hidden BOOLEAN DEFAULT FALSE,
    metadata JSONB DEFAULT '{}'::jsonb,
    file_info JSONB DEFAULT '{}'::jsonb,
    current_version JSONB DEFAULT '{}'::jsonb,
    versions_summary JSONB DEFAULT '{}'::jsonb,
    sync_info JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

#### 3.1.4 æ–‡ä»¶ç‰ˆæœ¬è¡¨ (file_versions)
```sql
CREATE TABLE file_versions (
    id VARCHAR(500) PRIMARY KEY,
    file_id VARCHAR(500) NOT NULL,
    project_id VARCHAR(255) NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    version_number INTEGER NOT NULL,
    version_name VARCHAR(500),
    create_time TIMESTAMP WITH TIME ZONE,
    create_user_id VARCHAR(100),
    create_user_name VARCHAR(255),
    last_modified_time TIMESTAMP WITH TIME ZONE,
    last_modified_user_id VARCHAR(100),
    last_modified_user_name VARCHAR(255),
    file_size BIGINT DEFAULT 0,
    storage_size BIGINT DEFAULT 0,
    storage_urn TEXT,
    download_url TEXT,
    process_state VARCHAR(100),
    mime_type VARCHAR(200),
    version_metadata JSONB DEFAULT '{}'::jsonb,
    sync_info JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

#### 3.1.5 è‡ªå®šç¾©å±¬æ€§å®šç¾©è¡¨ (custom_attribute_definitions)
```sql
CREATE TABLE custom_attribute_definitions (
    id SERIAL PRIMARY KEY,
    attr_id INTEGER NOT NULL,
    project_id VARCHAR(255) NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    folder_id VARCHAR(500),
    name VARCHAR(200) NOT NULL,
    type VARCHAR(50) NOT NULL,
    array_values JSONB,
    description TEXT,
    is_required BOOLEAN DEFAULT FALSE,
    default_value TEXT,
    sync_info JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

#### 3.1.6 è‡ªå®šç¾©å±¬æ€§å€¼è¡¨ (custom_attribute_values)
```sql
CREATE TABLE custom_attribute_values (
    id SERIAL PRIMARY KEY,
    file_id VARCHAR(500) NOT NULL,
    attr_id INTEGER NOT NULL,
    project_id VARCHAR(255) NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    value TEXT,
    value_date TIMESTAMP WITH TIME ZONE,
    value_number DECIMAL(15,4),
    value_boolean BOOLEAN,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_by_user_id VARCHAR(100),
    updated_by_user_name VARCHAR(255),
    sync_info JSONB DEFAULT '{}'::jsonb,
    UNIQUE(file_id, attr_id)
);
```

#### 3.1.7 åŒæ­¥ä»»å‹™è¡¨ (sync_tasks)
```sql
CREATE TABLE sync_tasks (
    id SERIAL PRIMARY KEY,
    task_uuid UUID DEFAULT uuid_generate_v4() UNIQUE,
    project_id VARCHAR(255) NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    task_type VARCHAR(100) NOT NULL,
    task_status VARCHAR(50) DEFAULT 'pending',
    performance_mode VARCHAR(50) DEFAULT 'standard',
    parameters JSONB DEFAULT '{}'::jsonb,
    progress JSONB DEFAULT '{}'::jsonb,
    performance_stats JSONB DEFAULT '{}'::jsonb,
    results JSONB DEFAULT '{}'::jsonb,
    start_time TIMESTAMP WITH TIME ZONE,
    end_time TIMESTAMP WITH TIME ZONE,
    duration_seconds DECIMAL(10,3),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

### 3.2 é—œéµç´¢å¼•
```sql
-- é …ç›®ç›¸é—œç´¢å¼•
CREATE INDEX idx_folders_project_id ON folders(project_id);
CREATE INDEX idx_folders_parent_id ON folders(parent_id);
CREATE INDEX idx_folders_rollup_time ON folders(last_modified_time_rollup);  -- ğŸ”‘ å„ªåŒ–é—œéµ

-- æ–‡ä»¶ç›¸é—œç´¢å¼•
CREATE INDEX idx_files_project_id ON files(project_id);
CREATE INDEX idx_files_folder_id ON files(parent_folder_id);
CREATE INDEX idx_files_modified_time ON files(last_modified_time);

-- ç‰ˆæœ¬ç›¸é—œç´¢å¼•
CREATE INDEX idx_file_versions_file_id ON file_versions(file_id);
CREATE INDEX idx_file_versions_project_id ON file_versions(project_id);

-- è‡ªå®šç¾©å±¬æ€§ç´¢å¼•
CREATE INDEX idx_custom_attr_values_file ON custom_attribute_values(file_id);
CREATE INDEX idx_custom_attr_values_project ON custom_attribute_values(project_id);

-- ä»»å‹™ç›¸é—œç´¢å¼•
CREATE INDEX idx_sync_tasks_project ON sync_tasks(project_id);
CREATE INDEX idx_sync_tasks_uuid ON sync_tasks(task_uuid);
CREATE INDEX idx_sync_tasks_status ON sync_tasks(task_status);
```

---

## 4. å…¨é‡åŒæ­¥æµç¨‹

### 4.1 æµç¨‹æ¦‚è¿°
```mermaid
graph TD
    A[APIè«‹æ±‚] --> B[åƒæ•¸é©—è­‰]
    B --> C[ç”Ÿæˆä»»å‹™UUID]
    C --> D[ç²å–èªè­‰é ­]
    D --> E[èª¿æ•´æ€§èƒ½æ¨¡å¼]
    E --> F[å‰µå»ºä»»å‹™è¨˜éŒ„]
    F --> G[é‹è¡Œæ™‚æ•¸æ“šæ¸…ç†]
    G --> H[ç²å–é ‚ç´šæ–‡ä»¶å¤¾]
    H --> I[BFSéæ­¸æ”¶é›†]
    I --> J[æ‰¹é‡APIèª¿ç”¨]
    J --> K[æ•¸æ“šè½‰æ›]
    K --> L[æ‰¹é‡æ•¸æ“šåº«æ“ä½œ]
    L --> M[çµæœçµ±è¨ˆ]
```

### 4.2 è©³ç´°æ­¥é©Ÿ

#### 4.2.1 åˆå§‹åŒ–éšæ®µ
```python
# 1. åƒæ•¸é©—è­‰
validation = AuthUtils.validate_sync_parameters(
    'full_sync', performance_mode, max_depth, include_custom_attributes
)

# 2. ä»»å‹™ç®¡ç†
task_uuid = TaskManager.generate_task_uuid()
await TaskManager.create_sync_task_record(
    project_id, task_uuid, 'full_sync', performance_mode, parameters
)

# 3. æ€§èƒ½æ¨¡å¼èª¿æ•´
SyncManagerFactory.adjust_sync_manager(sync_manager, performance_mode)
```

#### 4.2.2 é‹è¡Œæ™‚æ•¸æ“šæ¸…ç†
```python
# æŒ‰æ­£ç¢ºé †åºæ¸…ç†é …ç›®æ•¸æ“š
async with dal.get_connection() as conn:
    deleted_attrs = await conn.fetchval(
        "DELETE FROM custom_attribute_values WHERE project_id = $1 RETURNING COUNT(*)", 
        project_id
    )
    deleted_defs = await conn.fetchval(
        "DELETE FROM custom_attribute_definitions WHERE project_id = $1 RETURNING COUNT(*)", 
        project_id
    )
    deleted_versions = await conn.fetchval(
        "DELETE FROM file_versions WHERE project_id = $1 RETURNING COUNT(*)", 
        project_id
    )
    deleted_files = await conn.fetchval(
        "DELETE FROM files WHERE project_id = $1 RETURNING COUNT(*)", 
        project_id
    )
    deleted_folders = await conn.fetchval(
        "DELETE FROM folders WHERE project_id = $1 RETURNING COUNT(*)", 
        project_id
    )
```

#### 4.2.3 APIæ•¸æ“šæ”¶é›†
```python
# 1. ç²å–é ‚ç´šæ–‡ä»¶å¤¾
top_folders_data = await self._get_top_folders_async(project_id, headers)

# 2. BFSéæ­¸æ”¶é›†
all_folders, all_files = await self._collect_all_items_recursive_async(
    project_id, top_folders_data['data'], headers, max_depth
)

# 3. æ‰¹é‡è™•ç†æ–‡ä»¶å¤¾
folders_result = await self._batch_process_folders_async(all_folders, project_id)

# 4. æ‰¹é‡è™•ç†æ–‡ä»¶ï¼ˆåŒ…å«ç‰ˆæœ¬å’Œè‡ªå®šç¾©å±¬æ€§ï¼‰
files_result = await self._batch_process_files_async(all_files, project_id)
```

#### 4.2.4 BFSéæ­·å’ŒIDä¾†æºåˆ†æ

**ğŸ“‹ ç•¶å‰å¯¦ç¾åˆ†æ**

**1. æ–‡ä»¶å¤¾å’Œæ–‡ä»¶IDçš„ä¾†æºæµç¨‹**ï¼š

```python
# Step 1: å¾ACC APIç²å–é ‚ç´šæ–‡ä»¶å¤¾
top_folders_data = await self._get_top_folders_async(project_id, headers)
# API: GET /data/v1/projects/{project_id}/folders/{root_folder_id}/contents
# éŸ¿æ‡‰åŒ…å«é ‚ç´šæ–‡ä»¶å¤¾çš„IDå’Œmetadata

# Step 2: BFSéæ­·éç¨‹ä¸­ï¼Œå¾æ¯å€‹æ–‡ä»¶å¤¾çš„contents APIç²å–å­é …ç›®ID
for folder_data, depth, parent_path in current_batch:
    folder_id = folder_data['id']  # ğŸ”‘ å¾APIéŸ¿æ‡‰ä¸­æå–ID
    contents = await self._get_folder_contents_async(session, project_id, folder_id, headers)
    # API: GET /data/v1/projects/{project_id}/folders/{folder_id}/contents
    
    for item in contents['data']:
        if item['type'] == 'folders':
            queue.append((item, depth + 1, parent_path))  # itemåŒ…å«å­æ–‡ä»¶å¤¾ID
        elif item['type'] == 'items':
            all_files.append(item)  # itemåŒ…å«æ–‡ä»¶ID
```

**2. ç•¶å‰BFSå­˜å„²æ–¹å¼**ï¼š

```python
# ğŸ”„ ç•¶å‰å¯¦ç¾ï¼šå…§å­˜ä¸­BFS + åˆ†éšæ®µæ•¸æ“šåº«æ’å…¥
async def _collect_all_items_recursive_async(self, project_id: str, top_folders: list, 
                                           headers: dict, max_depth: int):
    all_folders = []  # ğŸ”‘ å®Œå…¨å­˜å„²åœ¨å…§å­˜ä¸­
    all_files = []    # ğŸ”‘ å®Œå…¨å­˜å„²åœ¨å…§å­˜ä¸­
    
    # BFS queue: (folder_data, depth, parent_path)
    queue = [(folder, 0, "") for folder in top_folders]
    
    while queue:
        current_batch = queue[:self.batch_size]  # æ‰¹é‡è™•ç†
        queue = queue[self.batch_size:]
        
        # ä¸¦ç™¼ç²å–æ–‡ä»¶å¤¾å…§å®¹
        for folder_data, depth, parent_path in current_batch:
            all_folders.append(folder_data)  # ğŸ”‘ æ·»åŠ åˆ°å…§å­˜åˆ—è¡¨
            
            contents = await self._get_folder_contents_async(session, project_id, folder_id, headers)
            for item in contents['data']:
                if item['type'] == 'folders':
                    queue.append((item, depth + 1, parent_path))
                elif item['type'] == 'items':
                    all_files.append(item)  # ğŸ”‘ æ·»åŠ åˆ°å…§å­˜åˆ—è¡¨
    
    return all_folders, all_files  # è¿”å›å®Œæ•´çš„å…§å­˜é›†åˆ

# Phase 2: æ‰¹é‡æ•¸æ“šåº«æ“ä½œï¼ˆBFSå®Œæˆå¾Œï¼‰
folders_result = await self._batch_process_folders_async(all_folders, project_id)
files_result = await self._batch_process_files_async(all_files, project_id)
```

#### 4.2.5 ä¸‰ç¨®BFSå¯¦ç¾æ–¹æ¡ˆå°æ¯”

**ğŸ”„ æ–¹æ¡ˆAï¼šç•¶å‰å¯¦ç¾ï¼ˆå…§å­˜BFS + æ‰¹é‡æ’å…¥ï¼‰**

```python
# å„ªé»ï¼š
âœ… å¯¦ç¾ç°¡å–®ç›´è§€
âœ… æ¸›å°‘æ•¸æ“šåº«é€£æ¥æ¬¡æ•¸
âœ… æ‰¹é‡æ“ä½œæ•ˆç‡é«˜
âœ… æ˜“æ–¼è™•ç†ä¾è³´é—œä¿‚ï¼ˆparent_idï¼‰

# ç¼ºé»ï¼š
âŒ å¤§é …ç›®å…§å­˜æ¶ˆè€—å·¨å¤§ï¼ˆ10è¬æ–‡ä»¶ â‰ˆ 500MB+ï¼‰
âŒ ä¸­é€”å¤±æ•—æ‰€æœ‰æ•¸æ“šä¸Ÿå¤±
âŒ ç„¡æ³•æä¾›å¯¦æ™‚é€²åº¦åé¥‹
âŒ ä¸é©åˆè¶…å¤§å‹é …ç›®

# é©ç”¨å ´æ™¯ï¼šä¸­å°å‹é …ç›®ï¼ˆ<50,000æ–‡ä»¶ï¼‰
```

**ğŸš€ æ–¹æ¡ˆBï¼šæµå¼è™•ç†ï¼ˆé‚Šéæ­·é‚Šæ’å…¥ï¼‰**

```python
async def _streaming_bfs_collect(self, project_id: str, top_folders: list, 
                               headers: dict, max_depth: int):
    queue = [(folder, 0, "") for folder in top_folders]
    
    while queue:
        current_batch = queue[:self.batch_size]
        queue = queue[self.batch_size:]
        
        # 1. ä¸¦ç™¼ç²å–æ–‡ä»¶å¤¾å…§å®¹
        batch_contents = await self._batch_get_folder_contents(current_batch, headers)
        
        # 2. ç«‹å³è™•ç†å’Œæ’å…¥æ•¸æ“šåº«
        folders_to_insert = []
        files_to_insert = []
        
        for folder_data, contents in batch_contents.items():
            folders_to_insert.append(folder_data)
            
            for item in contents.get('data', []):
                if item['type'] == 'folders':
                    queue.append((item, depth + 1, parent_path))
                elif item['type'] == 'items':
                    files_to_insert.append(item)
        
        # 3. ğŸ”‘ æ‰¹é‡æ’å…¥ç•¶å‰æ‰¹æ¬¡ï¼ˆå¯¦æ™‚è™•ç†ï¼‰
        if folders_to_insert:
            await self._batch_insert_folders(folders_to_insert)
        if files_to_insert:
            await self._batch_insert_files(files_to_insert)

# å„ªé»ï¼š
âœ… å…§å­˜ä½¿ç”¨ç©©å®šï¼ˆåªä¿å­˜ç•¶å‰æ‰¹æ¬¡ï¼‰
âœ… å¯¦æ™‚é€²åº¦åé¥‹
âœ… ä¸­é€”å¤±æ•—å¯éƒ¨åˆ†æ¢å¾©
âœ… æ”¯æŒè¶…å¤§é …ç›®ï¼ˆç„¡å…§å­˜é™åˆ¶ï¼‰

# ç¼ºé»ï¼š
âŒ æ•¸æ“šåº«é€£æ¥æ¬¡æ•¸å¢åŠ 
âŒ å¯¦ç¾è¤‡é›œåº¦æé«˜
âŒ é›£ä»¥è™•ç†ä¾è³´é—œä¿‚ï¼ˆparent_idå¯èƒ½æœªæ’å…¥ï¼‰
âŒ äº‹å‹™ç®¡ç†è¤‡é›œ

# é©ç”¨å ´æ™¯ï¼šè¶…å¤§å‹é …ç›®ï¼ˆ>100,000æ–‡ä»¶ï¼‰
```

**ğŸ¯ æ–¹æ¡ˆCï¼šåˆ†å±¤æ‰¹é‡è™•ç†ï¼ˆæ¨è–¦ï¼‰**

```python
async def _layered_bfs_processing(self, project_id: str, top_folders: list, 
                                headers: dict, max_depth: int):
    for depth in range(max_depth):
        # 1. ç²å–ç•¶å‰å±¤ç´šçš„æ‰€æœ‰æ–‡ä»¶å¤¾
        if depth == 0:
            current_level_folders = top_folders
        else:
            current_level_folders = await self._get_folders_by_depth(project_id, depth)
        
        if not current_level_folders:
            break
            
        # 2. æ‰¹é‡ç²å–ç•¶å‰å±¤ç´šæ‰€æœ‰æ–‡ä»¶å¤¾çš„å…§å®¹
        level_contents = await self._batch_get_level_contents(current_level_folders, headers)
        
        # 3. åˆ†é›¢æ–‡ä»¶å¤¾å’Œæ–‡ä»¶
        next_level_folders = []
        current_level_files = []
        
        for contents in level_contents:
            for item in contents.get('data', []):
                if item['type'] == 'folders':
                    next_level_folders.append(item)
                elif item['type'] == 'items':
                    current_level_files.append(item)
        
        # 4. ğŸ”‘ æ‰¹é‡æ’å…¥ç•¶å‰å±¤ç´šæ•¸æ“š
        if next_level_folders:
            await self._batch_insert_folders(next_level_folders)
        if current_level_files:
            await self._batch_insert_files(current_level_files)

# å„ªé»ï¼š
âœ… å…§å­˜ä½¿ç”¨å¯æ§ï¼ˆæŒ‰å±¤ç´šè™•ç†ï¼‰
âœ… ä¿æŒæ‰¹é‡æ“ä½œæ•ˆç‡
âœ… æ˜“æ–¼è™•ç†ä¾è³´é—œä¿‚ï¼ˆçˆ¶ç´šå…ˆæ’å…¥ï¼‰
âœ… æ”¯æŒæ–·é»çºŒå‚³
âœ… å¯¦æ™‚é€²åº¦åé¥‹ï¼ˆæŒ‰å±¤ç´šï¼‰
âœ… äº‹å‹™ç®¡ç†æ¸…æ™°

# ç¼ºé»ï¼š
âŒ éœ€è¦é¡å¤–çš„æ·±åº¦æŸ¥è©¢
âŒ å¯¦ç¾è¤‡é›œåº¦ä¸­ç­‰
âŒ å±¤ç´šä¸å‡å‹»æ™‚æ•ˆç‡ç•¥ä½

# é©ç”¨å ´æ™¯ï¼šå¤§å‹é …ç›®ï¼ˆ50,000-500,000æ–‡ä»¶ï¼‰
```

#### 4.2.6 æ‰¹é‡APIèª¿ç”¨ç­–ç•¥

**æ–‡ä»¶å¤¾å…§å®¹æ‰¹é‡ç²å–**ï¼š
```python
async def _batch_get_folder_contents(self, project_id: str, folder_ids: List[str], headers: dict):
    semaphore = asyncio.Semaphore(self.max_workers)  # ä¸¦ç™¼æ§åˆ¶
    
    async def get_single_folder_content(folder_id: str):
        async with semaphore:
            await asyncio.sleep(self.api_delay)  # APIç¯€æµ
            # èª¿ç”¨ACC API
            url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/folders/{folder_id}/contents"
            # ... APIèª¿ç”¨é‚è¼¯
    
    # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰APIèª¿ç”¨
    tasks = [get_single_folder_content(folder_id) for folder_id in folder_ids]
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

**æ–‡ä»¶ç‰ˆæœ¬æ‰¹é‡ç²å–**ï¼š
```python
async def _batch_get_file_versions(self, project_id: str, file_ids: List[str], headers: dict):
    # æ¯å€‹æ–‡ä»¶ä¸€å€‹APIèª¿ç”¨
    async def get_single_file_versions(file_id: str):
        url = f"https://developer.api.autodesk.com/data/v1/projects/{project_id}/items/{file_id}/versions"
        # ... APIèª¿ç”¨é‚è¼¯
    
    # ä¸¦ç™¼åŸ·è¡Œ
    tasks = [get_single_file_versions(file_id) for file_id in file_ids]
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

**è‡ªå®šç¾©å±¬æ€§æ‰¹é‡ç²å–**ï¼š
```python
async def _batch_get_custom_attributes(self, project_id: str, file_ids: List[str], headers: dict):
    # ä½¿ç”¨BIM360æ‰¹é‡ç«¯é»ï¼Œæ¯æ‰¹50å€‹æ–‡ä»¶
    batch_size = 50
    
    for i in range(0, len(file_ids), batch_size):
        batch_file_ids = file_ids[i:i + batch_size]
        batch_urns = [f"urn:adsk.wipprod:dm.lineage:{file_id}" for file_id in batch_file_ids]
        
        url = f"https://developer.api.autodesk.com/bim360/docs/v1/projects/{project_id}/versions:batch-get"
        payload = {"urns": batch_urns}
        # ... æ‰¹é‡APIèª¿ç”¨
```

#### 4.2.7 æ¨è–¦å¯¦ç¾æ–¹æ¡ˆ

**ğŸ¯ åŸºæ–¼é …ç›®è¦æ¨¡çš„å‹•æ…‹é¸æ“‡**ï¼š

```python
class AdaptiveBFSProcessor:
    def __init__(self, project_stats: dict):
        self.file_count = project_stats.get('estimated_file_count', 0)
        self.folder_count = project_stats.get('estimated_folder_count', 0)
        self.memory_limit_mb = project_stats.get('memory_limit_mb', 1024)
    
    async def choose_bfs_strategy(self) -> str:
        """æ ¹æ“šé …ç›®è¦æ¨¡å‹•æ…‹é¸æ“‡BFSç­–ç•¥"""
        
        estimated_memory_mb = (self.file_count + self.folder_count) * 0.005  # æ¯é …ç´„5KB
        
        if estimated_memory_mb > self.memory_limit_mb:
            # è¶…å¤§é …ç›®ï¼šä½¿ç”¨æµå¼è™•ç†
            return 'streaming'
        elif self.folder_count > 1000:
            # å¤§å‹é …ç›®ï¼šä½¿ç”¨åˆ†å±¤è™•ç†
            return 'layered'
        else:
            # ä¸­å°å‹é …ç›®ï¼šä½¿ç”¨å…§å­˜BFS
            return 'memory_bfs'
    
    async def execute_optimized_bfs(self, project_id: str, top_folders: list, 
                                  headers: dict, max_depth: int):
        strategy = await self.choose_bfs_strategy()
        
        if strategy == 'streaming':
            return await self._streaming_bfs_collect(project_id, top_folders, headers, max_depth)
        elif strategy == 'layered':
            return await self._layered_bfs_processing(project_id, top_folders, headers, max_depth)
        else:
            return await self._memory_bfs_collect(project_id, top_folders, headers, max_depth)
```

**ğŸ”§ å…§å­˜ä½¿ç”¨å„ªåŒ–**ï¼š

```python
# å…§å­˜ç›£æ§å’Œå‹•æ…‹èª¿æ•´
class MemoryAwareBFS:
    def __init__(self, memory_threshold_mb: int = 1024):
        self.memory_threshold_mb = memory_threshold_mb
        self.current_batch_size = 100
    
    async def adaptive_batch_processing(self, items: List, process_func):
        """è‡ªé©æ‡‰æ‰¹é‡è™•ç†ï¼Œæ ¹æ“šå…§å­˜ä½¿ç”¨å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°"""
        
        for i in range(0, len(items), self.current_batch_size):
            batch = items[i:i + self.current_batch_size]
            
            # è™•ç†å‰æª¢æŸ¥å…§å­˜
            memory_before = self._get_memory_usage()
            
            # è™•ç†æ‰¹æ¬¡
            await process_func(batch)
            
            # è™•ç†å¾Œæª¢æŸ¥å…§å­˜
            memory_after = self._get_memory_usage()
            memory_increase = memory_after - memory_before
            
            # å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°
            if memory_after > self.memory_threshold_mb * 0.8:
                # å…§å­˜ä½¿ç”¨éé«˜ï¼Œæ¸›å°æ‰¹æ¬¡
                self.current_batch_size = max(self.current_batch_size // 2, 10)
                logger.info(f"Reduced batch size to {self.current_batch_size} due to memory pressure")
                
                # å¼·åˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
            elif memory_increase < 50 and self.current_batch_size < 200:
                # å…§å­˜ä½¿ç”¨ç©©å®šï¼Œå¯ä»¥å¢åŠ æ‰¹æ¬¡
                self.current_batch_size = min(self.current_batch_size * 1.2, 200)
    
    def _get_memory_usage(self) -> float:
        """ç²å–ç•¶å‰å…§å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
```

#### 4.2.8 BFSæ–¹æ¡ˆé¸æ“‡æŒ‡å—

**ğŸ“Š æ–¹æ¡ˆå°æ¯”ç¸½çµè¡¨**ï¼š

| ç‰¹æ€§ | æ–¹æ¡ˆA (å…§å­˜BFS) | æ–¹æ¡ˆB (æµå¼è™•ç†) | æ–¹æ¡ˆC (åˆ†å±¤è™•ç†) |
|------|----------------|-----------------|-----------------|
| **å…§å­˜ä½¿ç”¨** | é«˜ (500MB+) | ä½ (50MB) | ä¸­ç­‰ (100-200MB) |
| **å¯¦ç¾è¤‡é›œåº¦** | ç°¡å–® | è¤‡é›œ | ä¸­ç­‰ |
| **æ•¸æ“šåº«é€£æ¥** | å°‘ | å¤š | ä¸­ç­‰ |
| **å¯¦æ™‚é€²åº¦** | âŒ | âœ… | âœ… |
| **æ–·é»çºŒå‚³** | âŒ | âœ… | âœ… |
| **ä¾è³´è™•ç†** | âœ… | âŒ | âœ… |
| **æ‰¹é‡æ•ˆç‡** | æœ€é«˜ | ä½ | é«˜ |
| **é©ç”¨é …ç›®è¦æ¨¡** | <50Kæ–‡ä»¶ | >100Kæ–‡ä»¶ | 50K-500Kæ–‡ä»¶ |
| **æ¨è–¦æŒ‡æ•¸** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |

**ğŸ¯ å¯¦éš›æ‡‰ç”¨å»ºè­°**ï¼š

```python
# æ ¹æ“šæ‚¨çš„å•é¡Œï¼Œæ¨è–¦ä½¿ç”¨æ–¹æ¡ˆCï¼ˆåˆ†å±¤æ‰¹é‡è™•ç†ï¼‰
# åŸå› åˆ†æï¼š

1. **IDä¾†æºæ¸…æ™°**ï¼š
   - æ–‡ä»¶å¤¾å’Œæ–‡ä»¶IDéƒ½ä¾†è‡ªACC APIéŸ¿æ‡‰
   - é€šéBFSéæ­·é€å±¤ç²å–ï¼Œä¿è­‰å®Œæ•´æ€§
   - æ¯å€‹APIèª¿ç”¨éƒ½è¿”å›ä¸‹ä¸€å±¤çš„IDåˆ—è¡¨

2. **å…§å­˜ä½¿ç”¨å¯æ§**ï¼š
   - ä¸æ˜¯å®Œå…¨å­˜å„²åœ¨å…§å­˜ä¸­
   - æŒ‰å±¤ç´šè™•ç†ï¼Œæ¯å±¤è™•ç†å®Œå³å¯é‡‹æ”¾
   - æ”¯æŒå¤§å‹é …ç›®è€Œä¸æœƒå…§å­˜æº¢å‡º

3. **æ•¸æ“šåº«æ’å…¥æ™‚æ©Ÿ**ï¼š
   - æ¯å±¤è™•ç†å®Œç«‹å³æ’å…¥æ•¸æ“šåº«
   - ä¿è­‰çˆ¶ç´šæ–‡ä»¶å¤¾å…ˆæ–¼å­ç´šæ’å…¥
   - ç¶­è­·referential integrity

4. **æ‰¹é‡APIèª¿ç”¨å„ªåŒ–**ï¼š
   - æ–‡ä»¶å¤¾å…§å®¹ï¼šä¸¦ç™¼èª¿ç”¨ï¼ˆæ¯å€‹æ–‡ä»¶å¤¾ä¸€å€‹APIï¼‰
   - æ–‡ä»¶ç‰ˆæœ¬ï¼šä¸¦ç™¼èª¿ç”¨ï¼ˆæ¯å€‹æ–‡ä»¶ä¸€å€‹APIï¼‰
   - è‡ªå®šç¾©å±¬æ€§ï¼šæ‰¹é‡ç«¯é»ï¼ˆ50å€‹æ–‡ä»¶ä¸€æ‰¹ï¼‰

# å¯¦ç¾ç¤ºä¾‹ï¼š
class OptimizedBFSProcessor:
    async def process_project_layered(self, project_id: str, max_depth: int):
        """åˆ†å±¤BFSè™•ç† - æ¨è–¦æ–¹æ¡ˆ"""
        
        for depth in range(max_depth):
            # 1. ç²å–ç•¶å‰å±¤ç´šæ–‡ä»¶å¤¾
            current_folders = await self._get_current_level_folders(project_id, depth)
            if not current_folders:
                break
            
            # 2. æ‰¹é‡ç²å–æ–‡ä»¶å¤¾å…§å®¹ï¼ˆä¸¦ç™¼APIèª¿ç”¨ï¼‰
            folder_contents = await self._batch_get_folder_contents(
                project_id, [f['id'] for f in current_folders], headers
            )
            
            # 3. åˆ†é›¢ä¸‹ä¸€å±¤æ–‡ä»¶å¤¾å’Œç•¶å‰å±¤æ–‡ä»¶
            next_folders, current_files = self._separate_folders_and_files(folder_contents)
            
            # 4. æ‰¹é‡æ’å…¥æ•¸æ“šåº«
            if next_folders:
                await self._batch_insert_folders(next_folders, project_id)
            if current_files:
                # ç²å–æ–‡ä»¶ç‰ˆæœ¬å’Œè‡ªå®šç¾©å±¬æ€§
                enriched_files = await self._enrich_files_with_details(
                    current_files, project_id, headers
                )
                await self._batch_insert_files(enriched_files, project_id)
            
            logger.info(f"Layer {depth}: {len(next_folders)} folders, {len(current_files)} files")
```

### 4.3 æ€§èƒ½å„ªåŒ–é…ç½®

#### 4.3.1 ä¸‰ç¨®æ€§èƒ½æ¨¡å¼
```python
PERFORMANCE_CONFIGS = {
    'standard': {
        'batch_size': 100,
        'api_delay': 0.02,
        'max_workers': 8,
        'memory_threshold_mb': 1024
    },
    'high_performance': {
        'batch_size': 200,
        'api_delay': 0.01,
        'max_workers': 16,
        'memory_threshold_mb': 2048
    },
    'memory_optimized': {
        'batch_size': 50,
        'api_delay': 0.05,
        'max_workers': 4,
        'memory_threshold_mb': 512
    }
}
```

#### 4.3.2 APIèª¿ç”¨çµ±è¨ˆç¤ºä¾‹
```python
# å‡è¨­é …ç›®ï¼š50å€‹æ–‡ä»¶å¤¾ï¼Œ1000å€‹æ–‡ä»¶
api_calls_breakdown = {
    'folder_contents': 50,      # æ–‡ä»¶å¤¾å…§å®¹
    'file_versions': 1000,      # æ–‡ä»¶ç‰ˆæœ¬
    'custom_attributes': 20,    # è‡ªå®šç¾©å±¬æ€§ï¼ˆæ‰¹é‡ï¼‰
    'total': 1070,
    'estimated_time': '45ç§’',   # é«˜æ€§èƒ½æ¨¡å¼
    'without_optimization': '300ç§’'
}
```

---

## 5. å¢é‡åŒæ­¥æµç¨‹

### 5.1 æµç¨‹æ¦‚è¿°
```mermaid
graph TD
    A[APIè«‹æ±‚] --> B[åƒæ•¸é©—è­‰]
    B --> C[ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“]
    C --> D{æœ‰åŒæ­¥è¨˜éŒ„?}
    D -->|å¦| E[åŸ·è¡Œå…¨é‡åŒæ­¥]
    D -->|æ˜¯| F[é ‚å±¤Rollupæª¢æŸ¥]
    F --> G{æ•´å€‹é …ç›®å¯è·³é?}
    G -->|æ˜¯| H[è¿”å›100%å„ªåŒ–æ•ˆç‡]
    G -->|å¦| I[æ™ºèƒ½åˆ†æ”¯éæ¿¾]
    I --> J[æ‰¹é‡APIæ“ä½œ]
    J --> K[æ–‡ä»¶ç´šæ™‚é–“æˆ³æ¯”å°]
    K --> L[æ‰¹é‡æ•¸æ“šåº«æ“ä½œ]
    L --> M[çµæœçµ±è¨ˆ]
```

### 5.2 æ ¸å¿ƒå„ªåŒ–ç­–ç•¥

#### 5.2.1 Layer 1: é ‚å±¤Rollupæª¢æŸ¥ ğŸš€
```sql
-- é—œéµå„ªåŒ–æŸ¥è©¢
SELECT 
    MAX(last_modified_time_rollup) as max_rollup_time,
    COUNT(*) as total_top_level_folders,
    COUNT(CASE WHEN last_modified_time_rollup > $2 THEN 1 END) as folders_with_changes
FROM folders 
WHERE project_id = $1 
  AND (parent_id IS NULL OR parent_id = '')
  AND last_modified_time_rollup IS NOT NULL
```

**åˆ¤æ–·é‚è¼¯**ï¼š
```python
# å¦‚æœ max_rollup_time <= last_sync_time
# å‰‡æ•´å€‹é …ç›®éƒ½å¯ä»¥è·³é
can_skip_entire_project = max_rollup_time <= last_sync_time

if can_skip_entire_project:
    return {
        'status': 'no_changes',
        'optimization_efficiency': 100.0,
        'folders_synced': 0,
        'files_synced': 0,
        'message': 'Entire project skipped due to top-level rollup optimization'
    }
```

#### 5.2.2 Layer 2: æ™ºèƒ½åˆ†æ”¯éæ¿¾
```python
async def _smart_branch_filtering(self, project_id: str, last_sync_time: datetime, headers: dict):
    # ç²å–å¯èƒ½æœ‰è®ŠåŒ–çš„æ–‡ä»¶å¤¾
    changed_folders = await dal.get_folders_for_smart_skip_check(project_id, last_sync_time)
    
    filtered_items = []
    for folder in changed_folders:
        rollup_time = self._parse_datetime(folder.get('last_modified_time_rollup'))
        
        if rollup_time and rollup_time <= last_sync_time:
            # ğŸš€ æ™ºèƒ½è·³éï¼šæ•´å€‹åˆ†æ”¯ç„¡è®ŠåŒ–
            self.stats['smart_skips'] += 1
            self.stats['api_calls_saved'] += folder.get('object_count', 1) * 2
            continue
        
        # éœ€è¦é€²ä¸€æ­¥æª¢æŸ¥çš„æ–‡ä»¶å¤¾
        filtered_items.append(folder)
    
    return filtered_items
```

#### 5.2.3 Layer 2.5: æ–‡ä»¶ç´šæ™‚é–“æˆ³æ¯”å°
```python
async def _identify_files_needing_updates(self, changed_files: List[Dict], 
                                        project_id: str, last_sync_time: datetime, dal):
    files_needing_updates = []
    
    for file_data in changed_files:
        file_last_modified = self._parse_datetime(
            file_data.get('attributes', {}).get('lastModifiedTime')
        )
        
        if self._is_changed(file_last_modified, last_sync_time):
            files_needing_updates.append(file_data)
    
    return files_needing_updates
```

#### 5.2.4 Layer 3: æ‰¹é‡APIæ“ä½œ
```python
async def _batch_api_operations(self, project_id: str, folders_to_check: List, headers: dict):
    # æ‰¹é‡ç²å–æ–‡ä»¶å¤¾å…§å®¹
    folder_ids = [folder.get('id') for folder in folders_to_check]
    contents_batch = await self._batch_get_folder_contents(project_id, folder_ids, headers)
    
    # åˆ†æè®Šæ›´
    changed_folders = []
    changed_files = []
    
    for folder_id, contents_data in contents_batch.items():
        # è™•ç†æ–‡ä»¶å¤¾å…§å®¹ï¼Œè­˜åˆ¥è®Šæ›´
        # ...
    
    return changed_folders, changed_files
```

#### 5.2.5 Layer 4: æ‰¹é‡æ•¸æ“šåº«æ“ä½œ
```python
async def _batch_database_operations(self, project_id: str, changed_folders: List, changed_files: List):
    # åªè™•ç†æœ‰è®ŠåŒ–çš„é …ç›®
    if changed_folders:
        folders_result = await self._batch_process_folders_async(changed_folders, project_id)
    
    if changed_files:
        files_result = await self._batch_process_files_async(changed_files, project_id)
    
    return results
```

### 5.3 å„ªåŒ–æ•ˆç‡è¨ˆç®—
```python
def _calculate_optimization_efficiency(self) -> float:
    total_operations = self.stats.get('concurrent_operations', 0) + self.stats.get('smart_skips', 0)
    smart_skips = self.stats.get('smart_skips', 0)
    
    if total_operations == 0:
        return 0.0
    
    # è¨ˆç®—è·³éçš„æ¯”ä¾‹ä½œç‚ºå„ªåŒ–æ•ˆç‡
    efficiency = (smart_skips / total_operations) * 100
    return round(efficiency, 2)
```

---

## 6. æ ¸å¿ƒå„ªåŒ–ç­–ç•¥

### 6.1 äº”å±¤å„ªåŒ–æ¶æ§‹

#### Layer 1: æ™ºèƒ½åˆ†æ”¯è·³éå„ªåŒ–
- **é ‚å±¤Rollupæª¢æŸ¥**ï¼šé …ç›®ç´šè·³éåˆ¤æ–·
- **åˆ†æ”¯ç´šè·³é**ï¼šæ–‡ä»¶å¤¾æ¨¹åˆ†æ”¯è·³é
- **APIèª¿ç”¨ç¯€çœ**ï¼šå¹³å‡ç¯€çœ80%ä»¥ä¸Š

#### Layer 2: æ‰¹é‡APIèª¿ç”¨å„ªåŒ–
- **ä¸¦ç™¼æ§åˆ¶**ï¼šä¿¡è™Ÿé‡é™åˆ¶ä¸¦ç™¼æ•¸
- **è‡ªé©æ‡‰ç¯€æµ**ï¼šå‹•æ…‹èª¿æ•´APIå»¶é²
- **æ‰¹é‡ç«¯é»**ï¼šä½¿ç”¨ACCæ‰¹é‡API

#### Layer 2.5: æ–‡ä»¶ç´šæ™‚é–“æˆ³æ¯”å°å„ªåŒ–
- **ç²¾ç¢ºæ¯”å°**ï¼šæ–‡ä»¶ç´šlastModifiedTimeæª¢æŸ¥
- **æ™ºèƒ½æ¨™è¨˜**ï¼šåªè™•ç†çœŸæ­£éœ€è¦æ›´æ–°çš„æ–‡ä»¶
- **å±¬æ€§å„ªåŒ–**ï¼šæ¸›å°‘è‡ªå®šç¾©å±¬æ€§APIèª¿ç”¨

#### Layer 3: æ‰¹é‡æ•¸æ“šåº«æ“ä½œå„ªåŒ–
- **æ‰¹é‡æ’å…¥**ï¼šPostgreSQL COPYå’Œæ‰¹é‡UPSERT
- **äº‹å‹™ç®¡ç†**ï¼šåˆç†çš„äº‹å‹™é‚Šç•Œ
- **é€£æ¥æ± **ï¼šé«˜æ•ˆçš„æ•¸æ“šåº«é€£æ¥ç®¡ç†

#### Layer 4: å…§å­˜ç®¡ç†å„ªåŒ–
- **æµå¼è™•ç†**ï¼šé‚Šç²å–é‚Šè™•ç†
- **å…§å­˜ç›£æ§**ï¼šå¯¦æ™‚å…§å­˜ä½¿ç”¨è¿½è¹¤
- **åƒåœ¾å›æ”¶**ï¼šåŠæ™‚é‡‹æ”¾ä¸éœ€è¦çš„å°è±¡

### 6.2 é—œéµå„ªåŒ–å­—æ®µ

#### 6.2.1 last_modified_time_rollup
```sql
-- é€™æ˜¯æœ€é—œéµçš„å„ªåŒ–å­—æ®µ
-- è¡¨ç¤ºæ–‡ä»¶å¤¾åŠå…¶æ‰€æœ‰å­é …ç›®çš„æœ€å¾Œä¿®æ”¹æ™‚é–“
last_modified_time_rollup TIMESTAMP WITH TIME ZONE
```

**ä½œç”¨**ï¼š
- é …ç›®ç´šè·³éåˆ¤æ–·
- åˆ†æ”¯ç´šè·³éåˆ¤æ–·
- æ¸›å°‘æ·±åº¦éæ­·

#### 6.2.2 object_count
```sql
-- æ–‡ä»¶å¤¾å…§å°è±¡æ•¸é‡çµ±è¨ˆ
object_count INTEGER DEFAULT 0
```

**ä½œç”¨**ï¼š
- ä¼°ç®—APIèª¿ç”¨ç¯€çœæ•¸é‡
- æ€§èƒ½çµ±è¨ˆå’Œé æ¸¬
- æ‰¹é‡è™•ç†å„ªåŒ–

### 6.3 æ€§èƒ½ç›£æ§æŒ‡æ¨™

#### 6.3.1 å¯¦æ™‚çµ±è¨ˆ
```python
self.stats = {
    'api_calls': 0,              # å¯¦éš›APIèª¿ç”¨æ¬¡æ•¸
    'api_calls_saved': 0,        # ç¯€çœçš„APIèª¿ç”¨æ¬¡æ•¸
    'smart_skips': 0,            # æ™ºèƒ½è·³éæ¬¡æ•¸
    'batch_operations': 0,       # æ‰¹é‡æ“ä½œæ¬¡æ•¸
    'concurrent_operations': 0,  # ä¸¦ç™¼æ“ä½œæ¬¡æ•¸
    'memory_peak_mb': 0,         # å…§å­˜å³°å€¼
    'processing_time': 0         # è™•ç†æ™‚é–“
}
```

#### 6.3.2 å„ªåŒ–æ•ˆç‡ç­‰ç´š
```python
def _calculate_performance_grade(self, avg_duration: float, avg_efficiency: float) -> str:
    if avg_duration < 30 and avg_efficiency > 80:
        return 'A+'
    elif avg_duration < 60 and avg_efficiency > 70:
        return 'A'
    elif avg_duration < 120 and avg_efficiency > 60:
        return 'B'
    elif avg_duration < 300 and avg_efficiency > 50:
        return 'C'
    else:
        return 'D'
```

---

## 7. APIèª¿ç”¨å„ªåŒ–

### 7.1 ACC APIç«¯é»æ˜ å°„

#### 7.1.1 æ ¸å¿ƒAPIç«¯é»
```python
API_ENDPOINTS = {
    # é …ç›®é ‚ç´šæ–‡ä»¶å¤¾
    'top_folders': 'GET /data/v1/projects/{project_id}/folders/{root_folder_id}/contents',
    
    # æ–‡ä»¶å¤¾å…§å®¹
    'folder_contents': 'GET /data/v1/projects/{project_id}/folders/{folder_id}/contents',
    
    # æ–‡ä»¶ç‰ˆæœ¬
    'file_versions': 'GET /data/v1/projects/{project_id}/items/{item_id}/versions',
    
    # è‡ªå®šç¾©å±¬æ€§ï¼ˆæ‰¹é‡ï¼‰
    'custom_attributes': 'POST /bim360/docs/v1/projects/{project_id}/versions:batch-get',
    
    # è‡ªå®šç¾©å±¬æ€§å®šç¾©
    'attribute_definitions': 'GET /bim360/docs/v1/projects/{project_id}/folders/{folder_id}/custom-attribute-definitions'
}
```

#### 7.1.2 æ‰¹é‡èª¿ç”¨ç­–ç•¥
```python
BATCH_STRATEGIES = {
    'folder_contents': {
        'method': 'ä¸¦ç™¼èª¿ç”¨',
        'max_concurrent': 16,
        'delay': 0.01,
        'retry': 3
    },
    'file_versions': {
        'method': 'ä¸¦ç™¼èª¿ç”¨',
        'max_concurrent': 8,
        'delay': 0.02,
        'retry': 2
    },
    'custom_attributes': {
        'method': 'æ‰¹é‡ç«¯é»',
        'batch_size': 50,
        'delay': 0.05,
        'retry': 3
    }
}
```

### 7.2 APIèª¿ç”¨æµç¨‹åœ–
```mermaid
graph TD
    A[é–‹å§‹åŒæ­¥] --> B[ç²å–é ‚ç´šæ–‡ä»¶å¤¾]
    B --> C[BFSéæ­·]
    C --> D[æ‰¹é‡ç²å–æ–‡ä»¶å¤¾å…§å®¹]
    D --> E[æå–æ–‡ä»¶åˆ—è¡¨]
    E --> F[ä¸¦ç™¼ç²å–æ–‡ä»¶ç‰ˆæœ¬]
    E --> G[æ‰¹é‡ç²å–è‡ªå®šç¾©å±¬æ€§]
    F --> H[æ•¸æ“šè½‰æ›å’Œå­˜å„²]
    G --> H
    
    D --> I[æ–‡ä»¶å¤¾API 1]
    D --> J[æ–‡ä»¶å¤¾API 2]
    D --> K[æ–‡ä»¶å¤¾API N]
    
    F --> L[ç‰ˆæœ¬API 1]
    F --> M[ç‰ˆæœ¬API 2]
    F --> N[ç‰ˆæœ¬API N]
    
    G --> O[å±¬æ€§æ‰¹æ¬¡1]
    G --> P[å±¬æ€§æ‰¹æ¬¡2]
    G --> Q[å±¬æ€§æ‰¹æ¬¡N]
```

### 7.3 éŒ¯èª¤è™•ç†å’Œé‡è©¦
```python
async def _api_call_with_retry(self, url: str, headers: dict, max_retries: int = 3):
    for attempt in range(max_retries):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        return await response.json()
                    elif response.status == 429:  # Rate limit
                        wait_time = 2 ** attempt  # æŒ‡æ•¸é€€é¿
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        raise Exception(f"API error: {response.status}")
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            await asyncio.sleep(1)
```

---

## 8. æ€§èƒ½é…ç½®

### 8.1 æ€§èƒ½æ¨¡å¼å°æ¯”

| é…ç½®é … | Standard | High Performance | Memory Optimized |
|--------|----------|------------------|------------------|
| **batch_size** | 100 | 200 | 50 |
| **max_workers** | 8 | 16 | 4 |
| **api_delay** | 0.02s | 0.01s | 0.05s |
| **memory_threshold** | 1024MB | 2048MB | 512MB |
| **é©ç”¨å ´æ™¯** | ä¸€èˆ¬é …ç›® | å¤§å‹é …ç›® | è³‡æºå—é™ç’°å¢ƒ |
| **é æœŸæ€§èƒ½** | å¹³è¡¡ | æœ€å¿« | æœ€ç©©å®š |

### 8.2 å‹•æ…‹é…ç½®èª¿æ•´
```python
def adjust_performance_config(self, project_stats: dict):
    """æ ¹æ“šé …ç›®çµ±è¨ˆå‹•æ…‹èª¿æ•´æ€§èƒ½é…ç½®"""
    
    folder_count = project_stats.get('folder_count', 0)
    file_count = project_stats.get('file_count', 0)
    
    if file_count > 50000:
        # å¤§å‹é …ç›®ï¼šä½¿ç”¨é«˜æ€§èƒ½æ¨¡å¼
        self.performance_mode = 'high_performance'
        self.max_workers = 16
        self.batch_size = 200
    elif file_count < 1000:
        # å°å‹é …ç›®ï¼šä½¿ç”¨å…§å­˜å„ªåŒ–æ¨¡å¼
        self.performance_mode = 'memory_optimized'
        self.max_workers = 4
        self.batch_size = 50
    else:
        # ä¸­å‹é …ç›®ï¼šä½¿ç”¨æ¨™æº–æ¨¡å¼
        self.performance_mode = 'standard'
        self.max_workers = 8
        self.batch_size = 100
```

### 8.3 å…§å­˜ç®¡ç†
```python
class MemoryManager:
    def __init__(self, threshold_mb: int = 1024):
        self.threshold_mb = threshold_mb
        self.current_usage = 0
    
    def check_memory_usage(self):
        """æª¢æŸ¥å…§å­˜ä½¿ç”¨æƒ…æ³"""
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > self.threshold_mb:
            # è§¸ç™¼åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            logger.warning(f"Memory usage high: {memory_mb:.1f}MB, triggered GC")
        
        return memory_mb
    
    async def batch_with_memory_check(self, items: List, batch_size: int, process_func):
        """å¸¶å…§å­˜æª¢æŸ¥çš„æ‰¹é‡è™•ç†"""
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            
            # è™•ç†æ‰¹æ¬¡
            await process_func(batch)
            
            # æª¢æŸ¥å…§å­˜
            memory_usage = self.check_memory_usage()
            
            # å¦‚æœå…§å­˜ä½¿ç”¨éé«˜ï¼Œæ¸›å°æ‰¹æ¬¡å¤§å°
            if memory_usage > self.threshold_mb * 0.8:
                batch_size = max(batch_size // 2, 10)
                logger.info(f"Reduced batch size to {batch_size} due to memory pressure")
```

---

## 9. éƒ¨ç½²æŒ‡å—

### 9.1 ç’°å¢ƒæº–å‚™

#### 9.1.1 ç³»çµ±è¦æ±‚
```yaml
# æœ€ä½è¦æ±‚
minimum_requirements:
  cpu: 2 cores
  memory: 4GB
  storage: 20GB
  python: 3.8+
  postgresql: 12+

# æ¨è–¦é…ç½®
recommended_requirements:
  cpu: 4+ cores
  memory: 8GB+
  storage: 100GB+ SSD
  python: 3.9+
  postgresql: 14+
```

#### 9.1.2 ä¾è³´å®‰è£
```bash
# å®‰è£Pythonä¾è³´
pip install -r requirements.txt

# ä¸»è¦ä¾è³´åŒ…
dependencies:
  - asyncio
  - asyncpg
  - aiohttp
  - flask
  - psutil
  - python-dateutil
```

### 9.2 æ•¸æ“šåº«åˆå§‹åŒ–

#### 9.2.1 å‰µå»ºæ•¸æ“šåº«
```bash
# æ–¹å¼1ï¼šå®Œæ•´å‰µå»º
python database_sql/create_optimized_db.py

# æ–¹å¼2ï¼šæ¸…ç†é‡å»º
python database_sql/clean_and_recreate.py

# æ–¹å¼3ï¼šé …ç›®ç´šæ¸…ç†
python database_sql/clean_and_recreate.py <project_id>
```

#### 9.2.2 é€£æ¥é…ç½®
```python
# database_sql/config.py
DATABASE_CONFIG = {
    'host': 'your-postgresql-host',
    'port': 5432,
    'database': 'acc_sync_db',
    'user': 'your_username',
    'password': 'your_password',
    'pool_size': 20,
    'max_overflow': 30,
    'pool_timeout': 30,
    'pool_recycle': 3600
}
```

### 9.3 æ‡‰ç”¨éƒ¨ç½²

#### 9.3.1 Flaskæ‡‰ç”¨é…ç½®
```python
# app.py
from flask import Flask
from api_modules.postgresql_sync_file.postgresql_sync_routes import register_postgresql_sync_routes

app = Flask(__name__)

# è¨»å†Šè·¯ç”±
register_postgresql_sync_routes(app)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

#### 9.3.2 Dockeréƒ¨ç½²
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 5000
CMD ["python", "app.py"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  acc-sync:
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/acc_sync
    depends_on:
      - db
  
  db:
    image: postgres:14
    environment:
      - POSTGRES_DB=acc_sync
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

### 9.4 é…ç½®æ–‡ä»¶
```yaml
# config.yml
sync_settings:
  default_performance_mode: "standard"
  max_depth: 10
  include_custom_attributes: true
  enable_top_level_rollup_check: true

api_settings:
  base_url: "https://developer.api.autodesk.com"
  timeout: 30
  max_retries: 3
  rate_limit_delay: 0.02

database_settings:
  connection_pool_size: 20
  query_timeout: 30
  batch_size: 100

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/acc_sync.log"
```

---

## 10. ç›£æ§å’Œç¶­è­·

### 10.1 æ€§èƒ½ç›£æ§

#### 10.1.1 é—œéµæŒ‡æ¨™
```python
MONITORING_METRICS = {
    # åŒæ­¥æ€§èƒ½æŒ‡æ¨™
    'sync_performance': {
        'avg_sync_duration': 'seconds',
        'optimization_efficiency': 'percentage',
        'api_calls_saved': 'count',
        'smart_skips': 'count'
    },
    
    # APIèª¿ç”¨æŒ‡æ¨™
    'api_metrics': {
        'total_api_calls': 'count',
        'api_success_rate': 'percentage',
        'avg_api_response_time': 'milliseconds',
        'api_error_count': 'count'
    },
    
    # æ•¸æ“šåº«æŒ‡æ¨™
    'database_metrics': {
        'connection_pool_usage': 'percentage',
        'query_execution_time': 'milliseconds',
        'database_size': 'MB',
        'index_usage': 'percentage'
    },
    
    # ç³»çµ±è³‡æºæŒ‡æ¨™
    'system_metrics': {
        'cpu_usage': 'percentage',
        'memory_usage': 'MB',
        'disk_usage': 'percentage',
        'network_io': 'MB/s'
    }
}
```

#### 10.1.2 ç›£æ§ç«¯é»
```python
# æ€§èƒ½çµ±è¨ˆAPI
GET /api/postgresql-sync/project/{project_id}/performance-stats

# å„ªåŒ–å ±å‘ŠAPI
GET /api/postgresql-sync/project/{project_id}/optimization-report

# ç³»çµ±å¥åº·æª¢æŸ¥
GET /api/postgresql-sync/health

# æ€§èƒ½æ¨¡å¼æŸ¥è©¢
GET /api/postgresql-sync/performance-modes
```

### 10.2 æ—¥èªŒç®¡ç†

#### 10.2.1 æ—¥èªŒç´šåˆ¥
```python
LOGGING_LEVELS = {
    'DEBUG': 'è©³ç´°èª¿è©¦ä¿¡æ¯ï¼ŒåŒ…æ‹¬APIèª¿ç”¨è©³æƒ…',
    'INFO': 'ä¸€èˆ¬ä¿¡æ¯ï¼ŒåŒæ­¥é€²åº¦å’Œçµæœ',
    'WARNING': 'è­¦å‘Šä¿¡æ¯ï¼Œæ€§èƒ½å•é¡Œæˆ–è¼•å¾®éŒ¯èª¤',
    'ERROR': 'éŒ¯èª¤ä¿¡æ¯ï¼ŒAPIèª¿ç”¨å¤±æ•—æˆ–æ•¸æ“šå•é¡Œ',
    'CRITICAL': 'åš´é‡éŒ¯èª¤ï¼Œç³»çµ±ç„¡æ³•æ­£å¸¸å·¥ä½œ'
}
```

#### 10.2.2 æ—¥èªŒæ ¼å¼
```python
# çµ±ä¸€æ—¥èªŒæ ¼å¼
LOG_FORMAT = {
    'timestamp': '2024-01-15T10:30:45.123Z',
    'level': 'INFO',
    'logger': 'postgresql_sync_manager',
    'message': 'Sync completed successfully',
    'context': {
        'project_id': 'proj_123',
        'task_uuid': 'uuid_456',
        'sync_type': 'incremental_sync',
        'duration': 45.67,
        'optimization_efficiency': 92.5
    }
}
```

### 10.3 ç¶­è­·ä»»å‹™

#### 10.3.1 å®šæœŸç¶­è­·
```python
# æ¯æ—¥ç¶­è­·ä»»å‹™
daily_maintenance = [
    'æ¸…ç†éæœŸçš„åŒæ­¥ä»»å‹™è¨˜éŒ„',
    'æ›´æ–°æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯',
    'æª¢æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…æ³',
    'ç›£æ§ç£ç›¤ç©ºé–“ä½¿ç”¨'
]

# æ¯é€±ç¶­è­·ä»»å‹™
weekly_maintenance = [
    'åˆ†ææ…¢æŸ¥è©¢æ—¥èªŒ',
    'å„ªåŒ–æ•¸æ“šåº«æ€§èƒ½',
    'æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§',
    'å‚™ä»½é‡è¦é…ç½®'
]

# æ¯æœˆç¶­è­·ä»»å‹™
monthly_maintenance = [
    'å…¨é¢æ€§èƒ½è©•ä¼°',
    'å®¹é‡è¦åŠƒåˆ†æ',
    'å®‰å…¨æ¼æ´æƒæ',
    'ä¾è³´åŒ…æ›´æ–°'
]
```

#### 10.3.2 æ•…éšœæ’é™¤

**å¸¸è¦‹å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆ**ï¼š

1. **APIèª¿ç”¨é »ç‡é™åˆ¶**
   ```python
   # è§£æ±ºæ–¹æ¡ˆï¼šå¢åŠ APIå»¶é²
   self.api_delay = 0.1  # å¢åŠ åˆ°100ms
   
   # æˆ–ä½¿ç”¨æŒ‡æ•¸é€€é¿
   async def exponential_backoff(self, attempt: int):
       wait_time = min(2 ** attempt, 60)  # æœ€å¤§60ç§’
       await asyncio.sleep(wait_time)
   ```

2. **å…§å­˜ä½¿ç”¨éé«˜**
   ```python
   # è§£æ±ºæ–¹æ¡ˆï¼šæ¸›å°‘æ‰¹æ¬¡å¤§å°
   self.batch_size = 50
   self.max_workers = 4
   
   # å¼·åˆ¶åƒåœ¾å›æ”¶
   import gc
   gc.collect()
   ```

3. **æ•¸æ“šåº«é€£æ¥æ± è€—ç›¡**
   ```python
   # è§£æ±ºæ–¹æ¡ˆï¼šå¢åŠ é€£æ¥æ± å¤§å°
   DATABASE_CONFIG['pool_size'] = 30
   DATABASE_CONFIG['max_overflow'] = 50
   
   # æª¢æŸ¥é€£æ¥æ´©æ¼
   async def check_connection_leaks(self):
       pool_status = await self.dal.get_pool_status()
       if pool_status['active_connections'] > pool_status['pool_size'] * 0.8:
           logger.warning("Connection pool usage high")
   ```

4. **åŒæ­¥æ€§èƒ½ä¸‹é™**
   ```python
   # è§£æ±ºæ–¹æ¡ˆï¼šåˆ†æå„ªåŒ–æ•ˆç‡
   if optimization_efficiency < 50:
       # è€ƒæ…®åŸ·è¡Œå…¨é‡åŒæ­¥é‡å»ºç´¢å¼•
       logger.warning("Low optimization efficiency, consider full sync")
   
   # æª¢æŸ¥rollupæ™‚é–“æ›´æ–°
   await self.verify_rollup_time_accuracy()
   ```

### 10.4 æ€§èƒ½èª¿å„ªå»ºè­°

#### 10.4.1 æ•¸æ“šåº«èª¿å„ª
```sql
-- PostgreSQLé…ç½®å„ªåŒ–
-- postgresql.conf
shared_buffers = 256MB
effective_cache_size = 1GB
maintenance_work_mem = 64MB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100

-- å®šæœŸæ›´æ–°çµ±è¨ˆä¿¡æ¯
ANALYZE;

-- é‡å»ºç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
REINDEX INDEX CONCURRENTLY idx_folders_rollup_time;
```

#### 10.4.2 æ‡‰ç”¨èª¿å„ª
```python
# é€£æ¥æ± èª¿å„ª
async def optimize_connection_pool(self):
    # æ ¹æ“šä¸¦ç™¼é‡èª¿æ•´é€£æ¥æ± 
    concurrent_syncs = await self.get_active_sync_count()
    optimal_pool_size = min(concurrent_syncs * 2, 50)
    
    await self.dal.resize_connection_pool(optimal_pool_size)

# APIèª¿ç”¨èª¿å„ª
def optimize_api_settings(self, api_response_stats: dict):
    avg_response_time = api_response_stats['avg_response_time']
    
    if avg_response_time > 2.0:
        # APIéŸ¿æ‡‰æ…¢ï¼Œæ¸›å°‘ä¸¦ç™¼
        self.max_workers = max(self.max_workers - 2, 2)
        self.api_delay += 0.01
    elif avg_response_time < 0.5:
        # APIéŸ¿æ‡‰å¿«ï¼Œå¢åŠ ä¸¦ç™¼
        self.max_workers = min(self.max_workers + 2, 20)
        self.api_delay = max(self.api_delay - 0.005, 0.005)
```

---

## 11. ç¸½çµ

### 11.1 æ–¹æ¡ˆå„ªå‹¢

1. **æ¥µé«˜çš„å„ªåŒ–æ•ˆç‡**ï¼šå¢é‡åŒæ­¥å¯é”70-100%å„ªåŒ–æ•ˆç‡
2. **æ™ºèƒ½è·³éæ©Ÿåˆ¶**ï¼šäº”å±¤å„ªåŒ–ç­–ç•¥å¤§å¹…æ¸›å°‘ä¸å¿…è¦çš„æ“ä½œ
3. **æ‰¹é‡è™•ç†å„ªåŒ–**ï¼šä¸¦ç™¼APIèª¿ç”¨å’Œæ‰¹é‡æ•¸æ“šåº«æ“ä½œ
4. **éˆæ´»çš„æ€§èƒ½é…ç½®**ï¼šä¸‰ç¨®æ€§èƒ½æ¨¡å¼é©æ‡‰ä¸åŒå ´æ™¯
5. **å®Œå–„çš„ç›£æ§é«”ç³»**ï¼šå¯¦æ™‚æ€§èƒ½çµ±è¨ˆå’Œå„ªåŒ–å»ºè­°
6. **å¥å£¯çš„éŒ¯èª¤è™•ç†**ï¼šé‡è©¦æ©Ÿåˆ¶å’Œå„ªé›…é™ç´š
7. **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šæ¸…æ™°çš„åˆ†å±¤æ¶æ§‹ï¼Œæ˜“æ–¼ç¶­è­·å’Œæ“´å±•

### 11.2 é©ç”¨å ´æ™¯

- **å¤§å‹ACCé …ç›®**ï¼š10è¬+æ–‡ä»¶çš„é …ç›®åŒæ­¥
- **é »ç¹æ›´æ–°é …ç›®**ï¼šéœ€è¦å®šæœŸå¢é‡åŒæ­¥çš„æ´»èºé …ç›®
- **è³‡æºå—é™ç’°å¢ƒ**ï¼šå…§å­˜å’Œç¶²çµ¡å¸¶å¯¬æœ‰é™çš„éƒ¨ç½²ç’°å¢ƒ
- **é«˜æ€§èƒ½è¦æ±‚**ï¼šå°åŒæ­¥é€Ÿåº¦æœ‰åš´æ ¼è¦æ±‚çš„æ¥­å‹™å ´æ™¯

### 11.3 æœªä¾†æ“´å±•

1. **å¤šé …ç›®ä¸¦ç™¼åŒæ­¥**ï¼šæ”¯æŒåŒæ™‚åŒæ­¥å¤šå€‹é …ç›®
2. **å¯¦æ™‚åŒæ­¥**ï¼šåŸºæ–¼WebSocketçš„å¯¦æ™‚æ•¸æ“šæ¨é€
3. **åˆ†ä½ˆå¼éƒ¨ç½²**ï¼šæ”¯æŒå¤šç¯€é»åˆ†ä½ˆå¼åŒæ­¥
4. **æ©Ÿå™¨å­¸ç¿’å„ªåŒ–**ï¼šåŸºæ–¼æ­·å²æ•¸æ“šé æ¸¬æœ€å„ªåŒæ­¥ç­–ç•¥
5. **æ›´å¤šæ•¸æ“šæº**ï¼šæ”¯æŒå…¶ä»–Autodeskç”¢å“å’Œç¬¬ä¸‰æ–¹ç³»çµ±

---

**æ–‡æª”ç‰ˆæœ¬**ï¼šv1.0  
**æœ€å¾Œæ›´æ–°**ï¼š2024å¹´1æœˆ  
**ç¶­è­·è€…**ï¼šPostgreSQLåŒæ­¥ç³»çµ±é–‹ç™¼åœ˜éšŠ

---

*æœ¬æ–‡æª”è©³ç´°æè¿°äº†PostgreSQLå„ªåŒ–åŒæ­¥æ–¹æ¡ˆçš„å®Œæ•´å¯¦ç¾ï¼ŒåŒ…æ‹¬æ¶æ§‹è¨­è¨ˆã€æ ¸å¿ƒç®—æ³•ã€éƒ¨ç½²æŒ‡å—å’Œç¶­è­·å»ºè­°ã€‚å¦‚æœ‰ç–‘å•æˆ–å»ºè­°ï¼Œè«‹è¯ç¹«é–‹ç™¼åœ˜éšŠã€‚*
