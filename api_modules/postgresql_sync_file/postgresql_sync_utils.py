# -*- coding: utf-8 -*-
"""
PostgreSQLåŒæ­¥ç³»çµ±çš„å…±åŒå·¥å…·å‡½æ•¸
æå–é‡è¤‡ä»£ç¢¼ï¼Œæé«˜å¯ç¶­è­·æ€§å’Œå¯æ¸¬è©¦æ€§
"""

import asyncio
import uuid
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional

try:
    from .postgresql_sync_manager import OptimizedPostgreSQLSyncManager
except ImportError:
    from postgresql_sync_manager import OptimizedPostgreSQLSyncManager
from database_sql.optimized_data_access import get_optimized_postgresql_dal

# å˜—è©¦å°å…¥èªè­‰ç®¡ç†å™¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ä½¿ç”¨utilsä¸­çš„èªè­‰
try:
    from .auth_manager import get_auth_headers
except ImportError:
    try:
        from auth_manager import get_auth_headers
    except ImportError:
        def get_auth_headers():
            """ä½¿ç”¨utilsä¸­çš„èªè­‰é‚è¼¯"""
            try:
                import sys
                import os
                # æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
                project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                if project_root not in sys.path:
                    sys.path.append(project_root)
                
                import utils
                access_token = utils.get_access_token()
                if access_token:
                    return {
                        "Authorization": f"Bearer {access_token}",
                        "Content-Type": "application/json"
                    }
                else:
                    logger.error("No access token available")
                    return None
            except Exception as e:
                logger.error(f"Failed to get auth headers: {e}")
                return None

logger = logging.getLogger(__name__)

# ============================================================================
# ğŸ”§ åŒæ­¥ç®¡ç†å™¨é…ç½®å·¥å…·
# ============================================================================

class SyncManagerFactory:
    """åŒæ­¥ç®¡ç†å™¨å·¥å» é¡"""
    
    PERFORMANCE_CONFIGS = {
        'standard': {
            'batch_size': 100,
            'api_delay': 0.02,
            'max_workers': 8,
            'memory_threshold_mb': 1024
        },
        'high_performance': {
            'batch_size': 200,
            'api_delay': 0.01,
            'max_workers': 16,
            'memory_threshold_mb': 2048
        },
        'memory_optimized': {
            'batch_size': 50,
            'api_delay': 0.05,
            'max_workers': 4,
            'memory_threshold_mb': 512
        }
    }
    
    @classmethod
    def create_sync_manager(cls, performance_mode: str = 'standard') -> OptimizedPostgreSQLSyncManager:
        """å‰µå»ºåŒæ­¥ç®¡ç†å™¨å¯¦ä¾‹"""
        if performance_mode not in cls.PERFORMANCE_CONFIGS:
            raise ValueError(f"Invalid performance mode: {performance_mode}")
        
        config = cls.PERFORMANCE_CONFIGS[performance_mode]
        return OptimizedPostgreSQLSyncManager(**config)
    
    @classmethod
    def get_available_modes(cls) -> List[str]:
        """ç²å–å¯ç”¨çš„æ€§èƒ½æ¨¡å¼"""
        return list(cls.PERFORMANCE_CONFIGS.keys())
    
    @classmethod
    def adjust_sync_manager(cls, sync_manager: OptimizedPostgreSQLSyncManager, 
                          performance_mode: str) -> None:
        """èª¿æ•´ç¾æœ‰åŒæ­¥ç®¡ç†å™¨çš„åƒæ•¸"""
        if performance_mode not in cls.PERFORMANCE_CONFIGS:
            logger.warning(f"Unknown performance mode: {performance_mode}, using standard")
            performance_mode = 'standard'
        
        config = cls.PERFORMANCE_CONFIGS[performance_mode]
        sync_manager.batch_size = config['batch_size']
        sync_manager.api_delay = config['api_delay']
        sync_manager.max_workers = config['max_workers']
        sync_manager.memory_threshold_mb = config['memory_threshold_mb']
        
        logger.info(f"åŒæ­¥ç®¡ç†å™¨å·²èª¿æ•´ç‚º {performance_mode} æ¨¡å¼")

# ============================================================================
# ğŸ”§ ä»»å‹™ç®¡ç†å·¥å…·
# ============================================================================

class TaskManager:
    """ä»»å‹™ç®¡ç†å·¥å…·é¡"""
    
    @staticmethod
    def generate_task_uuid() -> str:
        """ç”Ÿæˆä»»å‹™UUID"""
        return str(uuid.uuid4())
    
    @staticmethod
    async def create_sync_task_record(project_id: str, task_uuid: str, task_type: str,
                                    performance_mode: str, parameters: Dict[str, Any]) -> bool:
        """å‰µå»ºåŒæ­¥ä»»å‹™è¨˜éŒ„"""
        try:
            dal = await get_optimized_postgresql_dal()
            task_data = {
                'task_uuid': task_uuid,
                'project_id': project_id,
                'task_type': task_type,
                'task_status': 'running',
                'performance_mode': performance_mode,
                'parameters': parameters,
                'start_time': datetime.utcnow()
            }
            
            await dal.create_sync_task(task_data)
            return True
        except Exception as e:
            logger.error(f"Failed to create task record: {e}")
            return False
    
    @staticmethod
    async def complete_sync_task_record(task_uuid: str, results: Dict[str, Any], 
                                      sync_stats: Dict[str, Any] = None) -> bool:
        """Complete sync task record"""
        try:
            dal = await get_optimized_postgresql_dal()
            
            # Prepare complete results data
            complete_results = {
                'status': results.get('status', 'success'),
                'message': results.get('message', ''),
                'folders_synced': results.get('folders_synced', 0),
                'files_synced': results.get('files_synced', 0),
                'custom_attrs_synced': results.get('custom_attrs_synced', 0),
                'versions_synced': results.get('versions_synced', 0),
                'duration_seconds': results.get('duration_seconds', 0),
                'optimization_efficiency': results.get('optimization_efficiency', {}),
                'architecture_version': results.get('architecture_version', 'v2'),
                'sync_type': results.get('sync_type', 'unknown'),
                'synced_file_tree': True,  # Whether file tree was synced
                'synced_versions': results.get('versions_synced', 0) > 0,  # Whether versions were synced
                'synced_custom_attributes_definitions': results.get('custom_attrs_definitions_synced', 0) > 0,
                'synced_custom_attributes_values': results.get('custom_attrs_synced', 0) > 0,
                'synced_permissions': results.get('permissions_synced', False),  # Set to False for now, future expansion
                'performance_stats': sync_stats or results.get('performance_stats', {})
            }
            
            success = await dal.complete_sync_task(task_uuid, complete_results)
            if success:
                logger.info(f"Sync task record completed: {task_uuid}")
            else:
                logger.warning(f"Failed to complete sync task record: {task_uuid}")
            
            return success
        except Exception as e:
            logger.error(f"Failed to complete task record: {e}")
            return False
    
    @staticmethod
    async def fail_sync_task_record(task_uuid: str, error_message: str, 
                                  error_details: Dict[str, Any] = None) -> bool:
        """Mark sync task as failed"""
        try:
            dal = await get_optimized_postgresql_dal()
            
            # Prepare failure results data
            failure_results = {
                'status': 'failed',
                'error': error_message,
                'error_details': error_details or {},
                'folders_synced': 0,
                'files_synced': 0,
                'custom_attrs_synced': 0,
                'versions_synced': 0
            }
            
            success = await dal.complete_sync_task(task_uuid, failure_results)
            if success:
                logger.info(f"Sync task record marked as failed: {task_uuid}")
            
            return success
        except Exception as e:
            logger.error(f"Failed to mark task as failed: {e}")
            return False

# ============================================================================
# ğŸ”§ èªè­‰å’Œé©—è­‰å·¥å…·
# ============================================================================

class AuthUtils:
    """èªè­‰å’Œé©—è­‰å·¥å…·é¡"""
    
    @staticmethod
    def get_auth_headers_safe() -> Optional[Dict[str, str]]:
        """å®‰å…¨åœ°ç²å–èªè­‰é ­"""
        try:
            return get_auth_headers()
        except Exception as e:
            logger.error(f"ç²å–èªè­‰é ­å¤±æ•—: {e}")
            return None
    
    @staticmethod
    def validate_sync_parameters(sync_type: str, performance_mode: str, 
                               max_depth: int, include_custom_attributes: bool) -> Dict[str, Any]:
        """é©—è­‰åŒæ­¥åƒæ•¸"""
        errors = []
        
        # é©—è­‰åŒæ­¥é¡å‹
        if sync_type not in ['full_sync', 'incremental_sync']:
            errors.append(f"Invalid sync type: {sync_type}")
        
        # é©—è­‰æ€§èƒ½æ¨¡å¼
        if performance_mode not in SyncManagerFactory.get_available_modes():
            errors.append(f"Invalid performance mode: {performance_mode}")
        
        # é©—è­‰æ·±åº¦
        if not isinstance(max_depth, int) or max_depth < 1 or max_depth > 50:
            errors.append(f"Invalid max_depth: {max_depth}. Must be between 1 and 50")
        
        # é©—è­‰è‡ªå®šç¾©å±¬æ€§åƒæ•¸
        if not isinstance(include_custom_attributes, bool):
            errors.append(f"Invalid include_custom_attributes: {include_custom_attributes}")
        
        return {
            'valid': len(errors) == 0,
            'errors': errors
        }

# ============================================================================
# ğŸ”§ é ‚å±¤Rollupæª¢æŸ¥å·¥å…·
# ============================================================================

class RollupCheckUtils:
    """Rollupæª¢æŸ¥å·¥å…·é¡"""
    
    @staticmethod
    async def check_project_top_level_rollup(project_id: str, 
                                           sync_manager: OptimizedPostgreSQLSyncManager,
                                           last_sync_time: Optional[datetime] = None) -> Dict[str, Any]:
        """
        ğŸ”‘ é—œéµå„ªåŒ–ï¼šæª¢æŸ¥é …ç›®é ‚å±¤rollupæ™‚é–“
        é€™æ˜¯æœ€é‡è¦çš„å„ªåŒ– - å¯ä»¥åœ¨ä¸èª¿ç”¨ä»»ä½•APIçš„æƒ…æ³ä¸‹åˆ¤æ–·æ•´å€‹é …ç›®æ˜¯å¦éœ€è¦åŒæ­¥
        """
        try:
            dal = await get_optimized_postgresql_dal()
            
            # å¦‚æœæ²’æœ‰æä¾›last_sync_timeï¼Œå¾æ•¸æ“šåº«ç²å–
            if not last_sync_time:
                last_sync_time = await dal.get_project_last_sync_time(project_id)
                
            if not last_sync_time:
                return {
                    'can_skip_entire_project': False,
                    'reason': 'No previous sync time found',
                    'recommendation': 'Perform full sync'
                }
            
            # ğŸš€ æ ¸å¿ƒå„ªåŒ–ï¼šç²å–é …ç›®é ‚å±¤æ–‡ä»¶å¤¾çš„æœ€å¤§rollupæ™‚é–“
            async with dal.get_connection() as conn:
                query = """
                SELECT 
                    MAX(last_modified_time_rollup) as max_rollup_time,
                    COUNT(*) as total_top_level_folders,
                    COUNT(CASE WHEN last_modified_time_rollup > $2 THEN 1 END) as folders_with_changes
                FROM folders 
                WHERE project_id = $1 
                  AND depth = 0
                  AND last_modified_time_rollup IS NOT NULL
                """
                
                result = await conn.fetchrow(query, project_id, last_sync_time)
                
                if not result or not result['max_rollup_time']:
                    return {
                        'can_skip_entire_project': False,
                        'reason': 'No top-level folders found or no rollup time available',
                        'recommendation': 'Perform incremental sync with folder-level checks'
                    }
                
                max_rollup_time = result['max_rollup_time']
                total_folders = result['total_top_level_folders']
                folders_with_changes = result['folders_with_changes']
                
                # ğŸ¯ é—œéµåˆ¤æ–·ï¼šå¦‚æœæœ€å¤§rollupæ™‚é–“ <= ä¸Šæ¬¡åŒæ­¥æ™‚é–“ï¼Œæ•´å€‹é …ç›®éƒ½å¯ä»¥è·³é
                can_skip_entire_project = max_rollup_time <= last_sync_time
                
                skip_efficiency = 0.0
                if total_folders > 0:
                    skip_efficiency = ((total_folders - folders_with_changes) / total_folders) * 100
                
                return {
                    'can_skip_entire_project': can_skip_entire_project,
                    'max_rollup_time': max_rollup_time.isoformat(),
                    'last_sync_time': last_sync_time.isoformat(),
                    'total_top_level_folders': total_folders,
                    'folders_with_changes': folders_with_changes,
                    'skip_efficiency_percentage': skip_efficiency,
                    'recommendation': 'Skip entire project' if can_skip_entire_project else 'Perform incremental sync',
                    'optimization_level': 'project_level' if can_skip_entire_project else 'folder_level'
                }
                
        except Exception as e:
            logger.error(f"Top-level rollup check failed: {e}")
            return {
                'can_skip_entire_project': False,
                'reason': f'Check failed: {str(e)}',
                'recommendation': 'Perform incremental sync with error handling'
            }

# ============================================================================
# ğŸ”§ æ€§èƒ½çµ±è¨ˆå·¥å…·
# ============================================================================

class PerformanceUtils:
    """æ€§èƒ½çµ±è¨ˆå·¥å…·é¡"""
    
    @staticmethod
    def calculate_performance_grade(avg_duration: float, avg_efficiency: float) -> str:
        """è¨ˆç®—æ€§èƒ½ç­‰ç´š"""
        if avg_duration < 30 and avg_efficiency > 80:
            return 'A+'
        elif avg_duration < 60 and avg_efficiency > 70:
            return 'A'
        elif avg_duration < 120 and avg_efficiency > 60:
            return 'B'
        elif avg_duration < 300 and avg_efficiency > 50:
            return 'C'
        else:
            return 'D'
    
    @staticmethod
    async def get_efficiency_trend(project_id: str, 
                                 start_date: Optional[datetime] = None,
                                 end_date: Optional[datetime] = None) -> str:
        """ç²å–æ•ˆç‡è¶¨å‹¢"""
        try:
            dal = await get_optimized_postgresql_dal()
            
            async with dal.get_connection() as conn:
                # ç²å–æœ€è¿‘çš„åŒæ­¥è¨˜éŒ„ï¼ŒæŒ‰æ™‚é–“æ’åº
                conditions = ["project_id = $1", "task_status = 'completed'"]
                params = [project_id]
                
                if start_date:
                    conditions.append("start_time >= $2")
                    params.append(start_date)
                
                if end_date:
                    conditions.append("start_time <= $3")
                    params.append(end_date)
                
                query = f"""
                SELECT 
                    CAST(performance_stats->>'optimization_efficiency' AS FLOAT) as efficiency
                FROM sync_tasks 
                WHERE {' AND '.join(conditions)}
                ORDER BY start_time DESC
                LIMIT 10;
                """
                
                rows = await conn.fetch(query, *params)
                
                if len(rows) < 2:
                    return 'insufficient_data'
                
                efficiencies = [row['efficiency'] for row in rows if row['efficiency'] is not None]
                
                if len(efficiencies) < 2:
                    return 'insufficient_data'
                
                # è¨ˆç®—è¶¨å‹¢
                recent_avg = sum(efficiencies[:3]) / min(3, len(efficiencies))
                older_avg = sum(efficiencies[-3:]) / min(3, len(efficiencies[-3:]))
                
                if recent_avg > older_avg + 5:
                    return 'improving'
                elif recent_avg < older_avg - 5:
                    return 'declining'
                else:
                    return 'stable'
                    
        except Exception as e:
            logger.warning(f"ç²å–æ•ˆç‡è¶¨å‹¢å¤±æ•—: {e}")
            return 'unknown'

# ============================================================================
# ğŸ”§ éŸ¿æ‡‰æ ¼å¼åŒ–å·¥å…·
# ============================================================================

class ResponseUtils:
    """éŸ¿æ‡‰æ ¼å¼åŒ–å·¥å…·é¡"""
    
    @staticmethod
    def create_success_response(data: Dict[str, Any], message: str = "Success") -> Dict[str, Any]:
        """å‰µå»ºæˆåŠŸéŸ¿æ‡‰"""
        return {
            "success": True,
            "message": message,
            "data": data,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    @staticmethod
    def create_error_response(error: str, error_code: str = "UNKNOWN_ERROR") -> Dict[str, Any]:
        """å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰"""
        return {
            "success": False,
            "error": error,
            "error_code": error_code,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    @staticmethod
    def create_sync_response(task_id: str, sync_type: str, performance_mode: str, 
                           status: str, **kwargs) -> Dict[str, Any]:
        """å‰µå»ºåŒæ­¥éŸ¿æ‡‰"""
        response = {
            "success": True,
            "task_id": task_id,
            "sync_type": sync_type,
            "performance_mode": performance_mode,
            "status": status,
            "timestamp": datetime.utcnow().isoformat()
        }
        response.update(kwargs)
        return response

# ============================================================================
# ğŸ”§ åŒæ­¥åŸ·è¡Œå·¥å…·
# ============================================================================

class SyncExecutionUtils:
    """åŒæ­¥åŸ·è¡Œå·¥å…·é¡"""
    
    @staticmethod
    async def execute_sync(sync_manager: OptimizedPostgreSQLSyncManager, 
                         sync_type: str, project_id: str, max_depth: int, 
                         include_custom_attributes: bool, task_id: str) -> Dict[str, Any]:
        """åŸ·è¡ŒåŒæ­¥æ“ä½œ"""
        try:
            headers = AuthUtils.get_auth_headers_safe()
            if not headers:
                return {
                    'status': 'error',
                    'error': 'ç„¡æ³•ç²å–èªè­‰ä¿¡æ¯',
                    'task_id': task_id
                }
            
            if sync_type == 'full_sync':
                result = await sync_manager.optimized_full_sync(
                    project_id=project_id,
                    max_depth=max_depth,
                    include_custom_attributes=include_custom_attributes,
                    task_uuid=task_id,
                    headers=headers
                )
            else:  # incremental_sync
                result = await sync_manager.optimized_incremental_sync(
                    project_id=project_id,
                    max_depth=max_depth,
                    include_custom_attributes=include_custom_attributes,
                    task_uuid=task_id,
                    headers=headers
                )
            
            return result
            
        except Exception as e:
            logger.error(f"åŒæ­¥åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'task_id': task_id
            }
